
# Summary

```{r echo = FALSE, message = FALSE}
rm(list=ls());  set.seed(20210609);  
knitr::opts_chunk$set(fig.align = 'center', message = FALSE)
```


<tt>The following notes are mostly transcribed from Neath(0610,2021) lecture.</tt>

<p>&nbsp;</p>

We are done in the sense that no  new course material will be presented today.

Reminder:

Next Monday is a day of mandatory real-time participation. This will be our "in-class final exam". It will be interactive and collaborative. My goal for Monday is a learning experience more than final assessment. Attendance is mandatory. Be prepared to re-visit some homework problems and to answer questions posed directly to you ( if you're chosen by the $\texttt{sample()}$ function in $\texttt{R}$ ).


Given this is the last day and I said no new material let's spend a few minutes talking about the things we didn't cover in our course. 


## Missing data and imputation; 7b

We went over "7a" multivariate normal(MVN) models but we did not go over "7b" which would have been about missing data. Pity because that's a really cool application of the Gibbs sampler.

Multivariate normal data: $Y_{i,j}$ is response for variable $i$ for unit $j$. What if we observe $Y_{j} =$ (missing, observed, missing, observed). Suppose $p=4$ (there are 4 variables), and there's a case in our data set where we have observed $y_2$ and $y_4$ but are missing $y_1$ and $y_3$. We don't have to throw that case out! We can use the information that we have. The way we would do this:

Recall for the MVN model we have a Gibbs sampler that works by sampling from the full conditionals $\{~\boldsymbol\theta | \Sigma, \boldsymbol y~\}$ then $\{~\Sigma | \boldsymbol\theta, \boldsymbol y~\}$. Now write complete data $= \boldsymbol y = (\boldsymbol y_{obs}, \boldsymbol y_{mis}).$

A way to handle missing data is just by adding a step to the Gibbs sampler. We don't have to solve $\{~\boldsymbol\theta | \boldsymbol\Sigma, y_{obs}~\}$, $\{~\Sigma |\boldsymbol\theta, \boldsymbol y_{obs}~\}$. We've solved $\{~\boldsymbol\theta | \Sigma, \boldsymbol y~\}$ and $\{~\boldsymbol\Sigma | \boldsymbol{\theta, y}~\}$ already. So add a step to the Gibbs sampler where 

$$
\boldsymbol y_{mis}^{(s)} \sim  p( \boldsymbol y_{mis} | \theta^{(s)}, \Sigma^{(s)},\boldsymbol y_{obs})
$$ 

and then use those sampled values for the missing cases for the next update of $\boldsymbol\theta$ and $\boldsymbol\Sigma$. This is particularly convenient in the MVN model because;

If $\mathbf Y = (\mathbf Y_1, \mathbf Y_2)$ has the MVN normal distribution, $\mathbf Y_1 | \mathbf Y_2 = \boldsymbol y_2$ is MVN and $\mathbf Y_2 | \mathbf Y_1 = \boldsymbol y_1$ is MVN. So the 'data imputing' step of the Gibbs sampler is particularly straightforward. 

The result is $\boldsymbol\theta^{(s)} ,  \boldsymbol\Sigma^{(s)}$ are a Gibbs sample from $p(\boldsymbol\theta , \boldsymbol\Sigma | \boldsymbol y_{obs} )$


## Generalized linear mixed eï¬€ects models; 11b

We did not work on "11b", but we built the foundation for this. In "10a" we talked about Generalized Linear regression Models when the response is something other than normal e.g., Poisson regression and Logistic regression(for binary responses). In 11a we talked about hierarchical regression. Our data are obtained by a two-stage sampling plan and there's a different regression model (possibly different regression parameters) within the different groups of data. What our "11b" unit would have been about is putting those ideas together. That is, hierarchical regression models for non-normal regression. This is called generalized linear mixed models GLMM.




## Improper priors

We never did any work with improper priors largely because we were following the text by Hoff and Hoff does not mess with improper priors at all. The Gelman text uses improper priors as a default.

There's a brief discussion in Hoff page 78. The idea is this; Recall that in the case of the normal distribution we could characterize the conjugate prior for $\sigma^2$ and $\theta$ by 

$\theta\sim$ Normal$(\mu_0, \sigma^2/\kappa_0)$

$\sigma^2 \sim$ inverse-gamma$(\nu_0/2, ~ \nu_0\sigma^2_0/2)$


$\sigma^2_0 =$ prior best guess at $\sigma^2$ and $\nu_0 =$ "sample size on which that's based". Also $\mu_0 =$ prior best guess at $\theta$ and $\kappa_0 =$ "sample size on which that's based." 

What happens if we let $\kappa_0 = 0$ and $\nu_0 = 0$. This is a logical question: "What if we have no prior information?"

If you look at a chi-square density with degrees of freedom = 0 you get $f(x) = c \times 1 / x$ for $x > 0$. This is not a probability density because although it's non-negative for all $x$, it does not have a finite integral.

Similarly consider a normal distribution and let the prior variance go to infinity. Suppose I said I want my prior density for $\theta$ (the mean of a normal distribution) to be $p(\theta) = 1$ for all $\theta$, i.e., uniform on $-\infty$ to $+\infty$. You can't do that! that's not a probability distribution because it's not finite! But if we do it anyway and say $p(\theta | y) = c \times p(\theta) p(y | \theta),$ it still works out. Because of this, this has become a popular way to do Bayesian statistics. To routinely employ noninformative priors and if the parameter space is unbounded they'll generally be improper priors.

Def: A prior density $p(\theta)$ is proper if $\int { p(\theta) d\theta } < \infty$. A prior distribution is improper if the prior density $p(\theta)$ has a divergent integral; $\int { p(\theta) d\theta }$ is not finite. However, you can still do this as long as the posterior density $p(\theta | y) = c \times p(\theta) p(y | \theta)$ as is a proper density.

The difficulty is that there's no guarantee that it is. There are certain models where improper priors lead to proper posteriors. There are other situations where they don't. It's not always so easy to know the difference.

**Example:** 

For most Generalized Linear Models, if we take the improper prior distribution $p(\beta) = constant$, generally the posterior distribution will still be proper and therefore lead to valid conclusions. Let's look at the Poisson regression example we did where $x =$ age of female bird and $y =$ number of offspring and see if this is the case.


Fitting the model with a normal(0,10) prior.

```{r message = FALSE, warning = FALSE}
# 52 birds, response is 'fledged' -- number of offspring
# Predictor variable is age in years (1 to 5)
fledged <- c( 3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 
  4, 7, 2, 2, 1, 1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 
  4, 1, 2, 5, 1, 2, 1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2)

age <- c(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 
   2, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 
   4, 4, 4, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1)

library(rstan)

# description of our model
y <- fledged  
n <- length(y)
X <- cbind(rep(1,n), age, age^2)
rownames(X) <- 1:n;  
colnames(X) <- c("x1","x2","x3")
n           <- dim(X)[1]
p           <- dim(X)[2]

stan_model <- "

  data{
   int<lower=0> n; 
   int<lower=0> p;
   int<lower=0> y[n]; 
   matrix[n,p] X;
  }

  parameters{
   vector[p] beta;
  }

  model{
   beta ~ normal(0, 10);  // prior on beta
   y ~ poisson_log(X*beta);
  }
"

data     <-  list(n=n, p=p, y=y, X=X);
fit_stan <- stan(model_code=stan_model, data=data, 
 chains=1, iter=5000, warmup=1000, refresh = 0) 


Results <- summary(fit_stan)$summary[1:3, 4:8];  
Results_normal_prior <- round(Results, 2)
```


<p>&nbsp;</p>

Let's fit another Bayesian model, but this time with the prior commented out. Think about what this model says: There is a probability distribution for $\boldsymbol y | \beta,$ there are numeric values of $y$ but there are no values of $\beta$ provided there is no prior distribution specified! What stan does in the absence of a prior distribution is assign a uniform distribution. In this case it's a uniform distribution on an unbounded space so it's an improper distribution. What is stan going to do? is it going to crash? Let's see!


```{r message = FALSE, warning = FALSE}
stan_model2 <- "

  data{
   int<lower=0> n; 
   int<lower=0> p;
   int<lower=0> y[n]; 
   matrix[n,p] X;
  }

  parameters{
   vector[p] beta;
  }

  model{                 // no prior specified   
   y ~ poisson_log(X*beta);
  }
"

data     <-  list(n=n, p=p, y=y, X=X);
fit_stan <- stan(model_code=stan_model2, data=data, 
 chains=1, iter=5000, warmup=1000, refresh = 0) 

Results   <- summary(fit_stan)$summary[1:3, 4:8];  
Results_improper_prior <- round(Results, 2)
```

It didn't crash. 

```{r}
Results_improper_prior; Results_normal_prior
```

They are pretty much the same. Using a vague but proper prior like Normal(0 , 100) and using an improper prior lead to identical conclusions. I would argue that the former is safer because this can happen; You have a model where an improper prior does not lead to a proper posterior but MCMC may still work. You'll get output, but it won't be meaningful output and you may not even know it.



<p>&nbsp;</p>

Cheers!







