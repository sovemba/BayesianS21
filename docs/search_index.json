[["index.html", "Bayesian Statistics lecture notes Lecture 1 Belief and Probability 1.1 Example: 1.2 Random Variables 1.3 Binomial distribution 1.4 Poisson distribution 1.5 Continuous Random Variables", " Bayesian Statistics lecture notes Chisom Onyishi 2021-06-12 Lecture 1 Belief and Probability This is a compilation of transcribed lectures notes from Ronald Neath, June 2021 class on BAYESIAN STATISTICS STATGR5224. The course materials follow mostly from A First Course in Bayesian Statistical Methods, Hoff(2009). The following notes, mostly transcribed from Neath(0503,2021) lecture, summarize sections(2.1-2.4) of Hoff(2009). I like to use the chat I will post lecture slides to Courseworks before 9am each class day I will share the slides as I talk and I will type in the chat as I talk My hope is that by typing in the chat as we go along I will sort of reign myself in from going too fast. Also, there’s a printed record of the chat which is nice Gelman is on the third edition and there’s no reason to be looking at the 2nd or 1st edition of Gelman. However, for this course, I recommend get your hands on the book by Peter D. Hoff (2009) “A First Course in Bayesian Statistical Methods.” I will occasionally reference stuff in the Gelman text but the Hoff text is what we’re gonna follow. If you access “Springer link” using the Columbia network you can get a free copy of Hoff’s book by legitimate means! Gelman’s book is also (legitimately) free online because he has posted it! Find a link on Prof Gelman’s Columbia web page! Again, lecture will follow Hoff’s text. We’re starting in Chapter 2 and we’ll finish it or come very close (maybe skipping a few things along the way). We’ll leave some time at the end to talk about a few things that aren’t in Hoff’s book e.g. Stan software (which we’ll introduce toward the end of the course). But we’ll be doing computing from the get-go. Your first assignment (due next week) (not posted yet) will involve computing. I like R, but you may use Python if you know it. The Hoff book has some examples given in R. You know all about probability from the ‘relative frequency’ interpretation. If I roll a die a gazillion times it will land on the 3-side one-sixth of those rolls \\(Pr(3) = 1/6\\). In Bayesian statistics we use probability more generally than that. We use probability in a way that’s consistent with the informal use of probability. E.g., the probability that there will be criminal charges brought on former president is x%. In a strict frequency-based sense it doesn’t even make sense to speak of ‘probability’ for something like this. In a Bayesian sense it does and that is because in Bayesian statistics we use a different (more general) interpretation of probability where the probability of an event represents our degree of belief in that event. In Bayesian statistics any statement about the world can have a probability attached to it. E.g., The probability that climate change is a hoax brought by China and the Deep State. Though we might say this is absurd, some might assign this a probability other than zero. The only rule we’ll have for probabilities in our course is that they be internally consistent i.e., coherent – that they follow the rules of probability (mathematical laws of probability). We will use probability as a measure of our degree of belief in a statement about the world. Forget about probability for a second and just think about this idea of Belief. Let \\(F\\) be a statement about the world \\(Be(F)\\) is our degree of belief in that statement as measured by a numerical value. What’s a reasonable set of requirements on \\(Be()\\) for it to be a reasonable belief function? The higher the value, the higher the degree of belief. Let’s make this more concrete by thinking about bets \\(Be(F) &gt; Be(G)\\) means prefer betting on \\(F\\) to betting on \\(G\\) \\(Be(F | H) &gt; Be(G | H)\\) means if we know \\(H\\) to be true we prefer betting on \\(F\\) to betting on \\(G\\) \\(Be(F | G) &gt; Be(F | H)\\) means if forced to bet on \\(F\\) we prefer to do it under the condition that \\(G\\) is true than that \\(H\\) is true. The following Axioms of belief have been proposed as a set of conditions that any rational belief function must satisfy B1. \\(Be(\\text{not } H | H) \\le Be(F|H) \\le Be(H | H)\\). If \\(H\\) is known to be true there is no other function that I have higher belief in than \\(H\\) itself, and there is no statement that I have lower belief in other than not \\(H\\). B2. \\(Be(F \\text{ or } G | H) \\ge Be(F | H), ~~ Be(F \\text{ or } G | H) \\ge Be(G | H)\\) B3. \\(Be(F \\text{ and } G | H)\\) can be derived from \\(Be(G | H)\\) and \\(Be(F | G \\text{ and } H)\\) We propose as Bayesian statisticians to use probability as our measure of belief. Probability has its own set of axioms. Here they are! P1. \\(0 \\le Pr(F | H) \\le 1\\), where \\(Pr(\\text{ not } H | H) = 0, ~~ Pr( H | H ) = 1\\) so that takes care of B1 P2. If \\(F\\) and \\(G\\) are disjoint events then \\(Pr(F \\text{ or } G | H) = Pr(F | H) + Pr(G | H)\\) P3. \\(Pr(F \\text{ and } G | H) = Pr(G | H) \\times Pr(F | G \\text{ and } H)\\) You can verify that any probability function that satisfies the axioms of probability also satisfies the axioms of belief. So using probability as a language for measuring our belief in statements about the world is justified. And that’s what we do in Bayesian statistics. 1.1 Example: \\(H_j =\\) the event that { randomly selected person is in quartile \\(j\\) of income } \\(j = 1, 2, 3, 4\\) \\(H_1 =\\) { lower \\(25\\%\\) }, \\(H_4\\) = { upper \\(25\\%\\) } Let \\(E\\) = event that { randomly selected person has college degree } From survey data (a very large survey; the General Social Survey for a particular year). We have \\(Pr(E | H_j)\\) for each of \\(j = 1, 2, 3, 4\\) \\(.11 + .19 + .31 + .53 = 1.14\\). You might think uh oh. However, not uh oh at all. These numbers aren’t expected to add to \\(1.\\) These numbers don’t add to anything particularly meaningful. Using Bayes rule, we can obtain \\(Pr(H_j | E)\\) for each \\(j = 1, 2, 3, 4\\) and those had better add to \\(1\\). Let’s do \\(Pr(H_3 | E)\\); the probability that a person is in the 3rd quartile of income (between \\(50\\)th and \\(75\\)th percentile) given that they have a college degree. Using Bayes’ rule \\[ \\begin{aligned} Pr(H_3 | E) &amp;= \\frac{Pr(H_3 \\text{ and } E)}{Pr(E)}\\\\ &amp;=\\frac{Pr(H_3)Pr(E | H_3)}{Pr(E)}\\\\ &amp;= \\frac{Pr(H_3)Pr(E | H_3)}{Pr(H_1 \\text{ and } E) + Pr(H_2 \\text{ and } E) + Pr(H_3 \\text{ and } E) + Pr(H_4 \\text{ and } E)}\\\\ &amp;= \\frac{.25 \\times .31}{0.28} = \\frac{0.0775}{0.28} = 0.272 \\end{aligned} \\] \\[ \\begin{aligned} Pr(H_1 \\text{ and } E) = Pr(H_1 ) Pr(E | H_1) = .25 \\times .11\\\\ Pr(H_2 \\text{ and } E) = Pr(H_2 ) Pr(E | H_2) = .25 \\times .19\\\\ Pr(H_3 \\text{ and } E) = Pr(H_3) Pr(E | H3) = .25 \\times .31\\\\Pr(H_4 \\text{ and } E) = Pr(H_4) Pr(E | H_4) = .25 \\times .53 \\end{aligned} \\] \\(Pr(E) =\\) the sum of these four products. I get \\(Pr(E) = 0.285\\) This is a problem in Bayesian inference! Did you think this was just a fun little probability exercise? No no no no no, this was serious business. This was our first real Bayesian learning problem. I tell you I have a randomly selected person from this survey. What is your belief about their income? You think they’re in 1st, 2nd, 3rd or 4th quartile? Your belief is { \\(.25 , .25 , .25, .25\\) }. Now I tell you they have a college degree so you will update your belief! You will update your belief which is measured by a probability which is updated using Bayes rule. Bayesian inference: is the discipline of updating our belief about the world based on further observation of the world. If we know they have college degree our belief is not { \\(.25, .25, .25, .25\\) } anymore it’s skewed more toward the higher income groups { \\(.09, .17, .27, .47\\) }. The \\(H_k\\)’s in this set-up are usually ‘states of nature’ and the \\(E\\) in this set-up is the observed data 1.2 Random Variables In Bayesian statistics a random variable is any numerical quantity whose value is uncertain that includes things like experimental results (before the experiment is conducted) survey results (before the sample is taken). But it also includes model parameters states of nature. Let \\(Y\\) be a random variable \\(\\mathcal{Y}\\) is the set of possible values. If the set of possible values is a countable set { \\(y_1, y_2, \\ldots\\) }, then \\(Y\\) is a discrete random variable. We can compute \\(Pr(Y = y)\\) for any value of \\(y\\). We’ll define the pdf \\(p(y) = Pr(Y = y)\\). \\(Y\\) is the random variable, \\(y\\) is a possible realized value for \\(Y\\). Note we are using the pdf (density) terminology even for a discrete r.v. (Hoff, 2009). If you know the pdf you know \\(p(y) = Pr(Y = y)\\) for every possible value of \\(y\\) then you know the whole probability distribution. Two key properties for the pdf of a discrete r.v. \\(0 \\le p(y) \\le 1\\) They sum to \\(1\\) The two most important discrete probability distributions are the Binomial and Poisson distributions. Here are their definitions: 1.3 Binomial distribution \\[ Pr(Y=y|\\theta) = \\texttt{dbinom}(y,n,\\theta)=\\left(\\begin{array}{l}n\\\\y \\end{array}\\right)\\theta^y(1-\\theta)^{n-y} \\] \\(Y\\) counts the number of successes in \\(n\\) independent trials where the probability of success on each trial is \\(\\theta\\). Then \\(Pr(Y = y | \\theta)\\) is the probability of \\(y\\) successes in \\(n\\) trials = probability of \\(y\\) successes and \\(n-y\\) failures. Well the probability of \\(y\\) successes followed by \\(n-y\\) failures is \\(\\theta^y \\times (1-\\theta)^{n-y}\\). But \\(Pr(Y = y| \\theta)\\) is the probability of ANY possible sequence of \\(y\\) successes and \\(n-y\\) failures. There are “\\(n\\) choose \\(y\\)” such sequences; there are “\\(n\\) choose \\(y\\)” ways to arrange a sequence of \\(y\\) successes and \\(n-y\\) failures. Each sequence has the same probability \\(\\theta^y \\times (1-\\theta)^{n-y}\\). The probability that it’s one of these sequences is the sum of those probabilities \\(\\left(\\begin{array}{l}n\\\\y\\end{array}\\right)\\theta^y(1-\\theta)^{n-y}\\). We’ll use this dbinom notation for the binomial probability function which is also the R function to calculate these! Calculating binomial probabilities in R is easy. Suppose \\(n = 60\\) and \\(\\theta = .20\\). This is the binomial distribution; the probability distribution for number of successes in \\(60\\) trials where the success probability is \\(0.20\\). 1.4 Poisson distribution \\[ Pr(Y = y|\\theta) = \\texttt{dpois}(y,\\theta) = e^{-\\theta} \\frac{\\theta^y}{y!} \\] \\(\\theta =\\) mean (expected value of \\(Y\\)). \\(Y\\) counts the number of events. \\(\\theta\\) is the expected number. \\(y = \\{0, 1, 2, \\ldots\\}\\) In R dpois does this! Let’s do a Poisson distribution with expected value of \\(12\\) (will look not so different from that binomial distribution. That’s the Poisson distribution with expected value of \\(12\\). They’re pretty similar. But careful on plots like these. When we make comparative plots like this we should take care to put them on the same scale! You can see that the binomial distribution assigns higher probabilities close to the expected value whereas the Poisson distribution gives more probability mass farther away from the expected value. par(mfrow = c(1,2)) y &lt;- 0:30 p &lt;- dbinom(y, size = 60, prob = 0.2) plot(y, p, pch=19, main = &quot;Binomial(60, 0.2)&quot;, ylim = c(0, 0.13)) segments(y,0,y,p) p &lt;- dpois(y, 12) plot(y, p, pch = 19, main = &quot;Poisson(12)&quot;, ylim = c(0, 0.13)) segments(y,0,y,p) Figure 1.1: Binomial and Poisson Distributions They’re applicable to different situations. The binomial distribution is most appropriate when we have a fixed number of trials. However, if you’re not clear on what \\(n\\) and \\(\\theta\\) should be but you know what \\(n \\times \\theta\\) should be then you can’t do Binomial\\((n, \\theta)\\) but you can do Poisson\\((n\\times\\theta)\\). If there’s no upper bound then the binomial distribution is not appropriate (you can’t have \\(150\\) successes in \\(100\\) trials - in this case there’s no upper bound, and a Poisson model is more appropriate). 1.5 Continuous Random Variables Where the set of possible values is not a countable set and it’s, say, the whole real line. \\(F(y)\\) is the cumulative probability up to and including \\(y\\) \\(F(y) = Pr(Y \\le y)\\). If I know the cdf (cumulative distribution function) of a random variable, I know the whole distribution. Because I can find \\(Pr( a &lt; Y \\le b) = F(b) - F(a).\\) If \\(Y\\) is a discrete random variable it’s cdf is a step function. If \\(F\\) is a monotone continuous function then \\(Y\\) is a continuous random variable. In that case (maybe) there exists a function \\(p()\\) such that for any set \\(A,\\) \\(Pr(Y \\in A) = \\int_A { p(y) dy}.\\) \\(p(y)\\) is called the density function or pdf. If \\(p()\\) is the density function of a continuous random variable then \\(p(y) \\ge 0\\) for all \\(y\\) and \\(\\int_{-\\infty}^\\infty p(y) dy = 1\\). This is slightly different from the conditions on the pdf of a discrete random variable. If \\(p(y\\)) is the pdf of a continuous random variable, it is possible that \\(p(y) &gt; 1\\). \\(p(y)\\) does not represent a probability. The areas under the density curve are probabilities as long as that total area under the curve is \\(1\\) then it’s a valid probability distribution. The most important continuous distribution is the Normal distribution. Let’s look at the pdf and cdf of a Normal dist! In R these are computed by dnorm (for the density) and pnorm (for the cdf). 1.5.1 Example: Normal(\\(\\mu = 10.75, \\sigma=0.8\\)) par(mfrow = c(1,2)) mu &lt;- 10.75 sigma &lt;- 0.8 y &lt;- seq(7.5, 14, 0.05) # pdf p &lt;- dnorm(y, mu, sigma) plot(y, p, type = &quot;l&quot;, lwd = 2, main = &quot;Normalpdf(10.75, 0.8)&quot;) segments(y,0,y,p) #cdf F &lt;- pnorm(y, mu, sigma) plot(y, F, type = &quot;l&quot;, lwd = 2, main = &quot;Normalcdf(10.75, 0.8)&quot;) segments(y,0,y,F) Figure 1.2: Normal pdf and cdf The “bell curve” describes the density. The cdf is an S-curve. The mean of this distribution is \\(10.75\\). The density is symmetric about that value, the mode of this distribution is \\(\\mu = 10.75\\) and the median of this distribution is \\(\\mu = 10.75\\). Speaking of means and modes and medians, let’s define these things! The mean is the center of mass. It’s the weighted average of the possible values weighted by their probabilities. The mode is the value with the highest probability (or highest probability density for a continuous rv). The median is the \\(.50\\) quantile. The point where half the probability is to the left and half the probability is to the right. These are all measures of “location.” If we want a measure of the “spread” of a distribution we can use the standard deviation. Def: The variance of a random variable is defined by \\(\\text{Var}(Y) = E{ ( Y -E(Y) )^2 } = E(Y^2) - E(Y)^2\\) expected squared distance from the mean value. Take the square root of this (to back to the original scale) and call that quantity the standard deviation. We can also measure location and spread both using quantiles! For discrete distributions quantiles are weird. The \\(\\alpha\\)-quantile of a distribution is the value \\(y_{\\alpha}\\) such that \\(F(y_{\\alpha}) = \\alpha\\). Consider the normal cdf below. Where is the \\(.75\\) quantile? plot(y, F, type = &quot;l&quot;, lwd = 2, main = &quot;Normalcdf(10.75, 0.8)&quot;) abline( h = 0.75) abline(v = qnorm(0.75, mu, sigma)) Figure 1.3: Normal cdf with quantile In R, I can find normal quantiles using the qnorm function. With discrete distributions the cdf is a step function it jumps, so there’s not a uniquely defined point where the cdf curve passes through \\(.75\\) say. A \\(50\\%\\) probability interval for the distribution of \\(Y\\) is \\((y_{.25},~y_{.75}).\\) A \\(95\\%\\) probability interval is \\((y_{.025}, ~ y_{.975}),\\) where \\(y_\\alpha\\) represents \\(\\alpha\\) quantile of \\(y\\). "],["exchangeability.html", "Lecture 2 Exchangeability 2.1 Discrete joint distributions 2.2 Bayes’ rule and parameter estimation 2.3 Independent random variables 2.4 Exchangeability", " Lecture 2 Exchangeability The following notes, mostly transcribed from Neath(0504, 2021) lecture, summarize sections(2.5-2.8) of Hoff(2009). 2.1 Discrete joint distributions We will talk about joint probability distributions. \\(Y_1\\) and \\(Y_2\\) are two discrete random variables. We can define a joint probability density function (joint pdf) \\(p_{Y_1Y_2}(y_1, y_2) = Pr(Y_1 = y_1 \\text{ and } Y_2 = y_2)\\). We will also be interested in the marginal distributions e.g., the pdf of \\(Y_1\\) only. We will use subscripts on the \\(p\\) to be clear what density we’re talking about. I get the marginal of \\(Y_1\\) from the joint of \\((Y_1, Y_2)\\) by summing over the values that \\(Y_2\\) can take. We can also speak of the conditional density of \\(Y_2\\) given \\(Y_1\\) which follows immediately from the definition of conditional probability! Recall that \\(Pr( B | A ) = Pr(A \\text{ and } B) / Pr(A)\\). Conditional density of \\(Y_2 | Y_1=y_1\\) equal the joint density of \\(Y_1\\) and \\(Y_2\\) divided by the marginal of \\(Y_1\\). Given the joint density one can derive both marginals and both conditionals. Given the marginal of \\(Y_1\\) and the conditional of \\(Y_2\\) given \\(Y_1\\) one can construct the joint density of \\((Y_1, Y_2)\\). Given the two marginals, marginal of \\(Y_1\\), marginal of \\(Y_2\\), it is not possible to construct the joint density (knowing the row and columns totals doesn’t mean you know how to complete a contingency table). The joint distribution contains all the information the marginals do not! All the joint probabilities, all the marginal probabilities, and all possible conditional probabilities. We will not always use these subscripts on our \\(p\\)’s. We will just use a lower case \\(p\\) to denote a probability density. We’ll know the ‘density of what’ by the argument to the function! If we are talking about a pair of continuous random variables then the pdf’s really are probability DENSITY functions they’re not actually probabilities at all. We get probabilities from them by solving integrals. Proposition: \\(Y_1\\) and \\(Y_2\\) are two arbitrary rvs (random variables) with a joint distribution. That joint distribution is completely specified by the joint cdf. The ‘rigorous’ definition of a joint distribution starts with the joint cdf. The joint pdf is the function whose integral gives the joint cdf. Given a joint distribution of (\\(Y_1, Y_2\\)), the marginal distribution of \\(Y_1\\) is found by (summing over \\(Y_2\\) values in the discrete case) (in the continuous case it’s integrating over all the \\(Y_2\\) values). Although probability densities are not strictly probabilities, conditional densities can be defined analogously. The conditional density of \\(Y_2 | Y_1 = y_1\\) is found by joint density of \\(Y_1\\) and \\(Y_2\\) divided by marginal density of \\(Y_1\\). We can define a pair of rvs so that \\(Y_1\\) is discrete and \\(Y_2\\) is continuous. We still have \\(p(y_1, y_2) = p(y_1) p(y_2 | y_1)\\) and \\(p(y_1, y_2) = p(y_2) p(y_1 | y_2)\\). Probabilities are found by summing over values of the discrete one and integrating over values of the continuous one. 2.2 Bayes’ rule and parameter estimation Let \\(\\theta =\\) proportion of people who have a certain characteristic. E.g., public opinion poll: Approve of performance of President Biden? Yes or No? \\(\\theta\\) would be the proportion of people who would answer yes, \\(0 &lt; \\theta &lt; 1\\) Is \\(\\theta\\) discrete or continuous? If there are 300 million people in the population of interest then the possible values are 0, 1/300,000,000, 2/300,000,000, …, 299,999,999/300,000,000, 1. But that’s kind of stupid. It’s continuous. It makes most sense to treat \\(\\theta\\) as continuous, \\(0 &lt; \\theta &lt; 1\\). What if \\(Y =\\) number of people sampled who answer YES to “Approve of President Biden?” The population size is millions the sample size is hundreds? Here it does make sense to use a discrete probability model for \\(Y\\). \\(\\theta\\) is the value we want to know, \\(Y = y\\) is the quantity we’re able to observe. Statistical inference is making induction about \\(\\theta\\) based on observation of \\(Y = y\\). For a classical (non-Bayesian) analysis this requires a probability model for \\(Y\\) that depends on \\(\\theta\\). In Bayesian statistics it requires a joint probability model for \\((Y , \\theta)\\)! Where frequentist statistics treats \\(\\theta\\) as fixed (unknown but fixed) Bayesian statistics treats it as a random variable. The justification for this? Our belief about the value of \\(\\theta\\) is measured by a probability dist! Start with what we believe about \\(\\theta\\) before we’ve observed any data. Those beliefs determine the marginal distribution \\(p(\\theta)\\). We call this the prior distribution. Then we have (just as in non-Bayesian statistics) probability distribution for \\(Y\\) that depends on \\(\\theta\\). In Bayesian statistics we make it explicit that this is a CONDITIONAL probability, conditional on the value of \\(\\theta\\). After data are observed we want to update our beliefs about \\(\\theta\\). Use Bayes’ rule! \\(p(\\theta | y) = p(\\theta, y) / p(y) = p(\\theta) p(y | \\theta) / p(y)\\). This is called the posterior distribution (because it comes after observing the data). If \\(\\theta_a\\) and \\(\\theta_b\\) are two possible values of \\(\\theta\\) our belief in \\(\\theta_a\\) versus \\(\\theta_b\\) is defined by the ratio of their posterior probabilities (or posterior densities). That ratio depends on \\(p(\\theta_a) / p(\\theta_b)\\) ratio of ‘prior probabilities’ and \\(p(y | \\theta_a) / p(y | \\theta_b)\\) which is the ratio of the ‘likelihoods.’ Note! One need not calculate the marginal probability \\(p(y)\\) to calculate this ratio! That’s an important observation! Bayes’ rule tells us \\(p(\\theta | y) = p(\\theta) p(y | \\theta) / p(y)\\). Thinking of this thing as a function of \\(\\theta\\) we can write \\(p(\\theta | y) = c \\times p(\\theta) \\times p(y | \\theta)\\). Are you familiar with this “proportional to” symbol? You will be! We say \\(f(x) \\propto g(x)\\) if there exists a constant \\(c &gt; 0\\) such that \\(f(x) = c \\times g(x) ~\\forall ~x\\). \\(p(\\theta | y)\\) is a probability density. It integrates to 1. The function \\(p(\\theta) p(y | \\theta)\\) (a function of \\(\\theta\\)) integrates to something positive. Dividing by that positive thing would give us the normalized posterior density. Not dividing by that thing gives us an unnormalized posterior density which is just as good. Often solving for \\(p(y)\\) explicitly is impossible (or nearly impossible) :( But we’ll see in this course, turns out that doesn’t really matter the important information about the posterior is contained in the numerator of the posterior expression that is \\(p(\\theta ) p(y | \\theta)\\) which are not hard to solve :) 2.3 Independent random variables Continuing with probability review. \\(Y_1, Y_2, \\ldots , Y_n\\) are independent if their joint probability density factors into the product of their marginal probability densities. If these densities are specified conditionally on the value of \\(\\theta\\) (where \\(\\theta\\) is a random variable) we say \\(Y_1, Y_2, \\ldots, Y_n\\) are conditionally independent given \\(\\theta\\). If \\(Y_1, Y_2, \\ldots, Y_n\\) are generated from a common process, then the marginal densities of the \\(Y_i\\) (conditional on \\(\\theta\\)) are all the same. If \\(\\theta =\\) Biden’s true approval rate \\(Pr(Y_1 = 1 | \\theta) = Pr(Y_2 = 1 | \\theta) = … = \\theta\\). In such a case we’ll say \\(Y_1, Y_2, …,\\) are conditionally iid. 2.4 Exchangeability In Bayesian statistical models we generally assume conditional independence which does not imply independence it implies a different condition called exchangeability. \\[ Y_i= \\begin{cases} 1 \\text{ if subject answers yes }\\\\ 0 \\text{ if subject answers no (or doesn&#39;t answer) } \\end{cases} \\] We will pick 10 survey respondents out of very very many (thousands) at random \\(Y_i = 1\\) if the \\(i\\)th one answers yes. There are \\(2^{10} = 1024\\) possible realizations of \\((y_1, y_2, …, y_{10})\\). What’s the probability for each? Here are 3 of those 1024 possible sequences \\[ \\begin{array}{l} p(1, 0, 0, 1, 0, 1, 1, 0, 1, 1) = ?\\\\ p(1, 0, 1, 0, 1, 1, 0, 1, 1, 0) = ?\\\\ p(1, 1, 0, 0, 1, 1, 0, 0, 1, 1) = ?\\\\ \\end{array} \\] I’m thinking their probabilities should all be equal. Note that each of these sequences contains six \\(1&#39;\\)s and four \\(0&#39;\\)s. Does it seem reasonable to assume Pr(yes, yes, no) = Pr(yes, no, yes) = Pr(no, yes, yes)? I’d say it does! The technical term for this condition is exchangeability. \\(y_1, y_2, …, y_n\\) are a sequence of values then the probability of observing \\(Y_1 = y_1, Y_2 = y_2, … , Y_n = y_n\\) and the probability of observing \\(Y_1 = y_2, Y_2 = y_1\\), etc are the same. This is called exchangeability. Exchangeability holds in models where the subscript label contains no information about the outcome. Does exchangeability imply independence? NO. \\(Y_1, Y_2 , …\\) are conditionally independent given \\(\\theta\\), but unconditionally they’re not independent. Think about \\(Pr(Y_{10} = 1).\\) My answer to this would be whatever is my best guess of the overall success proportion \\(\\theta\\). What about \\(Pr(Y_{10} = 1 | Y_1 = Y_2 = … = Y_9 = 1)?\\) Suppose first probability is \\(a\\), second probability is \\(b\\). I don’t think we want \\(a = b\\)! In fact, \\(a&lt;b\\). What about conditionally on \\(\\theta\\), given that \\(\\theta\\) is the true success probability (for the population)? \\(Pr(Y_{10} = 1 | \\theta) \\approx \\theta?\\) yes \\(Pr(Y_{10} = 1 | Y_1, …, Y_9 , \\theta) \\approx \\theta?\\) yes These are 10 independent draws from a large population. Imagine a huge bowl of jelly beans, green ones and red ones. I know the proportion that are green is \\(\\theta.\\) \\(Pr(10\\)th pick is green | \\(\\theta) = \\theta,~ Pr(10\\)th pick is green | first \\(9\\) picks, \\(\\theta)\\) is about \\(\\theta\\). It’s a huge bowl so sampling without replacement is essentially the same thing as sampling with replacement. Given that 80% of this population is happy. If only 4 of the first 9 answered yes that doesn’t change my probability for the 10th, it’s still going to be .80. True or false: \\(Y_1\\) and \\(Y_2\\) and … and \\(Y_9\\) and \\(Y_{10}\\) are independent. We’re gonna see in a minute this statement is false. The true statement is \\(Y_1, Y_2, … , Y_9, Y_{10}\\) are CONDITIONALLY independent given \\(\\theta.\\) \\(Pr(Y_i = 1 | \\theta , Y_j,~ j \\neq i) = \\theta.\\) Probability of zero is \\(1-\\theta.\\) That’s the joint probability conditionally on \\(\\theta.\\) Unconditionally it’s… You take the conditional probability multiply by the marginal of \\(\\theta\\) integrate \\(\\theta\\) out of it! So we see \\(Y_1, Y_2, ... Y_n\\) are exchangeable. They are not iid because they’re not independent, because unconditionally on \\(\\theta,\\) \\(Y_{10}\\) is not marginally independent of \\(Y_1, Y_2, …, Y_9\\) because \\(Y_{10}\\) depends on \\(\\theta\\) and \\(Y_1, Y_2, …, Y_9\\) contain information about \\(\\theta\\). We do not have \\(a=b\\) above, but we do have exchangeability. That is, joint distribution is preserved under re-labeling of the subscripts (permutation of labels). Proposition: If \\(\\theta \\sim p(\\theta),\\) \\(Y_1, … Y_n | \\theta\\) are iid then \\(Y_1, …, Y_n\\) are exchangeable. The proof is straightforward it’s given in Section 2.7 or 2.8 of Hoff. The converse is also true. i.e., If \\(Y_1, …, Y_n\\) are an exchangeable sequence of rvs then there exists a prior distribution \\(\\theta\\) and conditional distribution \\(Y_i | \\theta\\) such that \\(Y_1, …, Y_n | \\theta\\) are iid \\(p(y_i | \\theta)\\) where \\(\\theta \\sim p(\\theta)\\). That result is known as di Finetti’s theorem. Why is this important? This means that any situation where modeling observed values as being exchangeable is appropriate then a Bayesian statistical analysis is also appropriate. "],["binomial-1.html", "Lecture 3 Binomial 3.1 Example - Exchangeable binary data 3.2 The beta distribution 3.3 Binomial distribution 3.4 Combining information 3.5 Prediction", " Lecture 3 Binomial The following notes, mostly transcribed from Neath(0504, 2021) lecture, summarize section 3.1 of Hoff(2009). 3.1 Example - Exchangeable binary data The quantity of interest is the proportion of women that rate themselves as generally happy. Denote this quantity as \\(\\theta\\), \\(0 &lt; \\theta &lt; 1\\). Our data is \\(n=129\\) women surveyed. \\[ Y_i = \\begin{cases} 1 \\text{ if } i\\text{th woman answered yes}\\\\ 0 \\text{ otherwise} \\end{cases} \\] \\(\\sum{y_i} =118\\)(number of \\(1\\)s). We describe our prior belief (before observing the data) by a prior probability distribution \\(p(\\theta)\\). In this example we’re saying \\(\\theta \\sim \\text{Uniform}(0, 1)\\) prior density \\(\\implies p(\\theta) = 1\\). Let \\(\\theta =\\) proportion of ALL women age 65+ who would answer yes. Binary means same thing as Bernoulli \\((1\\) with probability \\(\\theta\\), \\(0\\) with probability \\(1-\\theta)\\). Conditonal on \\(\\theta\\) the probability of any particular sequence of \\(0\\)s and \\(1\\)s is \\(\\theta^{\\sum{y_i}} \\times (1-\\theta)^{n-\\sum{y_i}}.\\) What do we know about \\(\\theta?\\) Our belief about \\(Y_1, …, Y_n\\) are determined by our beliefs about \\((1)~\\theta = \\sum_{i=1}^NY_i/N \\text{ and }~ (2) ~ p(y_1, \\ldots, y_n|\\theta)\\). The second one would be the bernoulli distribution! The first one is not resolved. What do we believe about \\(\\theta\\) prior to observing data? We know \\(0 &lt; \\theta &lt; 1.\\) Our belief about where between \\(0\\) and \\(1\\) is most likely to be is correctly described by some probability distribution on the interval \\([0, 1]\\). So Let’s say \\(\\theta \\sim \\text{Uniform}(0, 1).\\) This would say \\(Pr(a &lt; \\theta &lt; b) = b-a\\) for any \\(0 &lt; a &lt; b &lt; 1.\\) Okay fine. The prior density is \\(p(\\theta) = 1, ~ 0 &lt; \\theta &lt; 1.\\) Do Bayes’ rule to get the posterior \\(p(\\theta | y_1, y_2, \\ldots, y_n) =p(y | \\theta)p(\\theta)/p(y) = c \\times p(y | \\theta)\\). The denominator is constant because it doesn’t depend on \\(\\theta\\)! We have a general result here: Under a uniform prior distribution the posterior is proportional to the likelihood. Let’s proceed. The data are: \\(n = 129, ~118\\) answered Yes I am generally happy \\(11\\) did not. The probability of the specific sequence of values observed \\((\\)conditional on the value of \\(\\theta)\\) i.e., \\(p(y|\\theta)= \\theta^{118} \\times (1 - \\theta)^{11}.\\) Note here \\(y_1, y_2, …, y_{129}\\) represents a particular sequence of 118 1s and 11 0s. So we have \\(p(\\theta | y) = c \\times p(y | \\theta) = c \\times \\theta^{118} \\times (1 - \\theta)^{11}\\) so a plot of \\(p(y | \\theta)\\) versus \\(\\theta\\) tells us what the posterior distribution looks like! n &lt;- 129 sum.y &lt;- 118 theta &lt;- seq(0, 1, .001) x = expression(theta) y = expression(p(y*&#39;|&#39;*theta)) plot(theta, theta^sum.y * (1-theta)^(n-sum.y), type=&quot;l&quot;, lwd=2, col=&quot;red&quot;, ylab=y, xlab = x); grid(); abline(v = 118/129) #118 yeses -- the mode Figure 3.1: unnormalized posterior Our belief about \\(\\theta\\) based on 118 yeses out of 129 asked is; \\(0.80 &lt; \\theta &lt; 1\\) Note: The shape here is correct for the posterior distribution, but the values on the y-axis is not as we are missing the constant factor in \\(p(\\theta | y) = c \\times p(y | \\theta).\\) This curve does not integrate to 1 so it’s not a probability density but it does uniquely define a probability density by its shape and THAT probability density is indeed the posterior of \\(\\theta.\\) If the prior is uniform, we get a similar conclusion as a frequentist statistician although the interpretations are not the same. 3.2 The beta distribution The Beta distribution is a probability distribution on \\([0, 1].\\) Hey! \\(\\theta\\) lives on \\([0, 1]\\). We can use the beta distribution as our prior for \\(\\theta\\) ! Remember the Gamma function? For integer - value \\(x,~\\text{Gamma}(x) = (x-1)!\\) For noninteger it effectively interpolates between those factorials . 3.2.1 Properties of the beta distribution It has two parameters \\(a\\) and \\(b, ~a\\) describes tendency toward 1, \\(b\\) describes tendency toward 0, mean of this distribution is \\(a / (a+b)\\) variance is \\(ab/(a+b+1)(a+b)^2.\\) If \\(a\\) and \\(b\\) are big, the beta distribution is has a low variance (has a high peak). If \\(a\\) and \\(b\\) are not big then the beta distribution is more spread out. Mode is the value that maximizes our posterior belief! So if you want to report a single - number best guess at \\(\\theta\\) this would be a sensible choice. A plot of the posterior, \\(\\theta | y_1, …, y_n \\sim \\text{Beta}(119, 12)\\) post &lt;- dbeta(theta, sum.y+1, n-sum.y+1) plot(theta, post, type=&quot;l&quot;, lty=1, lwd=2, ylab=&quot;&quot;, xlab = x); lines(theta, rep(1, length(theta)), col=&quot;gray&quot;, lwd=2); legend(&quot;topleft&quot;, inset=.05, lwd=2, col=c(&quot;gray&quot;,&quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;));grid() Figure 3.2: the posterior density, with uniform(0,1){ Beta(1,1) } prior also shown This curve is the same as the first one above. But this one is a probability density. It has area under curve = 1. The light gray curve (the flat line) is the uniform density. That’s our prior belief about \\(\\theta\\). So we purposely chose a prior that did not make any super strong assumptions. We don’t want to pretend we know more than we do. With a uniform prior, the posterior will depend mostly on the data. In the prior our belief about \\(\\theta\\) was weak, in the posterior we have pretty strong beliefs! Based on the data observed! Our posterior belief about \\(\\theta\\) is entirely contained in ratios \\(p(\\theta_a | y_1,\\ldots,y_n) / p(\\theta_b | y_1,\\ldots,y_n)\\), because you can do this for any two values, if you have a way to compare any two possible values of \\(\\theta\\) you have a way to do complete inference about \\(\\theta\\). \\[ \\frac{p(\\theta_a | \\boldsymbol{y})}{p(\\theta_b | \\boldsymbol{y})} = \\bigg(\\frac{\\theta_a}{\\theta_b}\\bigg)^{\\sum y_i} \\bigg(\\frac{1-\\theta_a}{1-\\theta_b}\\bigg)^{n-\\sum y_i} \\frac{p(\\theta_a)}{p(\\theta_b)} \\] Look what happens to this ratio. It depends on the data \\(y_1, …, y_n\\) only through their sum, \\(\\sum{ y_i }\\). Any posterior belief about \\(\\theta\\) depends on the data only through the total value \\(\\sum{y_i}\\), This means that \\(\\sum{y_i}\\) is a sufficient statistic for the bernoulli sampling model (likelihood) and uniform prior. Another way to say this; If you observe \\(y = (1, 0, 0)\\) and I observe \\(y = (0, 0, 1)\\) and we use the same prior and likelihood our inference will be exactly the same because these two data sets have the same \\(\\sum{y_i}\\). If inference about \\(\\theta\\) depends on the data only through the total value let’s just call that our data. Notation: \\(Y_i = 1\\) or \\(0\\) for the \\(i\\)th observation, \\(Y = Y_1 + … + Y_n\\). Then conditionally on \\(\\theta, ~~ Y | \\theta \\sim \\text{Binomial}( n, \\theta)\\). 3.3 Binomial distribution \\(Pr(\\sum{Y_i} = y) = \\binom{n}{y} \\times \\theta^y \\times (1-\\theta)^{n-y}\\) Without the \\({n \\choose y}\\) we have the probability of any particular sequence of \\(y\\) 1s and \\(n-\\)y 0s. But what we want is just the probability of one of those. \\(E(Y | \\theta) = n \\times \\theta\\) var\\((Y | \\theta) = n \\times \\theta \\times (1-\\theta)\\) 3.3.1 Posterior inference for a binomial sampling model So we have a prior density \\(p(\\theta)\\) we have a sampling model \\(Pr(Y = y | \\theta) =\\) dbinom\\((y , n , \\theta)\\). From these we use Bayes’ rule to compute the posterior! \\[ \\begin{aligned} p(\\theta \\mid y) &amp;=\\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} \\\\ &amp;=\\frac{\\left(\\begin{array}{l} n \\\\ y \\end{array}\\right) \\theta^{y}(1-\\theta)^{n-y} p(\\theta)}{p(y)} \\\\ &amp;=c(y) \\theta^{y}(1-\\theta)^{n-y} p(\\theta) \\end{aligned} \\] \\(c(y) = {n \\choose y} / p(y)\\),which is a function of \\(y\\). Can we evaluate this? The answer is yes. But it would involve hard calculus. But we don’t need to. The key point here is: \\(c(y)\\) does not depend on \\(\\theta, \\implies p(\\theta | y) \\propto \\theta^y (1-\\theta)^{n-y} p(\\theta).\\) We know the shape of the posterior density! we don’t know it exactly i.e, the scale but we know the shape. Let’s take the uniform prior distribution \\(p(\\theta) = 1\\). We now have \\(c(y)\\) in an explicit form involving gamma functions( 03a slide 15 ) therefore we have the exact form of \\(p(\\theta | y)\\). We have just demonstrated \\(\\theta | y \\sim \\text{Beta}( y+1, n-y+1)\\). Recall what we noted about the Beta\\((a, b)\\) dist. \\(a\\) measures a tendency toward 1, \\(b\\) measures a tendency toward \\(0 ~(\\)or more accurately, \\(a/(a+b)\\) and \\(b/(a+b))\\). We’ve seen that this posterior results from the “independent Bernoulli’s” model it also results from the Binomial model. Thus confirming the sufficiency result. Intuitively speaking, you can make a note of who answered yes and who answered no in the happiness question but it doesn’t matter because all we need is the sum of the yeses. Uniform\\((0, 1) = \\text{Beta}(1,1)=\\) dbeta\\((\\theta, 1, 1) = 1, ~~ \\forall ~ 0 &lt; \\theta &lt; 1\\). We’ve demonstrated that if \\(\\theta \\sim \\text{Beta}(1,1), ~~ Y | \\theta \\sim \\text{Binomial}(n, \\theta),\\) then \\(\\theta | y \\sim \\text{Beta}(1 + y , 1 + n - y)\\). It looks like the Beta\\((1, 1)\\) prior combines with \\(y\\) successes and \\((n-y)\\) failures to give a Beta posterior with \\(1+y\\) and \\(1+n-y\\) parameters \\(\\implies p(\\theta|y) \\sim \\text{Beta}(a+y, b+n-y) =\\) dbeta\\((\\theta, a+y, b+n-y).\\) 3.3.2 Conjugacy Key result: If \\(\\theta \\sim \\text{Beta}(a, b)\\) and \\(Y | \\theta \\sim \\text{Binomial}(n , \\theta)\\) then \\(\\theta | y \\sim \\text{Beta}(a + y, b + n-y)\\). This situation where the posterior distribution is of the same family of distributions as the prior is called conjugacy. Specifically we say the conjugate prior for a Binomial sampling model is the Beta distribution. Conjugate distributions are characterized by nice math. Things work out nicely for conjugate priors, things cancel out etc. Priors should reflect our prior belief. If that’s well described by a conjugate distribution GREAT otherwise no reason to restrict attention to conjugate priors in Bayesian analyses. 3.4 Combining information The prior \\(p(\\theta)\\) is what we believed before we saw the data. The likelihood \\(p(y|\\theta)\\) contains what we learn from the data. Therefore the posterior should in some sense combine what we knew before with what we just learned. It does! Posterior combines prior and likelihood; \\(p(\\theta | y) = c \\times p(\\theta) \\times p(y | \\theta)\\). Let’s demonstrate this for the “beta-binomial” model where \\(\\theta | y \\sim \\text{Beta}( a + y , b + n-y).\\) \\[\\begin{aligned} \\text{Prior mean } = E(\\theta) &amp;= \\frac{a}{a+b}\\\\ \\text{Sample average is } &amp; y/n \\end{aligned}\\] \\[\\text{Posterior mean } = \\frac{a+y}{a + b + n} = \\frac{a+b}{a+b+n} \\cdot\\frac{a}{a + b} + \\frac{n} {a+b+n} \\cdot \\frac{y}{n}\\] What this equation demonstrates is that posterior expectation = weighted average of prior expectation and sample average. \\(a + b\\) is the weight given to the prior, \\(n\\) is the weight given to the data. Data contributes \\(y\\) successes and \\(n-y\\) failures (in \\(n\\) trials). \\(a\\) plays the exact same role as \\(y, ~b\\) plays the exact same role as \\(n-y.\\) Thus we can say the Beta\\((a, b)\\) prior contributes \\(a\\) “successes” and \\(b\\) “failures” in \\(a + b\\) “prior trials.” So the bigger is \\(n\\) relative to \\(a+b\\) the less the prior matters. 3.4.1 Example Happiness example with a uniform prior. \\(a + b = 2, ~ n = 129\\). Thus the posterior distribution mostly depends on the data. # Reproduce Figure 3.4 in Hoff (2009) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(2,2)) theta &lt;- seq(0, 1, .01) a &lt;- 1; b &lt;- 1; n &lt;- 5; y &lt;- 1; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b1, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) legend(&quot;topright&quot;, inset=.05, lty=1, lwd=2,col=c(&quot;gray&quot;,&quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), bty=&quot;n&quot;) a &lt;- 3; b &lt;- 2; n &lt;- 5; y &lt;- 1; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b2, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) a &lt;- 1; b &lt;- 1; n &lt;- 100; y &lt;- 20; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b3, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) a &lt;- 3; b &lt;- 2; n &lt;- 100; y &lt;- 20; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b4, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) Figure 3.3: Beta posterior distributions under two diﬀerent sample sizes and two different prior distributions. Look across a row to see the effect of the prior distribution, and down a column to see the eﬀect of the sample size In the top row \\(n=5\\) prior matters! In the bottom row, \\(n=100\\) prior does not matter. 3.5 Prediction Suppose we have \\(n+1\\) trials \\(Y_1, Y_2, …, Y_n, Y_{n+1}.\\) We will observe \\(n\\) of them and make our best prediction for the \\((n+1)\\)st. The model says \\(\\{Y_1, …, Y_n, Y_{n+1} | \\theta\\} \\sim \\text{iid Binary}(\\theta)\\). Under Bayesian, we have a distribution for our predictions. \\[ \\begin{aligned} Pr(\\tilde Y = 1|y_1,...,y_n)=E(\\theta|y_1,...,y_n)&amp;=\\frac{a+\\sum y_i}{a+b+n}\\\\ Pr(\\tilde Y = 0|y_1,...,y_n)=1-E(\\theta|y_1,...,y_n)&amp;=\\frac{b+\\sum(1-y_i)}{a+b+n} \\end{aligned} \\] \\(a\\) and \\(b\\) are called hyperparameters. The predictive distribution depends on hyperparameters which are known, and on the data which is observed. If the predictive distribution did not depend on \\(y_1, …, y_n\\) that would suggest \\(\\tilde Y\\) was independent of \\((Y_1, .., Y_n)\\). Also note that the predictive distribution does not depend on any unknown quantities. The Beta(1,1) prior \\(\\equiv\\) Uniform(0, 1) is equivalent to two prior observations (one success one failure). Does this seem right? Is this a ‘noninformative’ prior? Wouldn’t it be even more uninformative to take \\(a = b = 0.5?\\) What does this density look like? teta &lt;- seq(0, 1, 0.01) prior &lt;- dbeta(teta, 0.5, 0.5) plot(teta, prior, type = &quot;l&quot;, xlab = x, ylab = expression(p(theta)), lwd = 2) Figure 3.4: Beta(0.5, 0.5) density It is more uninformative to take Beta(0.5, 0.5) So although the 1 extra success and 1 extra failure in the Beta(1,1) feels like something kind of noninformative to some people it leads to a more sensible prediction as we will illustrate here; Under uniform prior, the posterior of the predictive probability for \\(\\tilde Y\\) has the posterior mean \\((y+1)/(n+2)\\). Suppose \\(y=0\\) then the posterior mean becomes \\(1/(n+2)\\) which is reasonable. Suppose \\(y=n\\), then it is \\((n+1)/(n+2)\\) which is also reasonable. Whereas the posterior mode of \\(y/n\\) doesn’t make sense as a predictive probability. Just because you observed \\(n\\) successes in \\(n\\) trials doesn’t mean you are 100% certain the next trial will be a success! Hence the posterior mean makes more sense as a predictive probability than the posterior mode and those two extra trials coming from the uniform prior seem sensible also. "],["CI.html", "Lecture 4 Confidence Intervals 4.1 Confidence Intervals 4.2 How do we compute intervals?", " Lecture 4 Confidence Intervals The following notes, mostly transcribed from Neath(2021) lecture, summarize sections(3.1.2 and 3.2) of Hoff(2009). In classical statistics you assume a probability model for a population of values based on the observation of a sample from that population. You estimate parameters of that probability model. A point estimate represents our best guess at that value, but we know it’s not right. Same thing is true with Bayesian inference. We can formulate a prior distribution, collect data, and compute the posterior using Bayes’ rule and from that take the posterior mean, \\(E(\\theta | y)\\), or posterior mode, \\(p(\\theta | y)\\), or posterior median as a point estimate and that value would represent our best guess. But if for example \\(\\theta\\) is a probability, \\(0 &lt; \\theta &lt; 1\\), whatever our best guess for \\(\\theta\\) is, our confidence in its correctness is 0%. \\(E(\\theta | y), p(\\theta | y)\\) are point estimates of \\(\\theta\\), NOT \\(\\theta\\) itself! There is a true \\(\\theta\\)! When we speak of \\(\\theta\\) as a random variable and give it a prior distribution and update that using the data and from that get a posterior distribution. We only involve probability in this discussion because we don’t know what \\(\\theta\\) is. Let’s use the percent of ‘happy women’ as an example. \\(\\theta =\\) proportion of women age 65+ who would call themselves “generally happy” \\((0 &lt; \\theta &lt; 1).\\) In classical statistics we resolve this issue (point estimate is our best guess but we know it’s wrong) by computing a standard error as well as a point estimate and reporting a 95% confidence interval. In Bayesian inference we’re gonna do the same thing. 4.1 Confidence Intervals Observe the data \\(Y = y\\). Compute \\(l(y) &lt; \\theta &lt;u(y)\\), \\(l\\) and \\(u\\) stand for lower and upper. The interval \\([l(y), u(y)]\\) is a 95% (Bayesian) confidence interval if \\(Pr[ l(y) &lt; \\theta &lt; u(y) | Y = y ] = 0.95\\). This is not the usual (frequentist) definition of 95% confidence interval. What does “95% confident” mean to a frequentist? It doesn’t mean true with 95% probability because a frequentist doesn’t allow “probability” to be interpreted like that. To a frequentist the statement \\(l(y) &lt; \\theta &lt; u(y)\\) is either true or false and we don’t know which it is but we know that prior to the data collection based on the likelihood(sampling model \\(p(y | \\theta)\\)), this is true: \\(Pr[ l(Y) &lt; \\theta &lt; u(Y) | \\theta] \\ge .95\\) for all values of \\(\\theta ~ (\\)this is a random interval, also notice conditioning on \\(\\theta)\\) So before we collect the data, we agree on a data collection mechanism, we agree on a formula for computing the interval based on the observed data, and we can say that there is a 95% chance that the interval will contain the value of \\(\\theta\\) and a 5% chance that is misses. Once you observe \\(Y = y\\) and you plug this data into your conﬁdence interval formula \\([l(y), u(y)]\\), then \\[ \\operatorname{Pr}(l(y)&lt;\\theta&lt;u(y) \\mid \\theta)=\\left\\{\\begin{array}{ll} 0 &amp; \\text { if } \\theta \\notin[l(y), u(y)] \\\\ 1 &amp; \\text { if } \\theta \\in[l(y), u(y)] \\end{array}\\right. \\] This statement makes sense: \\(Pr[ l(Y) &lt; \\theta &lt; u(Y) | \\theta] = 0.95.\\) We observe the data \\(Y= y\\) and \\(y\\) is a number and suppose our CI is \\(l(y) = .58, ~u(y) = .63\\). Then to say: \\(Pr[ .58 &lt; \\theta &lt; .63 ] = 0.95\\) ??????? That doesn’t make any sense! After we have observed the data, \\(\\theta\\) is either between 0.58 and 0.63 or it’s not. But to a Bayesian this makes perfect sense. Again, What does the frequentist 95% confidence mean? When a frequentist says: I’m 95% confident that \\(.58 &lt; \\theta &lt; .63\\) it means this interval [.58, .63] was computed using a method that would, in the long run, give correct results 95% of the time. This particular instance may be one of the 95% of cases we’re right or one of the 5% of cases where we get it wrong and we don’t which it is. We summarize this by saying we’re 95% confident in this particular interval. But that “95% confidence” statement to a frequentist is a verbal shorthand for this much more complicated interpretation. To a Bayesian 95% confidence is a perfectly direct statement. 4.2 How do we compute intervals? That’s easy! (conceptually easy). I have a probability distribution for \\(\\theta,\\) \\(p(\\theta | y)\\) is its density. I find numbers \\(l\\) and \\(u\\) such that \\(Pr( \\theta &lt; l | y) + Pr( \\theta &gt; u | y) = .05\\). That means there’s a 95% chance that \\(\\theta\\) is between those limits and thus \\([l, u]\\) give a 95% Bayesian confidence interval. There is not a unique solution. The most straightforward thing to do is to put \\(\\alpha/2\\) probability to to the left of \\(l\\) and \\(\\alpha /2\\) probability to the right of \\(u,\\) leaving \\(1 - \\alpha\\) probability in the middle (eg. for 95% confidence, \\(\\alpha = .05\\)). Our notation for quantiles is \\(\\theta_p, ~ \\theta_a, ~ \\theta_q\\) means \\(Pr[ \\theta &lt; \\theta_q | y ] = q\\) and \\(Pr[ \\theta &gt; \\theta_q | y ] = 1-q\\) (I’m assuming our parameter spaces are all continuous). 4.2.1 Example 2 successes in 10 trials. Find a 95% posterior interval for \\(\\theta\\) (posterior interval, Bayesian confidence interval, credible interval, credible set, posterior probability interval) all mean the same thing. \\[\\theta | Y = 2 \\sim \\text{Beta}(1+2, 1+8)\\] a &lt;- 1; b &lt;- 1; # prior n &lt;- 10; y &lt;- 2; # data qbeta(c(.025, .975), a+y, b+n-y) ## [1] 0.06021773 0.51775585 These quantiles are 0.06 and 0.52, respectively, and so the posterior probability that \\(\\theta \\in [0.06, 0.52]\\) is 95%. theta &lt;- seq(0, 1, .001) plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;, ylab=expression(p(theta*&#39;|&#39;*y)), lwd=2, xlab = expression(theta)) CI &lt;- qbeta(c(.025, .975), a+y, b+n-y) abline(v=CI, lty=2, lwd=2) legend(&quot;topright&quot;, inset=.05, lty=c(1,2), lwd=2, legend=c(&quot;Posterior dist&quot;, &quot;95% CI&quot;)) Figure 4.1: Posterior interval There is 95% posterior probability that \\(.06 &lt; \\theta &lt; .52\\). A thing to note about this interval. Putting .025 in the left tail and .025 in the right tail is not the only way to get .95 in the middle. In this case we could move the lower bound down a little and get to move the upper bound down by a little more. Some people (quite reasonably) would like to report the shortest possible 95% interval. In general the equal-tail interval that we just described is not that. There is a way to get a shorter interval. The term for this is the HPD(highest posterior density) interval. In most problems it does not make a huge difference (I think I heard there’s an R package that makes it pretty easy). But ok, look into this if you’re interested. In our case we’ll make it our standard to always report equal-tailed posterior probability intervals \\(\\alpha/2\\) to the left \\(\\alpha/2\\) to the right leaving \\(1-\\alpha\\) in the middle. "],["poisson-model.html", "Lecture 5 Poisson model 5.1 Posterior inference for the Poisson model 5.2 Posterior predictive distribution 5.3 Example: Birth rates 5.4 Explaining the parameters of the gamma distribution", " Lecture 5 Poisson model The following notes are transcribed from Neath(2021) lecture which summarizes Sections(3.1.2 and 3.2) of Hoff(2009). Suppose \\(Y =\\) number of facebook friends your neighbor has. There are a large number of people in the world. Some of them are your neighbors facebook friends and some are not. \\[ Y_i = \\begin{cases} 1 \\text{ if the }i\\text{th person in the world is a facebook friend of your neighbors}\\\\ 0 \\text{ otherwise} \\end{cases} \\] \\(Y_i \\sim ~\\)Binomial(size = number of people in the world). probability = proportion of those people who are facebook friends with your neighbor. This is not a good probability model for this variable, because the size is too big and the probability is too small. What works better in cases like this is the Poisson distribution. Sample space is { \\(0, 1, 2, ...\\) }. probability function is \\(e^{-\\theta} \\times \\theta^y / y!, ~ y \\in \\{~0, 1, 2,...\\},\\) \\(\\theta\\) is the mean, i.e., expected value of \\(Y\\). The Poisson model is a good first thing to think of and first thing to try for count data. A property of the Poisson distribution is; variance = \\(\\theta\\) = mean. When the variance is a little bigger than the mean, Poisson probabilities may still match the observed proportions reasonably closely but when variance is a lot bigger than the mean Poisson distribution is not such a good model. Let \\(Y_i =\\) count for \\(i\\)th unit, { \\(i = 1, 2, …, n\\) }. If \\(Y_i | \\theta\\) are iid Poisson(\\(\\theta\\)) then the joint probability of \\((y_1, y_2, …, y_n)\\) depends on the data only though the total value \\(\\sum{y_i}\\). We say the \\(\\sum{y_i}\\) is a sufficient statistic. If you observe the data \\(n=3, y =\\) c( 1 , 2, 3 ) and I observe \\(n = 3,~ y =\\) c( 4, 1, 1 ) our inference about \\(\\theta\\) will be exactly the same. It is also a property of Poisson random variables that sums of independent Poisson random variables are distributed as Poisson. Posterior of \\(\\theta\\) satisfies \\[ p(\\theta | y_1, ..., y_n) = c_0 \\times p(\\theta) \\times p(y|\\theta)=c \\times p(\\theta) \\times \\theta^{\\sum{y_i}}e^{-n \\theta } \\] We’ll work with a conjugate prior because it makes things simple. Looking at \\(p(\\theta)\\times\\theta^{\\sum{y_i}} e^{-n\\theta }\\), what could we plug in for \\(p(\\theta)\\) such that \\(p(\\theta | y)\\) will have the same parametric form? well \\(p(\\theta | y)\\) is gonna have a \\(\\theta^{\\text{something}}\\) and a something\\(^{\\theta}\\) so the prior can have those things too! What probability distribution for \\(\\theta &gt; 0\\) allows \\(\\theta^{\\text{something}}\\) and something\\(^{\\theta}\\)? That would be the gamma distribution \\(= \\tilde c\\times\\theta^{a-1} e^{-b \\theta}\\). In this parameterization \\(b\\) is the rate parameter, \\(1/b\\) is the scale parameter. 5.1 Posterior inference for the Poisson model For our prior on \\(\\theta\\) we’ll say \\(\\theta \\sim\\) gamma\\((a, b)\\). How do you choose a and b? In general, when considering the question of how to choose a prior look at what the prior leads to in terms of the posterior. Not by peeking at the data though, just by studying the model. i.e., Let’s look at what posterior the gamma\\((a, b)\\) prior will lead to and then come to the question of what \\(a\\) and \\(b\\) should be. Key Result: If \\(\\theta \\sim \\text{gamma}(a,b)\\) and \\(Y_1,\\ldots,Y_n | \\theta \\sim\\) iid Poisson\\((\\theta)\\), then \\((\\theta | Y_1 = y_1, \\ldots, Y_n = y_n) \\sim \\text{gamma}(a+\\sum y_i, b+n)\\). What do \\(a\\) and \\(b\\) contribute to the posterior? \\(a\\) contributes analogously to \\(\\sum{y_i}\\) which is the observed total count \\(b\\) contributes analogously to \\(n\\) which is the number of observed counts. Another way to say this: The data consist of \\(n\\) observations with average value \\(\\sum{y_i} / n,\\) the prior contributes (effectively) \\(b\\) observations with average value \\(a/b\\). So in the absence of genuine prior information about \\(\\theta\\) just make sure \\(b\\) is small relative to \\(n\\) and it won’t matter very much. Make \\(b\\) small relative to \\(n\\) and make \\(a/b\\) or \\((a-1)/b\\) your prior “best guess.” Student question: What’s the difference, definition wise, between \\(n\\) observations and \\(b\\) observations? Ans: The \\(n\\) observations are real data. The \\(b\\) observations are not data they’re “pseudo-data” or “prior observations” The value of \\(\\sum{y_i}\\) is determined by the data. The values of \\(a\\) and \\(b\\) are chosen by us. 5.2 Posterior predictive distribution Now suppose we have \\(n+1\\) observations and our job is to predict the \\((n+1)\\)st. Let \\(\\tilde Y\\) represent this \\((n+1)\\)st observation. The conditional distribution of \\(\\tilde Y|Y_1 = y_1, …, Y_n = y_n\\) is called the posterior predictive distribution. Posterior because it depends on the observed data. The distinction between “estimation” and “prediction”; If it’s an observable (but as yet unobserved) quantity it’s a prediction. Like \\(\\tilde Y;\\) the \\((n+1)\\)st data point that just hasn’t been observed yet. Versus an unknown parameter like \\(\\theta\\) is an unobservable quantity. Inference about \\(\\theta\\) would be called estimation. \\[ \\begin{aligned} p(\\tilde y | y_1,...,y_n) &amp;= \\int p(\\tilde y,\\theta | y_1,...,y_n)d\\theta\\\\ &amp;=\\int \\texttt{dpois}(\\tilde y, \\theta) \\texttt{dgamma}(\\theta, a+\\sum y_i, b+n)d\\theta \\end{aligned} \\] A general result: “A gamma mixture of Poissons is negative binomial.” One way the negative binomial distribution arises is; \\(y =\\) number of failures before the \\(n\\)th success(in Bernoulli trials). This is the same negative binomial distribution as that but not the same motivation and note the “\\(n\\)” parameter need not be an integer. \\(E (\\tilde Y) = E [ E(\\tilde Y | \\theta) ] = E(\\theta).\\) Simalarly, \\(\\text{Var}[ \\tilde Y ] = \\text{Var}[ E(\\tilde Y | \\theta) ] + E[\\text{Var}(\\tilde Y | \\theta) ] = \\text{Var}( \\theta ) + E (\\theta )\\) but the posteriors! 5.3 Example: Birth rates We are comparing two populations. There’s strong evidence that there is a difference between the two populations (see slide 22). a &lt;- 2 ; b &lt;- 1 # prior parameters n1 &lt;- 111; sy1&lt;-217 # data in group 1 # posterior mean ( a+sy1 )/( b+n1 ) ## [1] 1.955357 # posterior mode ( a+sy1-1)/(b+n1 ) ## [1] 1.946429 # posterior 95% CI qgamma( c (0.025 , 0.975) , a+sy1 , b+n1 ) ## [1] 1.704943 2.222679 n2 &lt;- 44 ; sy2&lt;-66 # data in group 2 # posterior mean ( a+sy2 )/( b+n2 ) ## [1] 1.511111 # posterior mode ( a+sy2-1)/(b+n2 ) ## [1] 1.488889 # posterior 95% CI qgamma( c (0.025 , 0.975) , a+sy2 , b+n2 ) ## [1] 1.173437 1.890836 5.4 Explaining the parameters of the gamma distribution Poisson \\[ f(x|\\theta)=e^{-\\theta}\\,\\frac{\\theta^x}{x!} \\quad \\text{ for } x\\in\\{0,1,...,\\},\\quad \\theta&gt;0\\\\ \\] Gamma \\[f(x) = \\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx}, \\quad x &gt; 1\\\\[0.6cm]\\] In the density function we have \\(x^a(\\exp(-bx).\\) If this was an exponential distribution for the waiting time to the next event where the event is occurring according to a Poisson process with rate \\(b\\) it would just be \\(e^{-bx}\\) and would have a mean waiting time of \\(1/b,\\) where \\(b\\) is the rate. Why is \\(a\\) called the shape parameter? If we change the variable from waiting time till next event to waiting time till \\(a\\)th event we get a gamma\\((a, b)\\) distribution. In what sense does \\(a\\) govern the shape of a gamma dist? The answer is: When \\(a = 1\\) we have an exponential dist and in the exponential dist \\(a &lt; 1\\) means asymptote at 0, \\(a=1\\) means mode at 0 that’s a severely right-skewed dist. When \\(a &gt; 1\\), the mode is at \\((a-1) / b\\). And the bigger \\(a\\) is the farther away from zero this mode is AND the more bell-shaped is the density curve. Property of the gamma\\((a, b)\\) dist is; If \\(a\\) is big, it’s well-approximated by a normal distribution. To the original question in what sense is \\(b\\) a “rate” and in what sense does \\(a\\) determine “shape?” \\(b\\) is a rate in the “Poisson-process-connection to exponential distribution” sense. The closer \\(a\\) is to zero the more right-skewed is the gamma distribution, the bigger \\(a\\) is the more bell-shaped (normal) is the gamma distribution. In the Poisson-gamma Bayesian problem the bigger is \\(\\sum{y_i}\\) the more data we’ve observed the more events we’ve observed. A general result in Bayesian inference is: The more data you have the more normal will be the posterior dist. In the case of the Poisson-gamma model having “more data” doesn’t just mean \\(n\\) increasing. It actually depends on observing lots of events so “lots of data” in the Poisson gamma model is not just big “exposure” it’s lots of events also which is determined by \\(\\sum{y_i}\\). The more events we’ve observed the more data we have the more normal is our posterior dist hence \\(\\sum{y_i}\\) (along with \\(a\\) in the prior dist) make the posterior more and more bell-shaped or normal so they drive the shape of the posterior toward the normal dist. The rate is there to normalize things. Observing 50 events in 10 days does not mean the same thing ( about \\(\\theta\\) ) as observing 50 events in 2 days. So the \\(n\\)(the rate) is defined with respect to a unit of time which determines how many units of data we’ve observed so in that sense \\(n\\) (along with \\(b\\)) define the rate parameter in our gamma posterior. "],["monte-carlo.html", "Lecture 6 Monte Carlo 6.1 Example: birth rate 6.2 The Monte Carlo method 6.3 Example: Numerical evaluation 6.4 Posterior inference for arbitrary functions 6.5 Example: Log-odds 6.6 Example: Functions of two parameters 6.7 How many Monte Carlo samples are needed?", " Lecture 6 Monte Carlo The following notes, mostly transcribed from Neath(0510,2021) lecture, summarize sections(4.1 and 4.2) of Hoff(2009). 6.1 Example: birth rate Let’s continue with the example from last time in which \\(\\theta_1 =\\) true birthrate (children per woman) among children with no college degree (less than bachelors degree), and \\(\\theta_2\\) is the true birthrate among women with a bachelor’s degree or higher. A birthrate of 2 would be a society with a steady population. In our data, among those women with less than bachelors degrees there were \\(n_1 = 111\\) such women with a total of 217 children\\((\\sum y_{i,1} = 217)\\), so that’s \\(1.95\\) children per woman. Among those women with bachelors or higher \\(n_2 = 44,~ \\sum{y_{i,2}} = 66\\), so that’s \\(1.50\\) children per woman. The prior we used was independent gamma\\((2,1),\\) with a prior mean of \\(2\\)(representing steady growth). \\(b =\\) prior sample size of \\(1.\\) The data sample sizes were \\(44\\) and \\(111\\) so the prior won’t have a lot of influence on our inference (which is usually how we want it). We are interested in the posterior probability that \\(\\theta_1 &gt; \\theta_2,\\) \\(\\theta_1\\) and \\(\\theta_2\\) are independent gammas in this posterior distribution. \\(Pr(\\theta_1 &gt; \\theta_2 | y) = Pr( \\theta_2 &lt; \\theta_1 | y) = \\int{Pr(\\theta_2 &lt; \\theta_1 | \\theta_1, y) p(\\theta_1 | y) d\\theta_1}\\); An application of “a law of total probability.” Note on using R: dgamma returns a gamma density value pgamma returns a gamma cdf value qgamma returns a gamma quantile rgamma simulates a random draw from a gamma dist integrand &lt;- function(x) {pgamma(x, 68, 45) * dgamma(x, 219, 112)} integrate(f=integrand, lower=0, upper=Inf) ## 0.9725601 with absolute error &lt; 6.5e-06 Our posterior belief that the birthrate is lower among women with bachelors degrees is 0.973. This was cool! We got the right answer. Exact answers are cool, But this really only worked because it was such a “nice” problem. The gamma distribution is nice. Having a dimension of only 2 made it nice. Numerical integration is great in low-dimensional problems but does not work so well in high dimensional ones. Today (the subject of Chapter 4) we take up a general approach that does continue to work in high dimensional problems. This method, known as Monte Carlo approximation, is based on random sampling, and its implementation does not require a deep knowledge of calculus or numerical analysis. 6.2 The Monte Carlo method Why is the method of Monte Carlo called Monte Carlo? It’s a resort on the French Riviera with casinos (people go gambling there). Monte Carlo was the code name for a project by allied scientists in World War II. \\(p(\\theta | y_1, …, y_n)\\) or just \\(p(\\theta | y)\\) for short is the posterior density function which uniquely identifies the posterior distribution so we say \\(p(\\theta | y)\\) defines the posterior. In the Monte Carlo method we let \\(\\theta^{(s)}\\) (we’re using a superscript rather than subscript because we use subscripts to indicate a position in a vector parameter) be a random draw from the posterior distribution for \\(s = 1, …, S\\). For this to work \\(S\\) must be reasonably large. In statistics we have an unknown population distribution but if we observe data that are a random sample from that distribution we can use the data to make inference about the population distribution. When we do Monte Carlo approximations to a posterior probability distribution we’re doing “an inference problem within the inference problem.” If \\(S\\) is a big enough number a histogram of the \\(\\theta^{(s)}\\) will be a decent representation of the probability density \\(p(\\theta | y).\\) \\(\\theta_2 \\sim \\text{gamma}(68,45)\\) (the posterior for the birth rate among college grads in our earlier example) # Reproduce Figure 4.1 in Hoff par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) a &lt;- 2; b &lt;- 1; n &lt;- 44; sum.y &lt;- 66; #simulating draws theta.sim1 &lt;- rgamma(10, a+sum.y, b+n) theta.sim2 &lt;- rgamma(100, a+sum.y, b+n) theta.sim3 &lt;- rgamma(1000, a+sum.y, b+n) theta &lt;- seq(.75, 2.25, .01) p.theta &lt;- dgamma(theta, a+sum.y, b+n) xlim=c(.75, 2.25); ylim=c(0, 2.5) op &lt;- par(mfrow=c(2,3)) hist(theta.sim1, freq=F, right=F, xlim=xlim, main=&quot;&quot;, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;, breaks=20) lines(theta, p.theta, lwd=2, col=&quot;gray&quot;) hist(theta.sim2, freq=F, right=F, xlim=xlim, main=&quot;&quot;, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;, breaks=20) lines(theta, p.theta, lwd=2, col=&quot;gray&quot;) hist(theta.sim3, freq=F, right=F, xlim=xlim, main=&quot;&quot;, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;, breaks=20) lines(theta, p.theta, lwd=2, col=&quot;gray&quot;) plot(theta, p.theta, type=&quot;l&quot;, lty=1, lwd=2, col=&quot;gray&quot;, xlim=xlim, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;) lines(density(theta.sim1), lty=2, lwd=2) plot(theta, p.theta, type=&quot;l&quot;, lty=1, lwd=2, col=&quot;gray&quot;, xlim=xlim, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;) lines(density(theta.sim2), lty=2, lwd=2) plot(theta, p.theta, type=&quot;l&quot;, lty=1, lwd=2, col=&quot;gray&quot;, xlim=xlim, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;) lines(density(theta.sim3), lty=2, lwd=2) Figure 6.1: Successive Monte Carlo approximations to the density of the gamma(68,45) distribution, along with the true density function for comparison. Row 1: histogram with true density for comparison. The gray curve is the true gamma(68, 45) density Row 2: “kernel density estimates” with true density for comparison As Monte Carlo sample size increases the agreement between the empirical distribution of the random samples and the target distribution gets better and better. “Empirical distribution” is a fancy name for the histogram. As if the population consisted of \\(S\\) different values each with equal probability and those values are the simulated draws. That’s what we mean by “empirical distribution.” This is a toy problem (we know the answer). A toy problem is the name we give to exercises where we use a method like Monte Carlo despite Monte Carlo not being necessary (the exact answer is readily available). We study the performance of a method like Monte Carlo in problems where the answer is known(i.e., toy problems) so that we have confidence in the method when applied to problems where the exact answer isn’t known (non-toy problems). The “empirical distribution approaches the target distribution” argument can be made a bit more formal by the Law of Large Numbers. LLN: As your flip a coin more and more times the proportion of observed heads will converge to the true heads probability with probability 1. Or more generally as the sample size increases the sample mean converges to the population mean with probability 1. As Monte Carlo sample size (number of simulations) increases i.e., as \\(S \\rightarrow \\infty\\) sample average converges to the posterior mean sample variance converges to the posterior variance sample proportions converge to posterior probabilities. The empirical distribution of \\(\\{\\theta^{(1)},...,\\theta^{(S)}\\} \\rightarrow p(\\theta|y_1,...,y_n)\\) A point where the proportion of \\(\\theta^{(s)} &lt;\\) this point is \\(\\alpha\\) converges to the \\(\\alpha\\)-quantile of the posterior distribution Unlike other numerical approximations, with Monte Carlo we know “If we run the computer long enough we’ll eventually get the right answer”(i.e., desired level of precision). 6.3 Example: Numerical evaluation Let’s keep studying this toy problem. This will demonstrate what happens when we go from 10 samples to 100 samples to 1000 samples – we get a better and better approximation to the true (a+sum.y) / (b+n) # true posterior mean ## [1] 1.511111 # Monte Carlo means c(mean(theta.sim1),mean(theta.sim2),mean(theta.sim3)) ## [1] 1.653986 1.522461 1.508636 pgamma(1.75, a+sum.y, b+n) # Pr(theta &lt; 1.75 | y) ## [1] 0.8998286 # Monte Carlo approximations c(mean(theta.sim1 &lt; 1.75), mean(theta.sim2 &lt; 1.75), mean(theta.sim3 &lt; 1.75)) ## [1] 0.700 0.890 0.907 # true quantile qgamma(c(.025, .975), a+sum.y, b+n) ## [1] 1.173437 1.890836 # Monte Carlo quantiles c(quantile(theta.sim1, c(.025, .975)), quantile(theta.sim2, c(.025, .975)), quantile(theta.sim3, c(.025, .975))) ## 2.5% 97.5% 2.5% 97.5% 2.5% 97.5% ## 1.276212 2.144389 1.229435 1.923309 1.190959 1.895615 We can also look at trace plot of successively better approximations to the quantity of interest # Reproduce Figure 4.2 theta.mc &lt;- theta.sim3 S &lt;- length(theta.mc) theta.bar &lt;- cumsum(theta.mc) / (1:S) #running average Prob.hat &lt;- cumsum(theta.mc &lt; 1.75) / (1:S) quant.hat &lt;- rep(NA, S) for(s in 1:S) { quant.hat[s] &lt;- quantile(theta.mc[1:s], .975) } par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,3)) plot(1:1000, theta.bar, type=&quot;l&quot;, xlab=&quot;No. samples&quot;, ylab=&quot;cumulative mean&quot;, main=&quot;&quot;) abline(h=(a+sum.y)/(b+n), col=&quot;gray&quot;, lwd=2) plot(1:1000, Prob.hat, type=&quot;l&quot;, xlab=&quot;No. samples&quot;, ylab=&quot;cumulative ecdf at 1.75&quot;, main=&quot;&quot;) abline(h=pgamma(1.75, a+sum.y, b+n), col=&quot;gray&quot;, lwd=2) plot(1:1000, quant.hat, type=&quot;l&quot;, xlab=&quot;No. samples&quot;, ylab=&quot;cumulative .975-quantile&quot;, main=&quot;&quot;) abline(h=qgamma(.975, a+sum.y, b+n), col=&quot;gray&quot;, lwd=2) Figure 6.2: Estimates of the posterior mean, Pr(theta &lt; 1.75|y) and the 97.5% posterior quantile as a function of the number of Monte Carlo samples. Horizontal gray lines are the true values. The behaviour of the sample quantile is weird. A couple samples push us way off and then we get back to the right thing. 6.4 Posterior inference for arbitrary functions A cool thing about the Monte Carlo method is this; suppose I know the distribution of \\(\\theta,\\) the posterior \\(p(\\theta | y)\\). The quantity I’m interested in is \\(\\gamma = g(\\theta)\\) for some possibly complicated function \\(g.\\) Ex: If \\(\\theta\\) is a probability \\(\\theta /(1 -\\theta)\\) is an odds \\(\\log( \\theta / (1-\\theta) ) = \\text{logit}(\\theta)\\) is a log-odds. We may be interested in posterior inference about the log odds! Monte Carlo makes this super easy. If \\(\\theta^{(s)}\\) is a random draw from \\(p(\\theta | y)\\) and \\(\\gamma^{(s)} = g( \\theta^{(s)} )\\) then \\(\\gamma^{(s)}\\) is a random draw from \\(p(\\gamma | y)\\) the posterior of \\(\\gamma\\) regardless of how complicated a function \\(g(\\theta)\\) may be. This is particularly important when \\(\\theta\\) is high dimensional. If I can simulate random draws from a probability distribution, I can learn anything I want about that probability distribution (as long as I’m patient enough to let the simulation run). The algorithm: sample \\(\\theta^{(1)} \\sim p(\\theta|y_1,...,y_n)\\), compute \\(\\gamma^{(1)} = g(\\theta^{(1)})\\); sample \\(\\theta^{(2)} \\sim p(\\theta|y_1,...,y_n)\\), compute \\(\\gamma^{(2)} = g(\\theta^{(2)})\\); etc sample \\(\\theta^{(S)} \\sim p(\\theta|y_1,...,y_n)\\), compute \\(\\gamma^{(S)} = g(\\theta^{(S)})\\); with each draw sampled independently 6.5 Example: Log-odds This is from a General Social Survey in 1998. Respondents in a General Social Survey were asked if they agreed with a Supreme Court ruling that prohibited state or local governments from requiring the reading of religious texts in public schools. Let \\(\\theta =\\) true proportion of population that would answer YES. Of the 860 respondents in the sample(860 trials) there were \\(y=441\\) successes. Given a Uniform\\((0, 1)\\) prior the posterior distribution of \\(\\theta\\) is Beta\\((1 + 441, 1 + 860 - 441) =\\) Beta\\(( 442, 420 )\\). This a pretty tight posterior distribution centered around \\(a/a+b= 442/862 =0.513\\). Using the Monte Carlo algorithm described above, we can obtain samples of the log-odds, \\(\\gamma= \\log[\\theta/(1−\\theta)]\\), from both the prior distribution and the posterior distribution of \\(\\gamma\\). # Example: Inference about log-odds rm(list=ls()); set.seed(20210506); a &lt;- 1; b &lt;- 1; theta.sim.prior &lt;- rbeta(1000, a, b) gamma.sim.prior &lt;- log( theta.sim.prior / (1-theta.sim.prior) ) n &lt;- 860; y &lt;- 441; theta.sim.post &lt;- rbeta(1000, a+y, b+n-y) gamma.sim.post &lt;- log( theta.sim.post / (1-theta.sim.post) ) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,2)) plot(density(gamma.sim.prior), xlab=expression(gamma), ylab=expression(p(gamma)),xlim=range(gamma.sim.prior),main=&quot;&quot;) plot(density(gamma.sim.post), xlab=expression(gamma), ylab=ylab, xlim=range(gamma.sim.prior),main=&quot;&quot;) lines(density(gamma.sim.prior), col=&quot;gray&quot;) Figure 6.3: Monte Carlo approximations to the prior and posterior distributions of the log-odds. theta.sim.prior is just runif(1000). A uniform prior is flat on [0, 1]. What sort of distribution does this imply for \\(\\log( \\theta / (1-\\theta) )\\)? Well it’s centered at 0 (probability = 0.5 means odds=1 means log-odds = 0). It’s symmetric. It’s not a normal distribution but it is a roughly bell shaped curve. What about the posterior? Well the posterior distribution of \\(\\theta \\sim\\) Beta\\(( 442, 420 )\\) is very tightly clustered around the value \\(\\theta = 0.513\\) or so. So the posterior distribution of the log odds is very tightly clustered around \\(0\\) or just \\(&gt; 0\\) or so. Why are we doing this? We are doing this now as a toy problem to prepare ourselves for later in the course when we encounter analytically intractable and high-dimensional posterior distributions. What we’ll see in these problems is posterior moments? Posterior probabilities? Posterior quantiles? Good luck with anything other than Monte Carlo. 6.6 Example: Functions of two parameters Returning to the birthrates example, \\(\\theta_1 =\\) birthrate for women with less than bachelor’s \\(\\theta_2 =\\) birthrate for women with at least a bachelor’s degree. \\(\\theta_1|y_{i,j}\\sim \\text{gamma}(219,112)\\) and \\(\\theta_2|y_{i,j}\\sim \\text{gamma}(68,45)\\) There are a variety of ways to describe our knowledge about the difference between \\(\\theta_1\\) and \\(\\theta_2\\). For example, we may be interested in the numerical value of \\(Pr(\\theta_1 &gt; \\theta_2|y_{i,j})\\), or in the posterior distribution of \\(\\gamma = \\theta_1/ \\theta_2.\\) Both of these quantities can be obtained with Monte Carlo sampling: This expression: \\(\\frac{1}{S}\\sum_{s=1}^S \\mathbf{1}(\\theta_1^{(s)}&gt;\\theta_2^{(s)})\\) is kinda sorta similar to: #{ \\(\\theta^{(s)} \\le c\\) }\\(/S\\). We can say “average of indicator functions” or # { successes } divided by \\(S\\) mean the same thing. # Return to the birthrates example a &lt;- 2; b &lt;- 1; n1 &lt;- 111; sum.y1 &lt;- 217; n2 &lt;- 44; sum.y2 &lt;- 66; theta1.sim &lt;- rgamma(10000, a+sum.y1, b+n1) theta2.sim &lt;- rgamma(10000, a+sum.y2, b+n2) mean(theta1.sim &gt; theta2.sim) ## [1] 0.9726 Our Monte Carlo approximation to \\(Pr( \\theta_1 &gt; \\theta_2 | y )\\) is 0.9726. The “exact value” is also 0.9726. This is not typical. Now suppose we want to do inference about the ratio \\(\\gamma = \\theta_1/ \\theta_2.\\) In your 4203/5203 (probability) class you may have studied bivariate transformations of random variables where you calculate the back-transformation then solve the “Jacobian” matrix (of partial derivatives). We could solve this problem that way OR we can get an arbitrarily precise numerical approximation with no difficult math required whatsoever. Take a random sample of : \\(\\theta_1^{(s)}, ~\\theta_2^{(s)}\\) and let \\(\\gamma^{(s)} = \\theta_1^{(s)} / \\theta_2^{(s)}\\) for \\(s = 1, … S.\\) gamma.sim &lt;- theta1.sim / theta2.sim hist(gamma.sim, freq=F, right=F, breaks=30, xlab=expression(gamma), ylab=ylab, main = &quot;&quot;) lines(density(gamma.sim)) Here is the posterior distribution of \\(\\gamma = \\theta_1 / \\theta_2.\\) It’s mostly \\(&gt; 1\\). I show a histogram and kernel density estimate on the same axes. This is not a theoretical density curve it’s the empirical one with \\(S=10000\\) and it’s pretty smooth. The posterior distribution of \\(\\gamma = \\theta_1 / \\theta_2.\\) quantile( gamma.sim, c(.025, .975) ) ## 2.5% 97.5% ## 0.9960863 1.7111331 The .025 and .975 quantiles are .996 and 1.715. I have 95% confidence that the birth rate for non-college grads is between .996 and 1.715 times that of women with college degree. 6.7 How many Monte Carlo samples are needed? We use Monte Carlo as an approximation tool. The Monte Carlo sample size does not determine posterior uncertainty. The posterior distribution \\(p(\\theta | y)\\), the posterior mean \\(E(\\theta | y)\\) and posterior variance \\(\\text{Var}(\\theta | y)\\) are what they are based on the prior distribution \\(p(\\theta)\\), and the data we observed. If the prior distribution is “vague” and the data set is not huge, the posterior variance \\(\\text{Var}(\\theta | y)\\) will be big. If the posterior distribution is approximately normal a 95% confidence will be approximately \\(E(\\theta|y) \\pm 2 \\sqrt{ \\text{Var}(\\theta|y) }\\), because with the normal distribution, mean \\(\\pm\\) 2 SD’s contains 95% of the distribution. None of that has anything to do with Monte Carlo. Monte Carlo comes in when the posterior distribution is not known analytically and we will approximate it by simulating draws from the posterior distribution and using that empirical distribution to approximate the posterior. Using Monte Carlo to approximate posteriors in Bayesian inference is “an inference problem within the inference problem.” We have uncertainty about \\(\\theta\\) that’s measured by Var\\((\\theta | y)\\), the posterior variance. How we use Monte Carlo is; \\(E(\\theta | y)\\) is only our best guess for \\(\\theta\\), it’s not the truth (it’s an estimator). What if we can’t even calculate \\(E(\\theta | y)\\) exactly, then we will approximate it using Monte Carlo. The bigger is the Monte Carlo sample size the better will be our approximation to \\(E(\\theta | y)\\). But the uncertainty about \\(\\theta\\) can’t be eliminated. In the birth rate example, our data consist of 111 women without college degree 44 women with college degree 155 women total. However precisely, we can estimate \\(\\theta_1\\) and \\(\\theta_2\\) based on these sample sizes is what it is. We can’t improve that by increasing Monte Carlo sample size. As to the question of: How big a Monte Carlo sample to take? One formal approach to this is; Take a big enough Monte Carlo sample size to estimate \\(E(\\theta | y)\\) to a desired level of precision that the mean of the empirical density (the dashed curve in figure 6.1) is within epsilon of the mean of the population curve (gray curve) which is unknown. This solution is an Intro Stat problem! Let \\(S\\) equal Monte Carlo sample size. In terms of Monte Carlo error, \\(\\bar \\theta\\)(the sample mean of the simulated draws) has approximately a Normal distribution because \\(S\\) is big. Its mean is \\(E(\\theta|y)\\). Its variance is Var\\((\\theta|y)/S\\). A 95% Monte Carlo CI for \\(E(\\theta|y)\\) is \\(\\bar \\theta \\pm 2\\sqrt{ \\text{Var}(\\theta|y) / S }\\). Now suppose we want the Monte Carlo sample size big enough so that the Monte Carlo error \\(|\\bar \\theta - E(\\theta|y )|\\) is &lt; .01 with 95% confidence. Then we need \\(2 \\sqrt{ \\text{Var}(\\theta|y) / S } &lt; .01\\) (could use \\(1.96 = \\texttt{qnorm(0.975)}\\) in place of 2). Solve \\(2 \\sqrt{\\text{Var}(\\theta|y) / S} &lt; .01\\) for \\(S\\). Of course the posterior variance is unknown so use an estimate in its place an estimate such as the sample variance in its place. Here the sample is !Monte Carlo sample! not the actual data!. In general we don’t really do this. We use \\(S = 1000\\) or \\(S = 10000\\). But this is still a thing we should know about. May be the basis for a homework 2 exercise. "],["predictive.html", "Lecture 7 Predictive 7.1 Sampling for predictive distribution 7.2 Example: Let \\(D = \\tilde Y_1 - \\tilde Y_2\\) 7.3 Posterior predictive model checking 7.4 Posterior predictive model checking", " Lecture 7 Predictive The following notes, mostly transcribed from Neath(0511,2021) lecture, summarize sections(4.3 and 4.4) of Hoff(2009). Last class, we talked about using simulated draws \\(\\theta^{(s)} \\sim p(\\theta | y)\\) to form a Monte Carlo approximation to the posterior distribution \\(p(\\theta | y)\\). Today we do Monte Carlo simulation for the posterior predictive distribution. A predictive distribution is characterized by two features: (1) known quantities are conditioned on (2) unknown quantities are integrated out. For example, if we integrate \\(\\theta\\) out of \\(\\int{ p(\\tilde y | \\theta) p(\\theta ) d\\theta } = p(\\tilde y)\\), we call this a prior predictive distribution; A predictive distribution that integrates over unknown parameters but is not conditional on observed data. A result of probability theory: If the sampling model is Poisson\\((\\theta)\\) and the prior is \\(\\theta \\sim \\text{gamma}(a, b)\\) then the prior predictive is \\(\\text{NegBinom}(a, b)\\); an overdispersed count distribution. The Poisson distribution has the property that \\(\\text{Var}(Y) = E(Y)\\). For a negative binomial distribution, \\(\\text{Var}(Y) &gt; E(Y)\\). A predictive distribution that accounts for the information available in the sample data \\((y_1, …, y_n) = y\\) is called a posterior predictive distribution and in the Poisson-gamma model, because the gamma is a conjugate for the Poisson and the posterior on \\(\\theta\\) is gamma\\((a+ \\sum{y_i},b + n)\\) the posterior predictive distribution (PPD) for an \\((n+1)\\)st observation is \\(\\text{NegBinom}(a + \\sum{y_i}, b + n)\\). R has dnbinom, pnbinom, rnbinom functions. So we don’t need to do Monte Carlo for the PPD any more than we need to for approximating the posterior of \\(\\theta\\) but we’re gonna anyway because it’s educational. Note: When I say “Poisson-gamma model” or I may say “gamma-Poisson model” that’s a shorthand for “The Bayesian statistical model where our data consists of exchangeable observations of conditionally independent Poissons with rate \\(\\theta\\) where the prior distribution on \\(\\theta\\) is gamma\\((a, b)\\)” 7.1 Sampling for predictive distribution Often, not only does the PPD not have a nice closed form but it’s also not even so straightforward to sample from. But we can still do Monte Carlo for predictive distributions! Proposition: If \\(\\theta^{(s)} \\sim p(\\theta | y)\\) and \\(y^{(s)} \\sim p(y | \\theta^{(s)})\\) then I should say \\(\\tilde y^{(s)} \\sim p(\\tilde y | \\theta^{(s)})\\) then jointly \\((\\theta^{(s)}, \\tilde y^{(s)}) \\sim p(\\theta | y) \\times p(\\tilde y | \\theta) = p(\\theta, \\tilde y | y)\\) the joint distribution of \\(\\theta \\text{ and } \\tilde Y\\). In general if \\(X_1, X_2 \\sim f(x_1, x_2)\\) then \\(X_2\\) is a draw from the marginal of \\(X_2.\\) 7.1.1 Example: birth rate Let’s return to the birthrates example. Our posterior for \\(\\theta_1\\) and \\(\\theta_2\\) were independent. \\(\\theta_1 | y \\sim\\) gamma(219, 112) and \\(\\theta_2 | y \\sim\\) gamma(68, 45). Suppose we want the posterior predictive probability \\(Pr(\\tilde Y_1 &gt; \\tilde Y_2 |\\) data). \\(\\tilde Y_1 =\\) number of children for a randomly selected 40-year-old woman with less than bachelor’s degree. \\(\\tilde Y_2 =\\) number of children for a randomly selected woman with bachelor’s or higher. \\[ \\begin{aligned} \\operatorname{Pr}\\left(\\tilde{Y}_{1}\\right.&amp;\\left.&gt;\\tilde{Y}_{2} \\mid\\left\\{y_{i,j}\\right\\}\\right)=\\sum_{\\tilde y_{1}=0}^{\\infty} \\sum_{\\tilde y_{2}=0}^{\\tilde y_{1}-1} \\operatorname{Pr}\\left(\\tilde{Y}_{1}=y_{1}, \\tilde{Y}_{2}=y_{2} \\mid\\left\\{y_{i,j}\\right\\}\\right) \\\\ &amp;=\\sum_{\\tilde y_{1}=0}^{\\infty} \\texttt {pnbinom}\\left(\\tilde y_{1}-1 \\mid 68,45\\right) \\times \\texttt {dnbinom}\\left(\\tilde y_{1} \\mid 219,112\\right) \\end{aligned} \\] Note: if I did pnbinom(y1.tilde) in the second line that’d be right for \\(\\tilde Y_2 \\le \\tilde Y_1.\\) I want \\(\\tilde Y_2 &lt; \\tilde Y_1.\\) y1.tilde &lt;- 0:100 sum( pnbinom(y1.tilde-1, size=68, mu=68/45)* dnbinom(y1.tilde, size=219, mu=219/112) ) ## [1] 0.4820895 Wait a second. Weren’t we thinking number of children for no college should be less than number of children for college grads so shouldn’t this probability be greater than 0.5? Not really. \\(Pr(\\tilde Y_1 &lt; \\tilde Y_2)\\) will also be less than 0.5 and in fact will be less than .48 (it will be 0.3 to be exact). Remember we’re talking about a discrete distribution. if you randomly select a women with less than bachelor’s degree and a woman with bachelor’s or higher there’s a decent chance they have the SAME number of kids. # Pr(Y1 tilde &gt;= Y2 tilde ) sum( pnbinom(y1.tilde, size=68, mu=68/45)* dnbinom(y1.tilde, size=219, mu=219/112) ) ## [1] 0.6997647 Notice how \\(Pr(\\tilde Y_2 \\le \\tilde Y_1|y_{i,j})\\) is over 0.50. Here we could do Monte Carlo simulations using the rnbinom function. Or we could do rgamma and rpois. They are both correct but the longer way generalizes to more complicated problems where the PPD may not be tractable. rgamma(3, 1, 1); I get 3 draws from a gamma(1,1) distribution. rpois(10, 5); I get 10 draws from a Poisson(5) rpois(10, 6:15); I get one Poisson(6) draw one Poisson(7) draw one Poisson(8) … one Poisson(15) draw These \\(\\tilde y\\)’s are Poisson variables but each with a different mean and since the Poisson means are gamma-distributed the \\(\\tilde y\\) will be negative binomially distributed. # Birth rates example; Posterior predictive simulations a &lt;- 2 ; b &lt;- 1; n1 &lt;- 111; sum.y1 &lt;- 217; n2 &lt;- 44 ; sum.y2 &lt;- 66; theta1.sim &lt;- rgamma(10000, a+sum.y1, b+n1) theta2.sim &lt;- rgamma(10000, a+sum.y2, b+n2) y1.tilde &lt;- rpois(10000, theta1.sim) y2.tilde &lt;- rpois(10000, theta2.sim) mean(y1.tilde == y2.tilde) ## [1] 0.2254 mean(y1.tilde &gt;= y2.tilde) ## [1] 0.7052 So indeed these conclusions are consistent with our calculations from above. i.e., Women without bachelors having more kids on average than women with bachelors. Here’s an important thing: Given a Monte Carlo sample from a probability distribution, I can compute (approximations to / estimates of) any feature of that distribution that interests me. E.g., moments (means, variance, etc) probabilities, quantiles. That holds for posterior predictive distributions as well as posterior distributions. 7.2 Example: Let \\(D = \\tilde Y_1 - \\tilde Y_2\\) What does \\(D\\) mean exactly? It means this: Randomly select a 40-year-old woman with less than bachelor’s and a 40-year-old woman with bachelors or higher \\(D =\\) number of kids first woman has minus number of kids second woman has. It’s a difference between two negative binomial distributions (independent NegBinom’s). I don’t know anything about the properties of such a distribution (it doesn’t matter, I don’t need to know). We can use our samples of \\(\\tilde y_1 \\text{ and } \\tilde y_2\\) to construct the posterior predictive distribution of this difference \\(D\\)! D.tilde &lt;- y1.tilde - y2.tilde D.vals &lt;- range(D.tilde)[1]:range(D.tilde)[2] ppd.D &lt;- (table(c(D.tilde, D.vals))-1) / length(D.tilde) plot(D.vals, as.vector(ppd.D), type=&quot;h&quot;, lwd=2, xlab=&quot;D=y1.tilde-y2.tilde&quot;, ylab=&quot;p(D|y1,y2)&quot;) points(D.vals, as.vector(ppd.D), pch=19) Figure 7.1: The posterior predictive distribution of D = Y1.tilde - Y2.tilde, the diﬀerence in the number of children of two randomly sampled women, one from each of the two educational population The most frequently occurring value is 0 the next most frequent is +1. The non-college-grad has 1 more kid than the college grad. This plot would have been hard to do any way other than by Monte Carlo. The weird business in the code with the (table(c(D.tilde, D.vals))-1) is there because the table function doesn’t count missing values. # Case in point x &lt;- c(1,1,1, 2,2,2, 3,3,3, 5, 6, 8);table(x) ## x ## 1 2 3 5 6 8 ## 3 3 3 1 1 1 If we are making a histogram we would want it to show that we have zero fours. Hence the weird business. 7.3 Posterior predictive model checking What we’ve done so far is assumed number of kids | \\(\\theta\\) are independent Poisson. Here’s a question: Number of kids is not a Poisson variable. The Poisson distribution counts events in a Poisson process which occur randomly and independently of each other. That is not how people have kids at all. Why did we use it? The answer is: because number of kids is a count variable and the Poisson distribution is a simple model for count variables. You know the most famous quote in statistics? “All models are wrong, some models are useful.” The real question is: Is it useful even though it’s wrong? Posterior predictive simulations (next section) are a good tool for addressing this question. Student question: I thought that Poisson was a good distribution for rare events? Ans: It is. But not only rare events. Suppose \\(Y \\sim \\text{Poisson}(\\theta).\\) No rule against \\(\\theta\\) being a big number. \\(\\theta =\\) expected count. “Rare events” just means probability is low but if exposure is high then expected number of counts can be high as well. 7.4 Posterior predictive model checking Let’s take the first group for illustration(less than bachelor’s degree). No.kids = { 0,1,2,3,4,5,6 } Frequency = { 20,19,38,20,10,2,2 } This is actually the first time we’ve seen these data because all we needed before was sufficient statistic \\((n = 111,~ \\sum{y_i} = 217)\\). Take each of these counts divided by 111 that defines the empirical distribution. The idea of “posterior predictive model checking” is; if the model is “correct” then the observed data should not appear unusual when compared to the posterior predictive distribution. \\(p(\\tilde y | y)\\) is the conditional distribution of number of chidren conditional on the observed data. If the data really were drawn from the model that we used the empirical distribution and the PPD should show close agreement. # Posterior predictive model checking for group 1 y &lt;- 0:10 e.dist &lt;- c(20,19,38,20,10,2,2,rep(0,length(y)-7)) / 111 plot(y-.05, e.dist, type=&quot;h&quot;, lwd=5, xlab=&quot;y&quot;, ylab=&quot;Pr(Y=y)&quot;) a &lt;- 2; b &lt;- 1; n &lt;- 111; sum.y &lt;- 217; p.dist &lt;- dnbinom(y, size=a+sum.y, mu=(a+sum.y)/(b+n)) points(y+.05, p.dist, type=&quot;h&quot;, lwd=5, col=&quot;gray&quot;) legend(&quot;topright&quot;, inset=.1, lwd=5, col=c(&quot;black&quot;,&quot;gray&quot;), legend=c(&quot;Empirical dist&quot;, &quot;Predictive dist&quot;)) Figure 7.2: Evaluation of model it. This shows the empirical and posterior predictive distributions of the number of children of women without a bachelor’s degree. the number of women with exactly two children is 38, which is twice the number of women in the sample with one child. In contrast, this group’s posterior predictive distribution, shown in gray, suggests that the probability of sampling a woman with two children is slightly less probable than sampling a woman with one. Should we be concerned about this? First, is it really a disagreement? There are two possible answers: It is a result of sampling variability. It is indeed a feature of the population, hence our model is wrong. The answer is based on frequentist hypothesis testing. We will simulate \\(S\\) replicated data sets (Remember \\(S\\) is a big number). For each \\(\\theta^{(s)} \\sim p(\\theta | y),\\) and for each replicated data set compute the test statistic. What test statistic to use? Whatever feature of the data you supsect the model is not capturing correctly. In this case we use \\(t(y_{obs}) = 38 / 19 = 2.0\\). What does the posterior predictive distribution of this test statistic look like? and how unusual a value is 2.0? # Define the test statistic t.sim &lt;- rep(NA, 10000) for(s in 1:10000) { theta.sim &lt;- rgamma(1, a+sum.y, b+n) y.tilde &lt;- rpois(n, theta.sim) t.sim[s] &lt;- sum(y.tilde==2) / sum(y.tilde==1) } hist(t.sim, freq=F, right=F, breaks=30, &quot;&quot;) abline(v=2, lty=2) Figure 7.3: The posterior predictive distribution of the empirical odds of having two children versus one child in a dataset of size n = 111 How often in these replicated(simulated) data sets does it happen that there are double the number of cases with \\(y=2\\) as with \\(y=1?\\) The answer is not very often. The observed test statistic \\(t(y_{obs}) = 2.0\\) is kind of out in the tail of this distribution. mean(t.sim &gt;= 2) ## [1] 0.0059 This quantity 0.0051 that’s the tail probability. This value is called a “Bayesian p-value”. This “Bayesian p-value” has nothing to do with Bayesian inference it’s strictly a tool in Bayesian model checking. We did the posterior predictive checks we found Bayesian p-value = .005 (close to zero). Based on this which bullet from above is the right one? It’s the second one! If this was to be expected due to sampling variability then it would have occurred with some frequency among the replicated data sets. But it didn’t! So this feature of the data “\\(y=2\\) with much greater frequency than \\(y=1\\)” is not explained by the model. So the conclusion is; This is indeed a deficiency of the Poisson model. To the extent that there our features of our data set that are not shared by the replicated data sets this MAY suggest a problem. We’re still back to the original question. How concerned should we be? Is predicting frequency of one kid and two kids an important goal of our inference? If yes, then we’ve got a problem because the Poisson model is not going to give accurate predictions here. It will underpredict the frequency of \\(y=2\\) and overpredict the frequency of \\(y=1\\). However if what we really care about is estimation of mean and variance of “number of kids” then this is not such a problem. So in terms of what’s the next step in this data analysis? The answer is: it depends. Thus endeth our discussion of Chapter 4. On to the next thing Chapter 5. "],["normal-mean.html", "Lecture 8 Normal Mean 8.1 Example: women’s height 8.2 Inference for the mean, conditional on the variance 8.3 Prediction 8.4 Example: Midge wing length", " Lecture 8 Normal Mean The following notes, mostly transcribed from Neath(0512,2021) lecture, summarize sections(5.1 and 5.2) of Hoff(2009). Where we been? where we going? Binomial model? check! Poisson model? check! Normal model? Next! Unlike Binomial and Poisson which are for discrete data, the normal distribution is a continuous distribution. It is completely characterized by the mean and standard deviation. Our notation will be mean \\(= \\mu = \\theta\\), standard deviation \\(= \\sigma\\), variance \\(= \\sigma^2\\). Normal distribution calculations are easy to do in R use for density values for cdf-values; \\(Pr(Y \\le y | \\theta, \\sigma^2) = \\texttt{pnorm}(y, \\texttt{mean}=\\theta, \\texttt{sd} =\\sqrt{\\sigma^2} )\\) # Reproduce Figure 5.1 of Hoff (2009) y &lt;- seq(0, 10, .05) plot(y, dnorm(y, mean=2, sd=.5), type=&quot;l&quot;, lty=1, lwd=2, col=&quot;black&quot;, xlab=&quot;y&quot;, ylab=&quot;p(y|theta, sigma2)&quot;);abline(h=0) lines(y, dnorm(y, mean=5, sd=2), lty=2, lwd=2, col=&quot;red&quot;) lines(y, dnorm(y, mean=7, sd=1), lty=3, lwd=2, col=&quot;blue&quot;) legend(&quot;topright&quot;, inset=.05, col=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lwd=2, lty=c(1,2,3), legend=legend) Figure 8.1: Some normal densities. A justification for the frequency with which the normal distribution is encountered in real-world applications is that any variable that is itself a sum or an average of a whole bunch of other variables (not necessarily observable quantities) will be well approximated by a normal distribution that’s because of the Central Limit Theorem. In practice, this means that the normal sampling model will be appropriate for data that result from the additive effects of a large number of factors. 8.1 Example: women’s height 1893 through 1989 (end of 19th century) women’s heights in inches in England. library(alr4) y &lt;- Heights$dheight c(mean(y), sd(y)) ## [1] 63.751055 2.600053 You don’t need to have an argument like 05a slide 7 at the ready to justify using a normal distribution. If you have enough observations plot the data and see! If the histogram looks like a bell curve or even better if the “normal probability plot” or the so-called QQ-plot which stands for quantile-quantile resembles a straight line then a normal model is appropriate. yvals &lt;- seq(min(y)*0.95, max(y)*1.05, length=100) hist(y, freq=F, right=F, breaks=20, xlab=&quot;Height in inches&quot;, ylab=&quot;&quot;, main=&quot;&quot;, col=&quot;gray&quot;) lines(density(y), lty = 2, lwd=2, col = &quot;red&quot;) lines(yvals, dnorm(yvals, mean(y), sd(y)), lwd=2) legend(&quot;topright&quot;, legend=c(&quot;population&quot;,&quot;sample&quot;), col=c(1,&quot;red&quot;), lwd=2, lty=c(1,2), bty=&quot;n&quot;) Figure 8.2: Height data and a normal density qqnorm(y) 8.2 Inference for the mean, conditional on the variance Our interest in this course is; How do you do Bayesian inference about the mean \\(\\theta\\) and the variance \\(\\sigma^2\\) in a normal model. \\[ p(y_1,...,y_n|\\theta, \\sigma^2)=\\left(2 \\pi \\sigma^{2}\\right)^{-n / 2} \\exp \\left\\{-\\frac{1}{2} \\sum\\left(\\frac{{y_{i}-\\theta}}{\\sigma}\\right)^{2}\\right\\} \\] From this we can see a two-dimensional sufficient statistic. Because the density only depends on the data set through \\(\\sum y_i\\) and \\(\\sum y_i^2\\). That means \\(\\sum y_i\\) and \\(\\sum y_i^2\\) are a sufficient statistic and since there exists a one to one mapping between these two statistics and \\((\\bar y, s^2)\\) that means \\((\\bar y, s^2)\\) is a sufficient statistic. \\[\\bar y=\\frac{1}{n}\\sum y_i ~; \\quad s^2=\\frac{1}{n-1}\\sum(y_i-\\bar y)^2\\] This is a feature of the normal distribution. We will take the problem of inference about \\((\\bar y, s^2)\\) and break it into two pieces. How will we do that? We will write \\(p(\\theta , \\sigma^2 | y_1, …, y_n)= p(\\theta | \\sigma^2, y_1, …, y_n) \\times p( \\sigma^2 | y_1, …, y_n).\\) Today we will only work on the first one; that is “inference about the mean of a normal distribution assuming the variance is known” or equivalently “inference about the normal mean conditional on the variance.” \\[ p(y_1,...,y_n|\\theta,\\sigma^2) = c\\times e^{-\\frac{1}{2\\sigma^2}\\sum (y_i - \\theta)^2} \\propto e^{c_1(\\theta - c_2)^2} \\] The “likelihood” as a function of \\(\\theta\\) is \\(e^{\\text{quadratic in }\\theta}.\\) So the conjugate prior \\(p(\\theta|\\sigma^2)\\) will be a distribution whose density consists of \\(e^{\\text{quadratic in }\\theta}\\). What probability distribution has a density that is \\(e^{\\text{quadratic thing }}?\\) The normal distribution! We have just proven the conjugate prior for the mean in a normal sampling model is the normal distribution. Suppose \\(\\theta \\sim \\text{Normal}( \\mu_0, \\tau_0^2 ),\\) where \\(\\tau_0^2\\) is the prior variance, \\(\\{Y_1, …, Y_n | \\theta\\}\\sim\\text{iid Normal}( \\theta, \\sigma^2)\\) (where \\(\\sigma^2\\) is known) then \\(\\{\\theta | y_1, …., y_n\\} \\sim \\text{Normal}\\) because it’s a conjugate prior. i.e., \\[ p(\\theta | y_1, …, y_n, \\sigma^2) \\propto \\exp\\bigg\\{ -\\frac{1}{2} \\bigg( \\frac{\\theta-b/a}{1/\\sqrt{a}}\\bigg)^2 \\bigg\\} \\] where \\(a = (1/\\tau_0^2)+(n/\\sigma^2) \\text{ and } b = (\\mu_0/\\tau_0^2)+(\\sum y_i/\\sigma^2).\\) Thus \\(p(\\theta | y, \\sigma^2)\\) has the same shape as a normal density with mean of \\(b/a\\) and a standard deviation of \\(1/\\sqrt a\\) therefore \\(\\{\\theta | y, \\sigma^2\\}\\) is normally distributed with mean \\(= b/a\\) and sd \\(= 1/\\sqrt a.\\) So we have \\(\\{\\theta | y_1, …, y_n , \\sigma^2\\} \\sim \\text{ Normal}(\\mu_n , \\tau_n^2)\\). So there’s a very sensible notational convention being employed here \\(\\mu_0\\) is prior mean (after observing 0 data) \\(\\mu_n\\) is posterior mean after observing \\(n\\) data points. \\(\\tau_0^2\\) is the prior variance prior to observing data (after observing 0 cases), \\(\\tau_n^2\\) is the variance after observing the data \\(n\\) observations. \\(\\tau_n^2 &lt; \\tau_0^2, ~~\\mu_n\\) should be an average between \\(\\mu_0\\) and \\(\\bar y\\). Let’s interpret the parameters in the posterior distribution. First for the posterior variance we have; \\(1 / \\tau_n^2 = 1 / \\tau_0^2 + n / \\sigma^2\\). It’s not the variances that add together, it’s the inverses added together, which makes sense because the uncertainty is decreasing. Definition: precision = 1 / variance. Think of precision as quantifying information and we have; posterior information = prior information + data information = \\((1/\\tau_0^2)+(n/\\sigma^2)=a\\). Data information = information in \\(n\\) observations times the information in a single observation. Data information is also the information in a single observation of \\(\\bar y,~ \\bar y \\sim \\text{Normal}(\\theta, \\sigma^2 / n)\\) so the variance of \\(\\bar y\\) is \\(\\sigma^2 / n\\) so the precision for \\(\\bar y\\) is \\(n / \\sigma^2\\) so the information contained in \\(\\bar y\\) is \\(n / \\sigma^2=\\) data information. What about the posterior mean? \\[ \\mu_{n}=\\frac{1/{\\tau}_{0}^{2}}{1/{\\tau}_{0}^{2}+n/{\\sigma}^{2}} \\mu_{0}+\\frac{n/{\\sigma}^{2}}{1/{\\tau}_{0}^{2}+n /{\\sigma}^{2}} \\bar{y} \\] \\(\\mu_n =\\) weighted average of \\(\\mu_0\\) and \\(\\bar y\\). Weight given to \\(\\mu_0\\) is proportional to \\(1/\\tau_0^2\\)(the prior precision), weight given to \\(\\bar y\\) is proportional to \\(n / \\sigma^2\\) the “data information.” 8.3 Prediction Let \\(\\tilde Y\\) be an (\\(n+1\\))st observation that has not as yet been observed but which we wish to predict based on observed values of \\(Y_1, …, Y_n\\). To find the posterior predictive distribution, we use the fact that \\[ \\tilde Y|\\theta,\\sigma^2 \\sim N(\\theta, \\tilde e) \\iff \\tilde Y=\\theta + \\tilde e \\text{ where } \\tilde e \\sim N(\\theta, \\sigma^2) \\] The posterior predictive mean and variance of \\(\\tilde Y\\) are \\[ \\begin{aligned} \\mathrm{E}\\left[\\tilde{Y} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] &amp;=\\mathrm{E}\\left[\\theta+\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\mathrm{E}\\left[\\theta \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right]+\\mathrm{E}\\left[\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\mu_{n}+0=\\mu_{n} \\end{aligned} \\] and \\[ \\begin{aligned} \\operatorname{Var}\\left[\\tilde{Y} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] &amp;=\\operatorname{Var}\\left[\\theta+\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\operatorname{Var}\\left[\\theta \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right]+\\operatorname{Var}\\left[\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\tau_{n}^{2}+\\sigma^{2} \\end{aligned} \\] So \\(\\{\\tilde Y | y_1, …., y_n, \\sigma^2\\} \\sim \\text{Normal}(\\mu_n, \\tau_n^2+\\sigma^2).\\) Note that there are two sources of uncertainty (variance) in our predictions (1) we don’t know what \\(\\theta\\) is! \\(\\theta|y \\sim \\text{Normal}(\\mu_n , \\tau_n^2 )\\) (2) even if we knew \\(\\theta\\) exactly \\(\\tilde Y \\sim \\text{Normal}( \\theta , \\sigma^2 )\\) won’t equal \\(\\theta\\) exactly. 8.4 Example: Midge wing length Goal is: Estimate the mean wing length for a species of midge (a fly), i.e., we wish to make inference about the population mean \\(\\theta\\). Current data: \\(n = 9\\) observations, \\(\\bar y = 1.804\\) Prior information : For other similar species the mean wing length is about \\(1.9\\)mm so this will be our prior mean \\(\\mu_0\\). In this case, \\(\\theta =\\) mean wing length, which means \\(Pr(\\theta &gt; 0) = 1.\\) This is not a property of the normal model since the normal model spans both negative and positive values. So we need to find a way to get our prior to have mass only on \\(\\theta &gt; 0\\). Our prior variance is gotten by; back into \\(\\tau_0\\) so that prior probability of \\(\\theta &lt; 0\\) is small. So we want \\[ \\mu_0-2\\tau_0&gt;0\\implies1.9&gt;2\\tau_0\\implies 0.95 &gt; \\tau_0 \\] So we will use \\(\\tau_0 = .95\\) as our prior variance. We have a prior distribution \\(\\theta \\sim \\text{Normal}( \\mu_0 = 1.9,\\tau_0^2 = .95^2)\\), we have data \\(\\{Y_1, …, Y_9 | \\theta\\} \\sim \\text{ iid Normal}(\\theta, \\sigma^2)\\). The posterior is \\(\\{\\theta | y_1, …, y_9, \\sigma^2\\} \\sim \\text{Normal}( \\mu_n, \\tau_n^2 )\\) where \\(1/\\tau_n^2 = (1 / \\tau_0^2) + (n/\\sigma^2).\\) We need \\(\\sigma^2\\) to finish this problem. Set \\(\\sigma^2 = \\text{sample variance} = s^2\\). options(digits = 4) y &lt;- c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08) # Prior mu.0 &lt;- 1.9; tau.0 &lt;- 0.95; tau2.0 &lt;- tau.0^2; # Calculations ybar &lt;- mean(y) s2 &lt;- var(y) # = sum((y-mean(y))^2)/(length(y)-1) n &lt;- length(y) sigma &lt;- sqrt(s2) mu.n &lt;- (mu.0/tau2.0 + n*ybar/s2) / (1/tau2.0 + n/s2) tau2.n &lt;- 1 / (1/tau2.0 + n/s2) c(ybar, s2) ## [1] 1.80444 0.01688 c(mu.n, tau2.n) ## [1] 1.804643 0.001871 So \\(\\{\\theta|y_1,...,y_9,\\sigma^2=0.017\\} \\sim \\text{Normal}(1.805,0.002)\\) Notice \\(\\bar y = 1.804,\\) \\(\\mu_0 = 1.9,\\) and \\(\\mu_0 = 1.805.\\) So we’re giving a lot more weight to the data than to the prior. This makes sense because our 9 observations are from the species we’re interested in. However, our prior was a similar species but not the one we’re interested in. # 95% posterior interval; qnorm(c(.025, .975), mu.n, sqrt(tau2.n)) (CI &lt;- mu.n+c(-1,1)*1.96*sqrt(tau2.n)) ## [1] 1.720 1.889 theta &lt;- seq(0, 4, .01) plot(theta, dnorm(theta, mean=mu.n, sd=sqrt(tau2.n)), type=&quot;l&quot;, lwd=2, ylab=&quot;p(theta|y,sigma2=0.017)&quot;) lines(theta, dnorm(theta, mean=mu.0, sd=sqrt(tau2.0)), lwd=2, col=&quot;gray&quot;) abline(v=CI, lty = 3, col = &quot;gray&quot;) legend(&quot;topright&quot;, inset=.05, lwd=2, col=c(&quot;gray&quot;, &quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;)) Figure 8.3: Prior and conditional posterior distributions for the population mean wing length. We have very high posterior belief that the mean wing length is close to 1.8. In this example, we were pretending that the population variance \\(\\sigma^2\\) (variance of midge wing lengths ) was known to be \\(s^2 =0.017\\). In fact we don’t know it! As a result, it is possible that this interval is narrower than it should be because it fails to account for our uncertainty about the population variance \\(\\sigma^2.\\) We deal with this in the next lecture. "],["joint-inference-for-normal-mean-and-variance.html", "Lecture 9 Joint inference for Normal mean and variance 9.1 Marginal posterior of \\(\\sigma^2\\) 9.2 Example: Midge wing length 9.3 Monte Carlo sampling 9.4 Summary of Normal formulas", " Lecture 9 Joint inference for Normal mean and variance The following notes, mostly transcribed from Neath(0513,2021) lecture, summarize section(5.3) of Hoff(2009). What if we don’t really care about the variance? We’re only interested in doing inference about the mean anyway? Can we use the methods from the last lesson? NO we should not. Unless of course we genuinely do know the population variance. So even if our inferential goals don’t include the variance and we’re only interested in the mean we still have to account for the fact that the variance is unknown(there’s uncertainty about the variance) to do valid inference about the mean. In this case we say the variance is a nuisance parameter. So let’s talk about how to do that. Model: We have \\(n\\) exchangeable observations from a population that is normal with a mean of \\(\\theta\\) and a variance of \\(\\sigma^2\\) both of which are unknown. Given a prior distribution on \\((\\theta, \\sigma^2)\\), we use Bayes rule to compute a posterior distribution where the prior describes our belief before observing the sample data the posterior will describe our belief after observing the data. Is there a conjugate distribution for \\((\\theta, \\sigma^2)\\) together? Last class we saw that conditional on \\(\\sigma^2\\) the conjugate prior for \\(\\theta\\) was the normal distribution. Let’s use that fact going forward. Let’s write our prior distribution as the joint density \\(p(\\theta, \\sigma^2) = p(\\theta|\\sigma^2)p(\\sigma^2) = \\texttt{dnorm} (\\theta|\\mu_0,\\tau_0^2)p(\\sigma^2).\\) Since we are conditioning on \\(\\sigma^2\\) anyway let’s set \\(\\tau_0^2=\\sigma^2/\\kappa_0,\\) where \\(\\kappa_0 =\\) number of prior observations, this way we have described(parameterized) our uncertainty about \\(\\theta\\) conditionally on \\(\\sigma^2\\). If \\(\\theta | \\sigma^2 \\sim \\text{Normal}( \\mu_0 , \\sigma^2 / \\kappa_0 )\\) and data are \\(\\{y_1, …, y_n | \\theta, \\sigma^2\\} \\sim \\text{ iid Normal}( \\theta, \\sigma^2)\\). Equivalently in terms of the sufficient statistic \\(\\{\\bar y | \\theta, \\sigma^2\\} \\sim \\text{Normal}( \\theta, \\sigma^2 / n)\\) The posterior of \\(\\theta, \\{\\theta | y, \\sigma^2\\} \\sim \\text{Normal}\\) with mean \\(\\mu_n = (\\kappa_0\\mu_0+n\\bar y)/\\kappa_n\\) where \\(\\mu_n\\) is a weighted average of prior mean \\(\\mu_0\\) and data mean \\(\\bar y\\) and these two weights are proportional to \\(\\kappa_0\\) (prior sample size) and \\(n\\) (“real data” sample size). From previous lecture we found that the posterior variance \\(\\tau_n^2 = 1/a=1/(1/\\tau_0^2+n/\\sigma^2)\\). Now plug in \\(\\tau_0^2 = \\sigma_0^2 / \\kappa_0\\) and you get \\(\\tau_n^2 = 1 / ( \\kappa_0 / \\sigma^2 + n / \\sigma^2 ) = \\sigma^2 / (\\kappa_0 + n)=\\sigma^2/\\kappa_n\\). This makes perfect sense! Our data consist of \\(n\\) observations. Our prior consists of \\(\\kappa_0\\) “observations.” So finally, we have \\[\\{\\theta | y_1,...,y_n, \\sigma^2\\} \\sim \\text{Normal}( \\mu_n = (\\kappa_0\\mu_0+n\\bar y)/\\kappa_n, ~\\sigma^2/\\kappa_n)\\] We’re half way there. \\(p(\\theta , \\sigma^2) = p(\\theta | \\sigma^2)p(\\sigma^2) = \\texttt{dnorm}( \\theta | \\mu_0 , \\sqrt{\\sigma^2/\\kappa_0} ) p(\\sigma^2)\\) What might be the conjugate prior for the variance \\(\\sigma^2?\\) The gamma distribution has support (0, infinity). Might this work? It does not. It turns out. The gamma distribution is not conjugate for the variance, \\(\\sigma^2\\). However, the gamma distribution is conjugate for the precision = 1 / variance. How did we get this? Condition on the likelihood \\(p(\\sigma^2 | y) \\propto p(y | \\sigma^2) p(\\sigma^2),\\) and look at what the likelihood contributes. Recall, \\(p(y|\\theta,\\sigma^2) = (\\sqrt{2\\pi\\sigma^2})^{-1}\\text{exp}\\{-(1/2\\sigma^2)(y-\\theta)^2\\} \\propto (\\sigma^2)^{-1}\\text{exp}(-1/\\sigma^2),\\) and the gamma distribution has the form \\(p(\\theta) \\propto \\theta^{a-1}e^{-b\\theta}.\\) So what the likelihood contributes in this case for a normal variance is \\((\\sigma^2)^{\\text{-something}} \\times e^{- \\text{something} / \\sigma^2}.\\) Since it’s not \\((\\sigma^2)^{\\text{something}} \\times e^{- \\sigma^2/\\text{something} },\\) then it’s not \\(\\sigma^2\\) that has a gamma distribution but rather the precision, \\(1/\\sigma^2\\). Definition: If \\(X \\sim\\) gamma\\((a, b)\\) and \\(Y = 1/X\\) then \\(Y \\sim\\) InvGamma\\((a, b).\\) The conjugate prior for the normal variance, \\(\\sigma^2\\) is to say \\(1/\\sigma^2 \\sim \\text{gamma}(a, b)\\) and since the parameter \\(a &gt; 0, ~ b &gt; 0\\) are arbitrary we can reparameterize this distribution to gamma\\(( \\nu_0 / 2 , ~ \\nu_0\\sigma_0^2 / 2)\\). We could do this by \\(\\nu_0 = 2a, ~ \\sigma_0^2 = b / a\\). The reason we did this reparameterization is that this gives a more natural way to think about the variance. We know the mean and variance of the gamma distribution are \\(a/b\\) and \\(a/b^2\\) so we have; \\(E(1/\\sigma^2) = 1/ \\sigma_0^2\\) Var\\(( 1 / \\sigma^2) = 2 / [ \\nu_0 \\times (\\sigma_0^2)^2 ]\\) What do all these parameters in this prior distribution represent? \\(\\mu_0\\) is prior best guess at \\(\\theta\\) \\(\\kappa_0\\) measures the strength of that belief \\(\\sigma^2_0\\) is our prior best guess at \\(\\sigma^2\\) \\(\\nu_0\\) measures the strength of that belief Remember that in inference for the normal distribution, inference about the mean and inference about the variance proceed “independently” in a sense \\((\\bar Y \\perp s^2)\\). So we’re allowed to have different prior sample sizes for the mean and the variance. Our data consist of \\(n\\) observations from the population so the data will contribute \\(n\\) to both the mean and the variance. The prior contributes \\(\\kappa_0\\) to the mean and \\(\\nu_0\\) to the variance. No requirement that these be equal. The posterior density satisfies \\(p(\\theta, \\sigma^2 | y) = p(\\theta | \\sigma^2, y)p(\\sigma^2 | y)\\). The first piece is already solved! \\(p(\\theta | \\sigma^2, y) = \\texttt{dnorm}( \\theta | \\mu_n , \\sqrt{\\sigma^2 / \\kappa_n })\\). Now for the second piece. 9.1 Marginal posterior of \\(\\sigma^2\\) The result is that \\(1/\\sigma^2 | y \\sim \\text{gamma}( \\nu_n / 2 , \\nu_n \\sigma_n^2 / 2 )\\). So that’s why that reparameterization was so useful. \\(\\nu_0\\) in the prior becomes \\(\\nu_n = \\nu_0 + n\\) in the posterior the \\(\\sigma_0^2\\) in the prior becomes \\(\\sigma_n^2\\) in the posterior which is: \\[\\sigma_n^2 = \\frac{1}{\\nu_n}[\\nu_0\\sigma_0^2 + (n-1)s^2+\\frac{\\kappa_0n} {\\kappa_n}(\\bar y - \\mu_0)^2]\\] It’s almost a weighted average of the “prior” variance \\(\\sigma_0^2\\) and the “data variance” \\(s^2\\). \\(\\nu_n = \\nu_0 + n = \\nu_0 + (n-1) + 1\\). The prior variance gets weight proportional to \\(\\nu_0\\). The sample variance \\(s^2\\) gets weight proportional to \\((n-1)\\). That extra piece is weird. It only gets weight of \\(1 / \\nu_n &lt; (n-1) / n\\). 9.2 Example: Midge wing length Our prior best guesses at the mean and variance for this population are \\(\\mu_0 = 1.9\\) and \\(\\sigma_0 = 0.01\\) based on studies of other populations. Our data consist of \\(n\\) observations. Our prior belief is based on not anything we have a whole lot of confidence in, but as long as we set \\(\\kappa_0\\) and \\(\\nu_0\\) to be small relative to \\(n=9\\) they won’t get much weight in the posterior anyway. Set \\(\\nu_0 = 1, ~ \\kappa_0 = 1\\) so prior gets 10% weight and data gets 90% weight in the posterior. That seems reasonable. y &lt;- c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08) # Prior mu.0 &lt;- 1.9; sigma.0 &lt;- 0.1; sigma2.0 &lt;- sigma.0^2; nu.0 &lt;- 1; kappa.0 &lt;- 1 ; # Calculations n &lt;- length(y); ybar &lt;- mean(y); s2 &lt;- var(y); nu.n &lt;- nu.0 + n ; kappa.n &lt;- kappa.0 + n; mu.n &lt;- (kappa.0*mu.0 + n*ybar) / kappa.n sigma2.n &lt;- (1/nu.n) * (nu.0*sigma2.0 + (n-1)*s2 + kappa.0*n*(ybar-mu.0)^2 / kappa.n) c(mu.n, sigma2.n, sqrt(sigma2.n)) ## [1] 1.81400 0.01532 0.12379 Our posterior belief about \\((\\theta , \\sigma^2)\\) is described by the joint probability distribution: \\(\\{\\theta|y_1,...,y_n,\\sigma^2\\} \\sim \\text{Normal}(\\mu_n, \\sigma^2/\\kappa_n) = \\text{Normal}(1.814, \\sigma^2/10)\\) \\(\\{1/\\sigma^2|y_1,...,y_n\\} \\sim \\text{gamma}(\\nu_n/2, \\nu_n\\sigma^2_n/2)=\\text{gamma}(10/2, 10\\times 0.015/2)\\) Joint density of \\((\\theta, \\sigma^2)\\) I want to draw a picture of the joint density \\(p(\\theta, \\sigma^2 | y)\\) but this is a three-dimensional figure and I have a two-dimensional dimensional monitor on my laptop. There’s lots of ways to do this. The one I happen to like is kind of old fashioned and that is the contour plot. A contour plot is defined by; If \\(f(x, y)\\) is a joint density for random variables \\((X, Y)\\) then find the mode of this density (the peak) and then find all the points \\((x, y)\\) such that \\(f(x, y) = 0.95 \\times f(x.\\text{mode}, y.\\text{mode})\\). That is all the points where the density is 95% of the peak value. Then draw a line connecting those points. Assuming a unimodal distribution, that will be the innermost contour. Then do this again for 90% of the peak value, and 85% of the peak value down to 0.001% of the peak value. # These values arrived at by lots of trial and error gs &lt;- 800 theta &lt;- seq(1.5, 2.1, length=gs) I.sig2 &lt;- exp(seq(log(1), log(250), length=gs)) sigma2 &lt;- 1 / exp(seq(log(1000), log(11), length=gs)) # Do mean and precision first log.post &lt;- matrix(NA, gs, gs); for(i in 1:gs){ for(j in 1:gs){ log.post[i,j] &lt;- dnorm(theta[i], mu.n, 1/sqrt(I.sig2[j]*kappa.n), log=T) + dgamma(I.sig2[j], nu.n/2, nu.n*sigma2.n/2, log=T) }} maxie &lt;- max(log.post) log.post &lt;- log.post - maxie post.P &lt;- exp(log.post) rm(maxie); rm(log.post) # Now do mean and variance log.post &lt;- matrix(NA, gs, gs); for(i in 1:gs){ for(j in 1:gs){ log.post[i,j] &lt;- dnorm(theta[i], mu.n, sqrt(sigma2[j]/kappa.n), log=T) + dgamma(1/sigma2[j], nu.n/2, nu.n*sigma2.n/2, log=T) - 2*log(sigma2[j]) }} maxie &lt;- max(log.post) log.post &lt;- log.post - maxie post.V &lt;- exp(log.post) contours &lt;- c(.001, .01, seq(.05, .95, .10)) rm(maxie);rm(log.post) par(mar=c(3,3,2,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0),mfrow=c(1,2)) contour(theta, I.sig2, post.P, levels=contours, drawlabels=F, xlab=xlab, ylab=ylab2, main=&quot;Mean and precision&quot;) contour(theta, sigma2, post.V, levels=contours, drawlabels=F, xlab=xlab, ylab=ylab1, main=&quot;Mean and variance&quot;) Figure 9.1: Joint posterior distributions of (theta,precision) and (theta, sigma^2). The posterior mode of (mean, precision) \\((\\theta, 1/\\sigma^{2} )\\) is at (1.8 and 60) or so. The innermost contour is the set of all points whose joint density is .95 times that peak value. The next contour shows all the points whose joint density is .85 times the max value. There should be 12 contours in this picture. They are the .95, .85, … .15, .05, .01, .001. This is the equivalent of Fig 5.4 in the book but the author doesn’t show as much as I have here there’s no .001 contour. I always like to see the tails. What do we know about this distribution? We know that conditional on \\(\\sigma^2, ~ \\theta\\) is Normal. What that means is that every horizontal slice from this joint distribution is a bell curve. Remember that if \\(f(x,y)\\) is the joint density of \\((X,Y)\\) the conditional density of \\(Y | X=x_0\\) is found by taking the vertical slice of the joint density at the point \\(x=x_0\\). Similarly, I’ve got in this picture the joint posterior of \\((\\theta, \\sigma^{-2})\\) the conditional of \\(\\theta\\) given \\(\\sigma^2\\) is a horizontal slice. Given that the shape makes perfect sense. The bigger is \\(\\sigma^2\\) (smaller is \\(1/\\sigma^2\\)) the weaker is our belief about \\(\\theta\\), hence the wideness of the LHS plot. The smaller is \\(\\sigma^2\\) (the bigger is \\(1/\\sigma^2\\)) the stronger our belief about \\(\\theta\\), hence the peak on the LHS plot. What the belief is does not depend on \\(\\sigma^2\\) that belief in “\\(\\theta\\) is about 1.805 or so.” Notes on the code I did \\(800 \\times 800\\) calculations of the joint density. For numerical reasons it is a good practice to compute log-densities then subtract the max log-density off of every value THEN exponentiate back. post.P represents joint posterior of mean and precision post.V is the joint posterior of mean and variance The calculation is; \\(p(\\theta, \\sigma^2) = p(\\theta | \\sigma^2, y) p(\\sigma^2 | y)\\) or \\(\\log[ p(\\theta, \\sigma^2 | y) ] = \\log[ p(\\theta | \\sigma^2, y) ] + \\log[ p(\\sigma^2 | y) ]\\) Question: why do we need to subtract off \\(2 \\log(\\sigma^2)?\\) in the calculation of post.V? Let \\(V =\\) variance, \\(P =\\) precision, \\(V = 1/P\\), \\(P = 1/V\\). The random variable \\(P\\) has a gamma distribution the random variable \\(V\\) has an inverse-gamma distribution. The probability density of \\(V\\) is the gamma pdf evaluated at \\(1/v \\times 1/v^2\\) because that’s the Jacobian. Look at “nonlinear transformations of random variables” from probability theory for more insight. 9.3 Monte Carlo sampling Of course, In the above problem the calculations were not easy but possible. But going forward when we get to the really messy problems we’re gonna have no choice but to do Monte Carlo. I want a Monte carlo sample that is \\(\\theta^{(1)} , ..., \\theta^{(S)}\\) such that \\(\\theta^{(s)} \\sim p(\\theta |y)\\). The problem is; I don’t know the marginal posterior of \\(\\theta\\), \\(p(\\theta | y).\\) I know \\(p(\\theta | y, \\sigma^2)\\) and I know \\(p(\\sigma^2 | y)\\), and that’s enough. It just means each simulation is gonna require two steps. First simulate \\(\\sigma^{2(s)}\\) and then simulate \\(\\theta^{(s)} \\sim p(\\theta |y, \\sigma^{2(s)}).\\) The result is \\((\\sigma^{2(s)} , \\theta^{(s)}) \\sim p(\\theta , \\sigma^2 | y)\\) which means marginally \\(\\theta^{(s)} \\sim p(\\theta | y).\\) S &lt;- 10000 sigma2.sim &lt;- 1 / rgamma(S, nu.n/2, nu.n*sigma2.n/2) theta.sim &lt;- rnorm(S, mu.n, sqrt(sigma2.sim/kappa.n)) # Scatterplot; empirical joint distribution of MC sample contour(theta, sigma2, post.V, levels=contours, drawlabels=F, xlab=xlab, ylab=ylab1, main=&quot;Mean and variance&quot;) points(theta.sim, sigma2.sim, pch=19, cex=.25, xlim=c(1.51, 2.11), ylim=c(0, .10)) # Marginal density estimates, and 95% CI for theta (CI.theta &lt;- quantile(theta.sim, c(.025, .975))) ## 2.5% 97.5% ## 1.727 1.901 I actually know the theoretical marginal distribution of \\(\\sigma^2\\). But if I didn’t I could draw a histogram of the Monte Carlo sample or a kernel density estimate based on the monte carlo samples as below. par(mar=c(3,3,1.5,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0),mfrow=c(1,2)) hist(sigma2.sim, xlim=c(0, .08), main=&quot;&quot;, freq=F, xlab=ylab1, ylab=ylab3, ylim=c(0,60), border=&quot;lightpink1&quot;,col=&quot;pink&quot;) lines(density(sigma2.sim), lwd = 2) hist(theta.sim, freq=F, xlim=c(1.6, 2.0), main=&quot;&quot;, xlab=xlab, ylab=ylab4, ylim=c(0,10), border=&quot;lightpink1&quot;,col=&quot;pink&quot;) lines(density(theta.sim), lwd=2) abline(v=CI.theta, lty=2, lwd=1.5) Figure 9.2: Monte Carlo samples from and estimates of the joint and marginal distributions of the population mean and variance. The vertical lines in the third plot give a 95% quantile-based posterior interval for theta The 95% Bayesian confidence interval is \\([1.72, 1.90]\\). In the last lecture, when we were pretending \\(\\sigma^2\\) was known, the CI was \\([1.720, 1.889]\\). Furthermore, the frequentist interval based on the t-distribution is \\([1.70, 1.90]\\). Our Bayesian interval brings the lower bound up just a tad (because the prior mean was 1.9 vs data mean of 1.8 or so) and it shortens the CI a bit because our prior counts for one observation so our “Bayesian sample size” was 10 not just 9. 9.4 Summary of Normal formulas \\(p(\\theta,\\sigma^2 | y_1,...,y_n) \\propto p(\\theta,\\sigma^2) p(y_1,...,y_n| \\theta,\\sigma^2)\\) \\(p(\\theta,\\sigma^2)=p(\\theta|\\sigma^2)p(\\sigma^2) = \\texttt{dnorm} (\\theta,\\mu_0,\\tau_0 = \\sigma/\\sqrt{\\kappa_0})p(\\sigma^2)\\) \\(1/\\sigma^2 \\sim\\) gamma\\((\\nu_0/2, \\nu_0 \\sigma_0^2/2)\\) \\(\\{\\theta|\\sigma^2\\} \\sim\\) normal\\((\\mu_0, \\sigma^2/\\kappa_0) \\equiv \\text{normal}(\\mu_0, \\tau_0^2)\\) if \\(\\{Y_1,\\ldots,Y_n\\} \\sim\\) i.i.d. normal\\((\\theta,\\sigma^2)\\) then \\(\\{1/\\sigma^2|y_1,...,y_n\\} \\sim \\text{gamma}(\\nu_n/2, \\nu_n \\sigma^2_n/2) \\equiv\\sigma^2_{mc} \\sim 1 / \\texttt{rgamma}(S, \\nu_n/2, \\nu_n \\sigma^2_n/2)\\) where \\(\\nu_n = \\nu_0 + n\\) \\(\\{ \\theta|y_1,...,y_n, \\sigma^2\\} \\sim \\text{normal}(\\mu_n, \\sigma^2 / \\kappa_n) \\equiv \\theta_{mc}\\sim\\texttt{rnorm}(S, \\mu_n, \\sqrt{\\sigma^2_{mc}/\\kappa_n})\\) where \\(\\kappa_n = \\kappa_0 + n\\) \\[ \\begin{aligned} \\mu_{n} &amp;= \\frac{1/{\\tau}_{0}^{2}}{1/{\\tau}_{0}^{2}+n/{\\sigma}^{2}} \\mu_{0}+\\frac{n/{\\sigma}^{2}}{1/{\\tau}_{0}^{2}+n /{\\sigma}^{2}} \\bar{y}\\\\[0.3cm] \\text{ if }\\tau_0^2=\\sigma^2/\\kappa_0, ~~\\mu_n &amp;= \\frac{\\kappa_{0}}{\\kappa_{0}+n} \\mu_{0}+\\frac{n}{\\kappa_{0}+n} \\bar{y} = \\frac{\\kappa_0\\mu_0+n\\bar y}{\\kappa_n}\\\\ \\frac{1}{\\tau_n^2} &amp;= \\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}\\\\[0.3cm] \\end{aligned} \\] \\[\\sigma_n^2 = \\frac{1}{\\nu_n}[\\nu_0\\sigma_0^2 + (n-1)s^2+\\frac{\\kappa_0n} {\\kappa_n}(\\bar y - \\mu_0)^2]\\] \\(\\{\\tilde Y|\\sigma^2, y_1,...,y_n\\} \\sim \\text{normal}(\\mu_n,\\tau_n^2+\\sigma^2) \\equiv \\texttt{rnorm}(S, \\theta_{mc}, \\sqrt{\\sigma^2_{mc}})\\) \\(\\mu_0 \\text{ and }\\kappa_0\\) is the mean and sample size from a prior set of observations. \\(\\nu_0\\) prior sample size, from which a prior sample variance \\(\\sigma_0^2\\) has been obtained. \\(s^2 = \\sum_{i=1}^n (y_i - \\bar y)/(n-1) = \\texttt{var}(\\boldsymbol{y})\\) sample variance \\((n-1)s^2\\) is the sum of squared observations from the sample mean \\(\\nu_0 \\sigma_0^2 \\text{ and } \\nu_n\\sigma_n^2\\) as prior and posterior sum of squares, respectively. "],["gibbs-sampler.html", "Lecture 10 Gibbs sampler 10.1 Review of conjugate prior for normal model 10.2 A semiconjugate prior distribution 10.3 Gibbs sampling 10.4 Example: Midge wing length 10.5 Discrete approximation of posterior distribution 10.6 Example", " Lecture 10 Gibbs sampler The following notes, mostly transcribed from Neath(0517,2021) lecture, summarize sections(6.1-6.4) of Hoff(2009). 10.1 Review of conjugate prior for normal model We write our model; \\(n\\) observations from a normal population with mean \\(\\theta\\) and variance \\(\\sigma^2\\) and we want to do Bayesian inference about \\(\\theta\\) (and maybe \\(\\sigma^2\\)). The conjugate prior for this model is: \\(\\sigma^2 \\sim \\text{InverseGamma}(a = \\nu_0 / 2, b = \\nu_0\\sigma_0^2 / 2).\\) Where does this come from? (Review) \\(X \\sim\\) Inverse gamma means \\(X = 1 / Z\\) where \\(Z \\sim\\) gamma distribution. When we say the conjugate prior for a normal variance is the inverse gamma that’s equivalent to saying the conjugate prior for a normal precision(precision = 1 / variance) is the gamma distribution. The conjugate prior for \\((\\theta, \\sigma^2)\\) is completed by \\(\\theta | \\sigma^2 \\sim \\text{Normal}( \\mu_0, \\sigma^2 / \\kappa_0 )\\). The variance depending on \\(\\sigma^2\\) makes sense because the distribution is specified conditionally on \\(\\sigma^2.\\) Back to that gamma distribution. The reparameterization from the usual gamma(\\(a =\\) shape, \\(b =\\) rate) to \\((a = \\nu_0 /2, b = \\nu_0 \\sigma^2_0 / 2)\\) is strategic. \\(\\sigma^2_0\\) is the “prior best guess” and \\(\\nu_0\\) measures the strength of belief in that best guess. More precisely, this prior contributes exactly the same information to the posterior as would \\(\\nu_0\\) observations with a sample variance of \\(\\sigma_0^2.\\) This conjugate prior represents a “prior sample of size \\(\\nu_0\\)” “with a sample variance of \\(\\sigma_0^2\\).” Similarly (and more straightforwardly) the prior on \\(\\theta\\) contributes to the posterior exactly what would be contributed by \\(\\kappa_0\\) observations with a sample mean of \\(\\mu_0\\). The parameters in the normal conjugate prior are; \\(\\mu_0\\) (prior sample mean) \\(\\kappa_0\\) (prior sample size for the mean) \\(\\sigma_0^2\\) (prior sample variance) \\(\\nu_0\\) (prior sample size for variance). Then the updating is very intuitive; \\[\\theta | \\sigma^2, y_1, …, y_n \\sim \\text{Normal}( \\mu_n, \\sigma^2 / \\kappa_n), ~~\\kappa_n = \\kappa_0 + n,~ \\mu_n = (\\kappa_0 \\mu_0 + n \\bar y)/ (\\kappa_0 + n)\\] Question: How to decide a proper \\(\\kappa_0\\)? If you’re uncertain, take \\(\\kappa_0\\) to be small relative to \\(n\\) and it doesn’t really matter. One of the strengths of the Bayesian paradigm is that it allows the incorporation of prior information. But in practice, non informative priors are much more commonly used (\\(\\kappa_0\\) is small relative to \\(n\\)), thus the posterior is mostly determined by the results of the data, experiment, sample, etc. Though, this is not the only reason we use Bayesian Statistics. We also like the updates and the interpretations in terms of probability. Back in chapter 5 (our previous class in fact) we skipped some stuff about “improper priors.” Given this line of questioning maybe we should “un-skip” this section some time in the next few days The punchline: There is a way to do Bayesian inference and not incorporate ANY prior information (be “objective”). You still have some decisions to make regarding the prior distribution but they’re in terms of form not content. It will often be (usually be) very similar in terms of the substantive conclusions if not exactly the same to the frequentist. But there are some very complex models (maybe we get to this stuff toward the end of our course) where the Bayesian answer is actually a lot easier to get to than the frequentist. 10.2 A semiconjugate prior distribution For today’s class, we’re still doing normal model but suppose I don’t like the “conjugate prior” above. I don’t like that the conjugate prior forces me to describe my uncertatinty about \\(\\theta\\) conditionally on \\(\\sigma^2\\) what if my prior knowledge of \\(\\theta\\) and my prior knowledge of \\(\\sigma^2\\) have nothing to do with each other and I want my prior on \\(\\theta\\) to be independent of my prior on \\(\\sigma^2\\). This joint prior distribution is ‘semiconjugate’ for the normal model. \\[ \\theta \\sim \\text{Normal}(\\mu_0, \\tau_0^2); \\quad 1/\\sigma^2 \\sim \\text{gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2/2) \\] With \\(\\sigma^2\\) fixed this would be the conjugate prior for \\(\\theta\\) instead, and with \\(\\theta\\) fixed this would be the conjugate prior for \\(\\sigma^2\\). Taking the prior above, what posterior results from it? Conditionally \\(\\sigma^2,\\) \\(\\theta\\) has a normal posterior. i.e., { \\(\\theta | y_1, …, y_n , \\sigma^2 ~ \\} \\sim \\text{Normal}( \\mu_n, \\tau_n^2 )\\). Remember we write \\(1/\\tau_n^2 = 1 / \\tau_0^2 + n / \\sigma^2.\\) i.e., posterior precision = prior precision + data precision. So ‘this is a prior that is defined not dependent on \\(\\sigma^2\\) but the posterior is specified conditionally on \\(\\sigma^2\\).’ Note: there is no \\(\\kappa_0\\) in this prior because \\(\\kappa_0\\) is a parameter of the prior \\(p(\\theta | \\sigma^2)\\). In this prior \\(p(\\theta | \\sigma^2) = p(\\theta).\\) Student question: Could you explain more about what a semi conjugate prior is? Ans: The conjugate prior for the normal model satisfies this \\(p(\\theta, \\sigma) = p(\\sigma^2) p(\\theta | \\sigma^2)\\) and \\(p(\\theta, \\sigma^2 | y) = p(\\sigma^2 | y) p(\\theta | \\sigma^2 , y).\\) For the fully conjugate prior, the joint posterior \\(p(\\theta, \\sigma^2 | y)\\) has the same parametric form as the joint prior \\(p(\\theta, \\sigma).\\) In the semiconjugate prior the conditional prior \\(p(\\theta | \\sigma)\\) has the same parametric form as the conditional posterior \\(p(\\theta | \\sigma, y)\\) and same thing with conditional prior \\(p(\\sigma^2 | \\theta) = p(\\sigma^2)\\) has the same parametric form as the conditional posterior \\(p(\\sigma | \\theta , y)\\). The seminconjugate prior is not strictly conjugate because \\(\\theta\\) and \\(\\sigma^2\\) are independent in the prior but not in the posterior however each conditional prior is conjugate. Again, \\(\\theta \\sim\\) Normal and \\(\\sigma^2 \\sim\\) inverse-gamma with \\(\\theta\\) and \\(\\sigma^2\\) independent is not strictly conjugate because the joint posterior has a different form than the joint prior however both conditional posteriors have the same form as the corresponding priors. (#fig:contour plot for precision)Conjugate prior for mean and precision. This is a posterior distribution for the (the flies’ wings) but pretend it’s the conjugate prior for \\(\\theta\\) and \\(1/\\sigma^2\\). The conditional prior \\(p(\\theta | \\sigma^2 )\\) is very tight (has low variance) when precision is high and is highly diffuse (has a high variance) when precision is low. Remember in these pictures the conditional distribution \\(p(\\theta | \\sigma^2 )\\) is visualized in these pictures by taking ‘horizontal slices’ Last week (Ch 5 in Hoff) we wrote \\(p(\\theta, \\sigma^2) = p(\\sigma^2) p(\\theta | \\sigma^2)\\). For the semi conjugate prior we’re studying today, this picture would not be like this. Instead, \\(p(\\theta | \\sigma^2)\\) would be the same for all \\(\\sigma^2\\) because \\(\\theta \\perp \\sigma^2.\\). Let’s agree that this semiconjugate prior \\(\\theta \\sim \\text{Normal}( \\mu_0, \\tau_0^2 ), ~ \\sigma^2 \\sim \\text{InverseGamma}(\\nu_0 / 2, \\nu_0 \\sigma_0^2 / 2 )\\) where \\(\\sigma^2\\) independent i.e., \\(p(\\theta, \\sigma^2) = \\texttt{dnorm}(\\theta | …) \\times \\texttt{dinvgamma}(\\sigma^2 | … ),\\) is worth considering. So we ask: what posterior results? and we got half way to answering the question. We know that \\(p(\\theta | \\sigma^2, y_1, …, y_n)\\) is Normal\\((\\mu_n, \\tau_n^2 )\\) so if we can solve \\(p(\\sigma^2 | y_1, …, y_n)\\) we solve the posterior. However, this doesn’t have a nice solution as it turns out. What does have a nice solution is \\[p(\\sigma^2 | \\theta, y_1, …, y_n)\\sim\\text{InverseGamma}( \\nu_n/2 , \\nu_n \\sigma_n^2(\\theta) / 2 )\\] \\[ \\nu_{n}=\\nu_{0}+n \\quad \\text { and } \\quad \\sigma_{n}^{2}(\\theta)=\\frac{1}{\\nu_{n}}\\left[\\nu_{0} \\sigma_{0}^{2}+n s_{n}^{2}(\\theta)\\right] \\] and \\(s_{n}^{2}(\\theta)=\\sum\\left(y_{i}-\\theta\\right)^{2} / n\\), the unbiased estimator of \\(\\sigma^{2}\\) if \\(\\theta\\) were known. The \\(\\sigma_n^2(\\theta)\\) parameter is a weighted average of \\(\\sigma_0^2\\) and the sample variance around \\(\\theta\\). 10.3 Gibbs sampling Here’s our predicament; to sample from the posterior \\(p(\\theta, \\sigma^2 | y_1, …, y_n)\\) it would be sufficient to be able to sample from \\(p(\\theta | y_1, …, y_n)\\) and \\(p(\\sigma^2 | \\theta, y_1, …, y_n)\\). But the marginal of \\(\\theta\\) is not nice. Similarly, we could also sample from the posterior \\(p(\\theta , \\sigma^2 | y_1, …., y_n)\\) if we could sample from \\(p(\\theta | \\sigma^2, y_1, …, y_n)\\) and \\(p(\\sigma^2 | y_1, ….,y_n)\\), but the marginal of \\(\\sigma^2\\) is also not nice. So we have answers for both of the conditionals i.e., \\(p(\\sigma^2 | \\theta, y_1, …, y_n)\\) and \\(p(\\theta | \\sigma^2, y_1, …, y_n)\\), but not either of the marginals. Just thinking about a pair of random variables call them \\((X_1, X_2)\\). I can write their joint density as \\(f(x_1, x_2) = f(x_1)f(x_2 | x_1)\\) or \\(f(x_1, x_2) = f(x_2 )f(x_1 | x_2)\\). But if I don’t know \\(f(x_1)\\) or \\(f(x_2)\\) but I know both of \\(f(x_1|x_2)\\) and \\(f(x_2 | x_1)\\). what can I do with this? I can do a Gibbs sampler! Let’s pretend we had a draw from \\(\\sigma^{2(1)} \\sim p(\\sigma^2 | y_1,...,y_n ),\\) then we could sample \\[\\theta^{(1)} \\sim p(\\theta | \\sigma^{2(1)}, y_1,...,y_n)\\] and \\((\\theta, \\sigma^2)^{(1)}\\) would be a sample from the joint distribution \\(p(\\theta, \\sigma | y_1,...,y_n).\\) Additionally, \\(\\theta^{(1)}\\) can be considered a sample from the marginal distribution \\(p(\\theta | y_1,...,y_n).\\) From this \\(\\theta\\)-value, we can generate \\[\\sigma^{2(2)} \\sim p(\\sigma^2 | \\theta^{(1)}, y_1,...,y_n)\\] so now we got \\((\\theta^{(1)}, \\sigma^{2(2)}) \\sim p(\\theta , \\sigma^2 | y_1,...,y_n),\\) thus \\(\\sigma^{2(2)} \\sim p(\\sigma^2 | y_1,...,y_n).\\) Now sample \\(\\theta^{(2)} \\sim p(\\theta | \\sigma^{2(2)}, y_1,...,y_n)\\) etc. So the answer to the question : What can we do with both conditional distributions (but neither marginal distribution)? “with” here means “the means to simulate samples from.” Well if we could just get a starting point \\(\\sigma^{2(1)}\\) we could simulate a sequence such that each element in this sequence is marginally drawn from the joint posterior distribution \\(p(\\theta, \\sigma^2 | y).\\) One complication here is that the draws would not be independent. You see why? \\(\\theta^{(1)} \\sim p(\\theta | y), \\theta^{(2)} \\sim p(\\theta | y)\\) but they are not independent because \\(\\theta^{(2)}\\) is drawn conditionally on \\(\\sigma^{2(2)}\\) and \\(\\sigma^{2(2)}\\) is drawn conditionally on \\(\\theta^{(1)}\\) and as a result there is dependence between \\(\\theta^{(2)}\\) and \\(\\theta^{(1)}\\). We’ll worry about that tomorrow. This iterative sampling for the iteratively updated conditional distributions is called the Gibbs sampler. We’ll define it here in our 2-parameter model. The algorithm goes: sample \\(\\theta^{(s+1)} \\sim p\\left(\\theta \\mid \\sigma^{2(s)}, y_{1}, \\ldots, y_{n}\\right)\\); sample \\(\\sigma^{2(s+1)} \\sim p\\left(\\sigma^{2} \\mid \\theta^{(s+1)}, y_{1}, \\ldots, y_{n}\\right)\\); let \\(\\phi^{(s+1)}=\\left(\\theta^{(s+1)}, \\sigma^{2(s+1)}\\right)\\). The code: n &lt;- length(y) ybar &lt;- mean(y) s2 &lt;- var(y) S &lt;- 1000 phi &lt;- matrix(NA, S, 2) # starting values theta &lt;- ybar # sample mean sigma2 &lt;- (nu.0*sigma2.0 + (n-1)*s2) / (nu.0 + n) for(s in 1:S){ tau2.n &lt;- 1 / (1/tau2.0 + n/sigma2) mu.n &lt;- tau2.n * (mu.0/tau2.0 + n*ybar/sigma2) theta &lt;- rnorm(1, mean=mu.n, sd=sqrt(tau2.n)) sigma2 &lt;- 1/rgamma(1, (nu.0 + n)/2, (nu.0*sigma2.0 + (n-1)*s2 + n*(ybar-theta)^2)/2) phi[s,] &lt;- c(theta, sigma2) } This R code assumes we have a data vector \\(\\boldsymbol{y}\\) and parameter variables \\(\\mu_0=\\texttt{mu.0}\\), \\(\\tau_0^2=\\texttt{tau2.0}\\) (the parameters of the normal prior on \\(\\theta\\)) \\(\\nu_0=\\texttt{nu.0},\\) \\(\\sigma_0^2=\\texttt{sigma2.0}\\)(parameters of the inverse gamma prior on \\(\\sigma^2\\)). As we learn more of these Markov chain Monte Carlo methods we’ll see this structure more and more. Where the simulation step is not done by theta.sim &lt;- r&quot;dist&quot; ( S, … ) but rather it is done by for-loops because each draw depends on the previous draw. The object \\(\\phi=\\texttt{phi}\\) in this code is the matrix of simulations. The \\(s\\)th row of \\(\\texttt{phi}\\) is the \\(s\\)th iteration of the Gibbs sampler. The first column of \\(\\texttt{phi}\\) is the \\(\\theta\\)-components the second column is the \\(\\sigma^2\\) components. See Hoff’s book. He does mean and precision. Note: \\(s_{n}^{2}(\\theta)=\\sum\\left(y_{i}-\\theta\\right)^{2} / n\\),so {\\(\\sigma^2 | \\theta, y\\)} depends on \\(\\sum (y_i - \\theta)^2.\\) Recalculating this for every updated \\(\\theta\\) is inefficient. Instead of calculating \\(\\sum (y_i - \\theta)^2\\) for each updated \\(\\theta\\) value we use \\(s_{n}^{2}(\\theta)=(n-1)s^2+n(\\bar y - \\theta)^2\\) and only recalculate \\((\\bar y - \\theta)^2\\). 10.4 Example: Midge wing length Our prior information about these insects leads us to expect mean \\(\\texttt{mu.0 = 1.9}\\) What variance to put on that? the logic that went into \\(\\texttt{tau2.0 = .95^2?}\\) was to give low prior probability to \\(\\theta &lt; 0.\\) Expected sd around 0.10 or so, so set \\(\\sigma_0^2= (.10)^2 = .01\\) and then set \\(\\nu_0 = 1.\\) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,3)) # First the joint dist of (theta, sigma2) plot(phi, type=&quot;l&quot;, col=&quot;gray&quot;, xlab=&quot;theta&quot;, ylab=&quot;sigma2&quot;) points(phi, pch=19, cex=.75) # Now the marginal dist of theta plot(density(phi[,1]), lwd=2, xlab=&quot;theta&quot;, ylab=&quot;p(theta|y1,...,yn)&quot;, main=&quot;&quot;) abline(v= quantile(phi[,1], c(.025, .975)), lty=2) # Now the marginal of sigma2 plot(density(phi[,2]), lwd=2, xlab=&quot;sigma2&quot;, ylab=&quot;p(sigma2|y1,...,yn)&quot;, main=&quot;&quot;) Figure 10.1: The ﬁrst panel shows 1,000 iterations of the Gibbs sampler. The second and third panels give kernel density estimates to the distributions of Gibbs samples of theta and sigma2. Vertical dashed bars on the second plot indicate 2.5% and 97.5% quantiles of the Gibbs samples of theta. The left-most is a scatterplot. The “point cloud” represents the empirical joint posterior! The marginal of \\(\\theta\\) is not Normal but it appears symmetric and bell shaped so that’s nice. The marginal \\(p(\\sigma^2 | y_1, …, y_n)\\) is right-skewed the peak is around .02 or so. The sample variance was .017 so I guess this makes sense. There’s another thing in the scatterplot that we wouldn’t normally draw (and we wouldn’t draw these lines in our data analysis reports either) they’re just to illustrate the path that the draws have taken. What would it look like if we did a plot like this but with independent draws? I believe it would be uglier than this. Let’s see.. Let’s go back one class for an example where we could do independent simulations So the difference between these two picture. In the left hand side the draws are independent, i.e., \\(\\theta^{(s)}\\) is independent of \\(\\theta^{(s-1)}.\\) In the right hand plot there is serial dependence. Is that apparent? I think not really. In this case (the normal model with Gibbs sampling) the dependence between \\(\\theta^{(s)}\\) and \\(\\theta^{(s-1)}\\) is VERY weak. # Confidence interval for population mean quantile(phi[,1], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 1.716 1.806 1.903 # Confidence interval for population variance quantile(phi[,2], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 0.007533 0.017341 0.053634 # Confidence interval for population standard deviation quantile(sqrt(phi[,2]), c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 0.08679 0.13169 0.23159 10.5 Discrete approximation of posterior distribution Let’s be real general here. Suppose you have a single-parameter \\(\\theta\\). You can write the posterior \\(p(\\theta|y) = c\\times p(\\theta)p(y | \\theta) = p(\\theta) p(y | \\theta) / p(y). ~~ p(y)\\) depends on integrating \\(\\theta\\) out of the numerator of this thing. That may be hard. So here’s a thing you can do. Pick a bunch of \\(\\theta\\) values, \\(\\theta^{(1)} &lt; \\theta^{(2)} &lt; … &lt; \\theta^{(S)}\\). I’m using the notation of MC simulation but it’s not that these are fixed points. \\(Pr(\\theta &lt; \\theta^{(1)} |y) = 0\\) \\(Pr(\\theta &gt; \\theta^{(S)} | y) = 0\\) If those two conditions are met and \\(|\\theta^{(s)} - \\theta^{(s-1)}|\\) is small then the continuous distribution \\(p(\\theta | y)\\) can be well approximated by the discrete distribution \\(p(\\theta^{(s)} | y)\\) for \\(s = 1, …, S\\) and the discrete distribution can be computed exactly because I can compute \\(p(\\theta^{(s)}) p(y | \\theta^{(s)})\\) for each value of \\(\\theta^{(s)}.\\) Divide each entry by the sum of all entries and the sum of the entries becomes 1 so it’s a probability distribution! You did something like this on your first HW assignment. For the mixture distribution posterior you calculated it at a bunch of points between 0 and 1. Now suppose you had two parameters \\(p(\\theta_1, \\theta_2 | y)\\). You can do the exact same thing except it doesn’t require double the computation. What does it require? If I split the range of \\(\\theta\\) into 100 points. I had to compute the posterior 100 times. If I split the range of \\(\\theta^{(1)}\\) and the range of \\(\\theta^{(2)}\\) into 100 points each, I have to calculate the posterior \\(100^2\\) times. Still fine. What if I had 16 parameters \\((\\theta_1, \\theta_2, …., \\theta_{16} ) = \\boldsymbol\\theta\\). Then I couldn’t do discrete approximation any more but I could still do a Gibbs sampler (or some other MCMC approach). So that’s going to become our go-to method. See Hoff chapter 6.2 for more information on discrete approximation. 10.6 Example The R-code below evaluates \\(p(\\theta, 1/\\sigma^2| y_1 ,..., y_n )\\) on a \\(121\\times250\\) grid of evenly \\(1/\\sigma^2\\) spaced parameter values, with \\(\\theta \\in \\{{ 1.500, 1.505, . . . , 2.095, 2.100 \\}}\\) and \\(1/\\sigma^2 \\in \\{ 1, 2, . . . , 249, 250 \\}\\). theta &lt;- seq(1.5, 2.1, .005) I.sig2 &lt;- seq(1, 250, 1) G &lt;- length(theta); H &lt;- length(I.sig2); #121 #250 log.post &lt;- matrix(NA, G, H); for(g in 1:G) { for(h in 1:H) { log.post[g,h] &lt;- dnorm(theta[g], mu.0, 1/sqrt(tau2.0), log=T) + dgamma(I.sig2[h], nu.0/2, nu.0*sigma2.0/2, log=T) + sum(dnorm(y, theta[g], 1/sqrt(I.sig2[h]), log=T)) }} post.grid &lt;- exp(log.post); rm(log.post); post.grid &lt;- post.grid / sum(post.grid) par(mar=c(3,3,3,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,3)) contours &lt;- c(.001, .01, seq(.05, .95, .10)) * max(post.grid) contour(theta, I.sig2, post.grid, levels=contours, drawlabels=F, xlab=&quot;theta&quot;, ylab=&quot;1/sigma2&quot;, main=&quot;Joint distribution&quot;) plot(theta, apply(post.grid, 1, sum), type=&quot;l&quot;, lwd=2, xlab=&quot;theta&quot;, ylab=&quot;Probability&quot;, main=&quot;Marginal of mean&quot;) plot(I.sig2, apply(post.grid, 2, sum), type=&quot;l&quot;, lwd=2, xlab=&quot;1/sigma2&quot;, ylab=&quot;Probability&quot;, main=&quot;Marginal of precision&quot;) The first panel gives the discrete approximation to the joint distribution of \\((\\theta, 1/\\sigma^2).\\) "],["mcmc-diagnostics.html", "Lecture 11 MCMC diagnostics 11.1 The Gibbs sampler 11.2 Distinguishing estimation from approximation 11.3 Introduction to MCMC diagnostics 11.4 Discussion", " Lecture 11 MCMC diagnostics The following notes, mostly transcribed from Neath(0518,2021) lecture, summarize sections(6.5 and 6.6) of Hoff(2009). 11.1 The Gibbs sampler We had a posterior distribution \\(p(\\theta, \\sigma^2 | y)\\) where the full conditional distributions \\(p(\\theta | \\sigma^2, y)\\) and \\(p(\\sigma^2 | \\theta, y)\\) took a convenient form, but neither marginal distribution did. In a Gibbs sampler you alternately draw from both “full conditional” distributions and the result is; as \\(s\\) increases the sampling distribution of \\((\\theta, \\sigma^2)^{(s)}\\) approaches the target distribution (which is the posterior). The Gibbs sampler we saw last time was very simple, just two parameters, but the idea extends to \\(p\\) parameters. Let \\(\\boldsymbol{\\phi} = (\\phi_1,\\phi_2,...,\\phi_p)\\) be a vector of parameters. Let \\(p(\\boldsymbol{\\phi}) = p(\\phi_1,\\phi_2,...,\\phi_p)\\) be the target distribution and the goal is to approximate probabilities and moments and quantiles etc with respect to this target distribution. In Bayesian statistics the target distribution is generally the posterior distribution \\(p(\\boldsymbol{\\phi} | y)\\). Often in today’s class you’ll notice we’re just writing \\(p(\\boldsymbol{\\phi})\\) to denote a generic target distribution. But it’s probably (in practical application) the posterior not the prior. We are using subscripts to denote the position in a vector \\(\\boldsymbol{\\phi} = (\\phi_1, …, \\phi_p)\\) and superscripts to denote the iteration number of the simulation. Superscripts are in parentheses so we don’t think it means “raised to that power.” The way the Gibbs sampler works is; each iteration of the Gibbs sampler itself requires \\(p\\) steps (\\(p=2\\) in yesterday’s example). Exercise: Write down the form of the \\(j\\)th update of the gibbs sampler \\(\\phi_j^{(s)} \\sim p(\\phi_j | \\phi_1^{(s)}, …, \\phi_{j-1}^{(s)}, \\phi_{j+1}^{(s-1)}, …., \\phi_p^{(s-1)} )\\) We are conditioning on parameters that have been updated as well as those that are yet to be updated. The set of conditional distributions \\(p(\\phi_j | \\phi_1, …, \\phi_{j-1}, \\phi_{j+1}, …., \\phi_p )\\) for all of \\(j = 1, 2, … p,\\) are collectively called the full conditional distributions for the target distribution \\(p(\\phi_1, …., \\phi_p)\\). You see why it’s a dependent sample? Take the semiconjugate normal model to illustrate. \\(\\theta^{(s)}\\) depends on \\(\\sigma^{2(s-1)}\\) but \\(\\sigma^{2(s-1)}\\) depends on \\(\\theta^{(s-1)}\\) therefore \\(\\theta^{(s)}\\) depends on \\(\\theta^{(s-1)}\\). The Gibbs sampler output is generally less good than would be ordinary Monte Carlo simulations (that is independent draws). However, the Gibbs sampler method works in more complicated models where direct simulation may not be feasible. How does (in very broad terms) Markov chain Monte Carlo relate to ordinary Monte Carlo? It’s not as good because (1) the draws are dependent and (2) they don’t have the right sampling distribution exactly. Note this property; \\(\\boldsymbol \\phi^{(s)}\\) depends on \\(\\boldsymbol \\phi^{(s-1)}\\) and \\(\\boldsymbol \\phi^{(s-1)}\\) is dependent on \\(\\boldsymbol \\phi^{(s-2)}\\) therefore \\(\\boldsymbol \\phi^{(s)}\\) is dependent on \\(\\boldsymbol \\phi^{(s-2)}\\). By the principle of induction \\(\\boldsymbol \\phi^{(s)}\\) is dependent on \\(\\boldsymbol \\phi^{(0)}\\). However, \\(\\boldsymbol \\phi^{(s)}\\) is conditionally independent of \\(\\boldsymbol \\phi^{(s-2)}\\) and \\(\\boldsymbol \\phi^{(s-3)}\\) etc given \\(\\boldsymbol \\phi^{(s-1)}\\). If you’ve taken stochastic processes (stat 4207 / 5207) you’ve seen this idea before you know this is called the Markov property(evolution of the Markov process in the future depends only on the present state and does not depend on past history). So the Gibbs sampler produces a realization of a Markov chain and such is an example of a more general method called Markov chain Monte Carlo. If \\(\\boldsymbol{\\phi}^{(0)} \\sim p(\\boldsymbol{\\phi})\\)(the right target distribution) then \\(\\boldsymbol{\\phi}^{(1)} \\sim p(\\boldsymbol{\\phi})\\) and \\(\\boldsymbol{\\phi}^{(2)} \\sim p(\\boldsymbol{\\phi})\\) etc. In general this will not be the case. The starting point \\(\\boldsymbol{\\phi}^{(0)}\\) is determined somehow, but it’s not a draw from the target distribution. Therefore, the marginal distribution of \\(\\boldsymbol{\\phi}^{(s)}\\) is NOT \\(p(\\boldsymbol{\\phi})\\), i.e., is not the target distribution. However, as \\(s\\) increases the marginal distribution of \\(\\boldsymbol{\\phi}^{(s)}\\) approaches the target distribution. Below is a technical statement of this property; \\[ Pr(\\boldsymbol\\phi^{(s)} \\in A) \\rightarrow \\int_A p(\\boldsymbol\\phi)d \\boldsymbol\\phi \\quad \\text{ as } s \\rightarrow \\infty \\] You know about the Law of Large Numbers (LLN) for independent draws. This is the LLN for Markov chains; \\[\\frac{1}{S} \\sum_{s=1}^{S} g\\left(\\boldsymbol\\phi^{(s)}\\right) \\rightarrow \\mathrm{E}[g(\\boldsymbol\\phi)]=\\int g(\\boldsymbol{\\phi}) p(\\boldsymbol{\\phi}) d \\boldsymbol{\\phi} \\quad \\text { as } S \\rightarrow \\infty\\] In the midge data example we did 1000 iterations of the Gibbs sampler for the semiconjugate normal model and from that we approximate posterior mean by \\(1.808\\) and posterior 95% interval by \\([1.72, 1.90]\\). 11.2 Distinguishing estimation from approximation What role does MCMC (or Monte Carlo in general) play in a Bayesian analysis? It is not an inferential method. The inferential method is Bayesian inference. We have a sampling model for our observable data \\(p(\\boldsymbol{y}|\\phi)\\), we have a probability distribution that describes our prior belief about \\(\\phi, ~p(\\phi)\\). Once these items are specified according to Bayes rule our belief about \\(\\phi\\) is “updated” to reflect the observed data; \\(p(\\phi | \\boldsymbol{y}) = p(\\phi) p(\\boldsymbol{y} | \\phi) / p(\\boldsymbol{y})\\). So what is the Gibbs sampler used for? The problem is \\(p(\\phi | \\boldsymbol{y})\\) may be a very complicated object particularly if \\(\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, …., \\phi_p)\\) and \\(p\\) is a big number. That’s where Monte Carlo comes in. Monte Carlo is a computational tool for describing features of the posterior distribution. Confusion comes about because Monte Carlo is an approximation method and is based on the principles of statistical inference. If \\(\\phi\\) is a scalar the average value of the simulated \\(\\phi^{(s)}\\) is an approximation to the posterior mean \\(E(\\phi|\\boldsymbol{y}).\\) Sample quantiles of \\(\\phi^{(1)} ,..., \\phi^{(S)}\\) approximate \\(L\\) and \\(U\\) such that \\(Pr(L &lt; \\phi &lt; U | y) = 1-\\alpha\\). It is better we not use the term estimation for this purpose and refer to such approximations as Monte Carlo approximations. The inferential problem is; what do we know about \\(\\phi\\) after observing the data \\(\\boldsymbol{y}?\\) and that problem is completely answered by the posterior probability distribution \\(p(\\phi | \\boldsymbol{y}).\\) Where Monte Carlo methods come in is as a tool for helping us understand this complex object that is \\(p(\\phi | \\boldsymbol{y}).\\) Again, distinction between estimation and approximation; The estimation problem(the inference problem) is in principle solved the minute we write down \\(p(\\phi|\\boldsymbol{y}) = c \\times p(\\phi) p(\\boldsymbol{y}|\\phi).\\) But in practice in order to make useful statements about this posterior distribution we use various computational tools including Monte Carlo simulation and Markov chain Monte Carlo (like the Gibbs sampler). So the distinction is estimation (inference) is solved by Bayes rule and approximation is where Monte Carlo comes in. Student question - Suppose we take the .025 and .975 quantiles of a 1000 iterations of a Gibbs sampler and call that our 95% confidence interval for \\(\\theta\\). Is that estimation or approximation? The answer is: Both are going on here. The true value of \\(\\theta\\) is unknown. The true quantiles \\([\\theta_{.025}, \\theta_{.975}]\\) that satisfy \\(Pr( L &lt; \\theta &lt; U | y ) = 0.95\\) is a solution to the estimation problem. When \\(\\theta_{.025}\\) and \\(\\theta_{.975}\\) are not solvable exactly because the posterior distribution is too complicated and we use the sample quantile from a simulation based on Gibbs sampler that’s an approximation. 11.3 Introduction to MCMC diagnostics We never talked about ordinary Monte Carlo diagnostics because there’s no such thing. What MCMC diagnostics is concerned with is those two features of MCMC that make it less good than ordinary Monte Carlo: \\(\\theta^{(s)}\\) is not exactly marginally \\(\\sim p(\\theta | y)\\) only approximately (with this approximation improving as \\(s\\) increases). The draws are not independent, they are positively correlated. Recall that if \\(X_1\\) and \\(X_2\\) have the same mean and variance then \\(\\text{Var}( [ X_1+X_2]/2 ) = \\text{Var}(X) / 4\\) if they are independent. But is greater than that if they are positively correlated \\(\\text{Var}( [X_1+X_2]/2 ) = \\text{Var}(X_1) /4 + \\text{Var}(X_2) /4 + 2\\text{Cov}(X_1, X_2) / 4.\\) So if that covariance is positive, the average of two draws is still probably better(lower variance) than a single draw but not by as much as if \\(X_1\\) and \\(X_2\\) were independent. Let \\(\\phi\\) be a “parameter,” \\(p(\\phi)\\) is the target distribution which is probably a posterior. The gold standard is ordinary Monte Carlo in which \\(\\phi^{(s)} \\stackrel{\\text{ iid }} \\sim ~ p(\\phi)\\). That means each has sampling distribution that is exactly \\(p(\\phi)\\) and the draws are independent. The two ways Gibbs sampling (and MCMC more generally) is less good than ordinary (iid) Monte Carlo are; The sampling distribution of \\(\\phi^{(s)}\\) is exactly \\(p(\\phi)\\) in ordinary Monte Carlo but only approximately so under MCMC. Draws are independent under ordinary Monte Carlo but positively correlated in a Gibbs sampler. This will be illustrated by the example below. 11.3.1 Example: mixture of normal densities Consider the following target distribution \\(\\boldsymbol{\\phi} = (\\delta, \\theta), ~ \\delta \\in \\{1,2,3\\}\\) with probability { 0.45, 0.10, 0.45 } \\(\\{\\theta | \\delta = 1 \\}\\sim \\text{Normal}(-3 , 1/3 )\\) \\(\\{\\theta | \\delta=2\\} \\sim \\text{Normal}( 0, 1/3 )\\) \\(\\{\\theta | \\delta=3\\} \\sim \\text{Normal}( 3 , 1/3 )\\). This is a mixture distribution (mixture of three normals) and simulating draws from a mixture distribution is straightforward: If \\(\\delta \\sim\\) 1 or 2 or 3 with probability .45 or .10 or .45, then \\(\\theta | \\delta \\sim \\text{Normal}( \\mu_\\delta, \\sigma^2)\\). In this case the “mixture of 3 normals” distribution has three modes. Though this is not always the case. In your homework problem the prior mixture of two betas had two modes but the posterior mixture of two betas does not. Student question: Is there a way to know in general whether a mixture distribution will have multiple modes or not? In the case of a mixture of normals it depends on how far apart the means are relative to the variances. Here’s a rule for sampling from a mixture distribution: Simulate \\(\\delta^{(s)} \\sim p(\\delta)\\) and \\(\\theta^{(s)} \\sim p(\\theta | \\delta^{(s)} )\\) result; \\((\\delta^{(s)}, \\theta^{(s)}) \\sim p(\\delta,\\theta)\\). Also marginally \\(\\theta^{(s)} \\sim p(\\theta)\\). Below is what a mixture distribution looks like. Its density is \\(\\{ [p_1\\times \\texttt{dnorm}(\\theta, \\mu_1, \\sigma)] + [p_2 \\times\\texttt{dnorm}(\\theta | \\mu_2, \\sigma)] + [p_3 \\times \\texttt{dnorm} (\\theta | \\mu_3 , \\sigma)] \\}\\) S &lt;- 1000 mu &lt;- c(-3, 0, 3); sigma &lt;- rep(1/sqrt(3),3); p &lt;- c(.45, .10, .45) delta.MC &lt;- sample(3, S, replace=T, prob=p) theta.MC &lt;- rnorm(S, mu[delta.MC], sigma[delta.MC]) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) hist(theta.MC, freq=F, right=F, col=&quot;pink&quot;, xlim=c(-6,6), ylim=c(0, .32), breaks=30, xlab=expression(theta), ylab=expression(p(theta)), main=&quot;&quot;) theta.vals &lt;- seq(-6, 6, .01) p.theta &lt;- p[1] * dnorm(theta.vals, mu[1], sigma[1]) + p[2] * dnorm(theta.vals, mu[2], sigma[2]) + p[3] * dnorm(theta.vals, mu[3], sigma[3]) lines(theta.vals, p.theta, lwd=2) Figure 11.1: A mixture of normal densities and a Monte Carlo approximation In this picture, the curve represents the target distribution i.e., the exact marginal density of \\(p(\\theta) = \\sum_\\delta p(\\theta|\\delta)p(\\delta)\\). The histogram (empirical distribution) is 1000 independent samples from the target distribution. Agreement is pretty good. Let’s do a Gibbs sampler for this problem. In ordinary Monte Carlo we can get independent samples by going; \\(\\delta^{(s)} \\sim p(\\delta), ~ \\theta^{(s)} \\sim p(\\theta | \\delta^{(s)}).\\) A Gibbs sampler would go; (Though this is a practically silly thing to do in this problem because in fact ordinary Monte Carlo is more straightforward than the Gibbs sampler.) \\(\\delta^{(s)} \\sim p(\\delta | \\theta^{(s-1)}),\\) then \\(\\theta^{(s)} \\sim p(\\theta | \\delta^{(s)}).\\) However, \\(\\delta^{(s)}\\) depends on \\(\\theta^{(s-1)}\\) and therefore \\(\\theta^{(s)}\\) is dependent on \\(\\theta^{(s-1)}\\). Full conditionals The full conditional distribution of \\(\\theta\\) is \\(p(\\theta \\mid \\delta)=\\texttt{dnorm}\\left(\\theta \\mid \\mu_{\\delta}, \\sigma\\right)\\) where \\(\\left(\\mu_{1}, \\mu_{2}, \\mu_{3}\\right)=(-3,0,+3)\\) with probabilities \\((.45, .10, .45)\\), and \\(\\sigma^{2}=1 / 3\\) Using Bayes’ rule we can show that the full conditional distribution of \\(\\delta\\) is given by \\[ \\begin{aligned} p(\\delta=k | \\theta) &amp;= p(\\delta)p(\\theta|\\delta) / p(\\theta)= \\frac{p(\\delta)p(\\theta|\\delta)}{\\sum_\\delta p(\\theta|\\delta)p(\\delta)}\\\\ &amp;= \\frac{Pr(\\delta=k)\\texttt{dnorm}(\\theta|\\mu_k,\\sigma)}{\\sum_{d=1}^3 Pr(\\delta=d)\\texttt{dnorm}(\\theta|\\mu_d,\\sigma)}, \\quad k=1,2,3 \\end{aligned} \\] Since \\(\\delta \\in \\{1,2,3\\},\\) let’s start it at \\(3.\\) # Gibbs sampler delta.Gibbs &lt;- rep(NA, S) theta.Gibbs &lt;- rep(NA, S) delta &lt;- 3 # starting value for(s in 1:S) { theta &lt;- rnorm(1, mu[delta], sigma[delta]) pdgt &lt;- p * dnorm(theta, mu, sigma) delta &lt;- sample(3, 1, prob=pdgt) delta.Gibbs[s] &lt;- delta theta.Gibbs[s] &lt;- theta } hist(theta.Gibbs, freq=F, right=F, col=&quot;pink&quot;, xlim=c(-6,6), breaks=30, # ylim=c(0, .32), breaks=30, xlab=&quot;theta&quot;, ylab=&quot;p(theta)&quot;, main=&quot;&quot;) theta.vals &lt;- seq(-6, 6, .01) p.theta &lt;- p[1] * dnorm(theta.vals, mu[1], sigma) + p[2] * dnorm(theta.vals, mu[2], sigma) + p[3] * dnorm(theta.vals, mu[3], sigma) lines(theta.vals, p.theta, lwd=2) Figure 11.2: A mixture of normal densities and 1000 Gibbs samples Here, 1000 draws from the Gibbs sampler do not agree very closely with the target distribution. Namely, values close to -3 are way overrepresented, values close to zero are overrepresented, values close to +3 are underrepresented. It’s a mixture of the right 3 things but it’s not the right mixture. What went wrong? par(mfrow=c(1,2)) plot(1:S, theta.MC, type=&quot;l&quot;, xlab=&quot;Iteration s&quot;, ylab=&quot;theta[s]&quot; ,main = &quot;Monte Carlo trace plot&quot;) plot(1:S, theta.Gibbs,type=&quot;l&quot;,xlab=&quot;Iteration s&quot;,ylab=&quot;theta[s]&quot; ,main = &quot;Gibbs sampler trace plot&quot;) Figure 11.3: Trace plots of theta-sequence What went wrong is that Gibbs sampler draws are highly correlated on RHS so there is a very strong tendency for \\(\\theta^{(s)}\\) to be close to \\(\\theta^{(s-1)}\\). So when I get \\(\\theta^{(s)}\\) close to 3 there is very high probability that \\(\\theta^{(s+1)}\\) is also going to be close to 3. Compare this to the independent Markov chain Monte Carlo draws on LHS which are jumping all over the place. Result is even a very small value of \\(S\\) is likely to be representative of the target distribution under ordinary MC. With the Gibbs sampler it takes a LOT more simulation to get a representative sample. The target distribution; 45% of it is concentrated around 3, 10% concentrated around 0, 45% concentrated around -3. With independent samples that just happens. With the Gibbs sampler that happens in the long run but not so well in the short run. BIG IDEA: The information about \\(p(\\theta)\\) contained in \\(S\\) draws of a Gibbs sampler is less than the information about \\(p(\\theta)\\) contained in \\(S\\) independent draws. Recall that we have: \\(\\{\\theta | \\delta = 1 \\}\\sim \\text{Normal}(-3 , 1/3 )\\) \\(\\{\\theta | \\delta=2\\} \\sim \\text{Normal}( 0, 1/3 )\\) \\(\\{\\theta | \\delta=3\\} \\sim \\text{Normal}( 3 , 1/3 )\\). If \\(\\theta^{(s)}\\) is close to zero it is highly probable that we will get \\(\\delta^{(s)} = 2,\\) if we get \\(\\delta^{(s)} = 2\\) we expect \\(\\theta^{(s+1)}\\) should be close to zero and so on. We can basically tell from the trace plot for \\(\\theta\\) what the trace plot for \\(\\delta\\) would look like. Isn’t the Gibbs sampler guaranteed to eventually provide a good approximation? Yes it is. If we ran a lot more than 1000 draws, “eventually” we’d get about 45% of the time hovering around +3 about 10% hovering around 0 and about 45% hovering around -3. In the long run yes we would see exactly this, but \\(S = 1000\\) is apparently “short” for our example. 11.4 Discussion Concrete example: A particle moving around the parameter space. In this example the parameter is \\(\\phi\\) the parameter space is the real line but generally the probability in the target distribution is contained between -5 and 5. Think of \\(\\phi\\) as a particle moving around the line and where it is at iteration \\(s\\) is recorded in this plot (like the trace plot above). In the long run let \\(A_1, A_2\\) and \\(A_3\\) be three subsets of the parameter space so that \\(\\int_{A2}p(\\phi) d\\phi\\) is pretty small and \\(A_1\\) and \\(A_3\\) are separated by \\(A_2\\) (just like the example above!). In the long run our particle should spend little time in \\(A_2\\) but a lot more time in \\(A_1\\) and \\(A_3\\). Suppose we start in \\(A_2\\), two things we would want to see happen (1) the chain should move out of \\(A_2\\) pretty quickly (stationarity) (2) the chain to move between \\(A_1\\) and \\(A_3\\) fairly readily, that is, not get stuck for long stretches in either one (low autocorrelation). Definition: If \\(\\phi^{(0)} \\sim p(\\phi)\\) then the chain is stationary. And if the chain is stationary then the sampling distribution of \\(\\phi^{(s)}\\) is \\(p(\\phi)\\) (the target distribution) for every \\(s\\). And in that case: Issue number 1 (the “non-stationarity” issue) would be a non-issue. Regarding the stationarity issue, the practical issue presented is where to start? In the problems we’ve done so far this really hasn’t been an issue. A mode of the target distribution is a good starting point. The second issue, the autocorrelation issue, is also called mixing. Low autocorrelation and fast mixing go together, high autocorrelation and slow mixing go together. 11.4.1 How does autocorrelation(slow mixing) affect our MCMC approximation? For this discussion assume stationarity. With good starting values this is a reasonable assmption. We have \\(\\phi^{(0)} \\sim p(\\phi)\\) or close enough therefore \\(\\phi^{(s)} \\sim p(\\phi)\\) for each \\(s,\\) the only issue is the draws are not independent. How does that muck up our estimation? Suppose autocorrelation was zero i.e., independent samples. \\(\\bar \\phi = (1/S)[ \\phi^{(1)} + … + \\phi^{(S)} ]\\) is our Monte Carlo approximation to \\(E(\\phi)\\). In this notation \\(\\phi_0 = E(\\phi)\\). \\(\\phi\\) is the parameter (an uncertain quantity), \\(p(\\phi)\\) is the probability distribution, \\(\\phi_0\\) is the mean of that distribution. \\(\\phi^{(s)}\\) for \\(s = 1, …, S\\) are a set of \\(S\\) generated values being used to approximate \\(\\phi_0\\). If the \\(\\phi^{(s)}\\) are uncorrelated, then we can define Monte Carlo standard error (mcse) as \\(\\sqrt{\\text{Var}_{MC}}\\), where \\(\\text{Var}_{MC} = E[(\\bar \\phi - \\phi_0)^2] = \\text{Var}(\\phi)/S=\\) Expected squared approximation error associated with using \\(\\bar \\phi\\) to estimate \\(\\phi_0\\). \\(\\text{Var()}\\) with no subscript means the variance of the target distribution. Want to have high confidence (say 95%) that your approximation error will be less than \\(\\epsilon?\\) solve for \\(S\\) so that \\(2\\text{mcse} = 2\\sqrt{\\text{Var}_{MC}} &lt; \\epsilon\\). If we are using a Gibbs sampler so the \\(\\phi^{(s)}\\) are not independent (they’re correlated) the expected squared approximation error \\(\\text{Var}_{MCMC}\\) is not Var\\((\\phi) / S\\). It’s bigger. It’s bigger by a term that depends on how strongly correlated are \\(\\phi^{(s)}\\) and \\(\\phi^{(t)}\\) for \\(t \\neq s\\) “MCMC standard error” equals square root of ( expected squared approximation error under MCMC sampling ) i.e., \\(\\text{mcmcse} = \\sqrt{\\text{Var}_{MCMC}}\\), where \\(\\text{Var}_{MCMC} &gt; \\text{Var}_{MC}\\) by an amount that depends on how highly correlated are successive draws in the Gibbs sampler. The bigger is our expected squared approximation error \\((\\text{Var}_{MCMC})\\) the more we expect our approximation to not be good. 11.4.2 Autocorrelation par(mfrow=c(1,2)) acf(theta.MC) acf(theta.Gibbs) Figure 11.4: acf for independent MC draws and Gibbs samples Corr\\((\\phi^{(s)} , \\phi^{(s+1)})\\) is called the lag-1 autocorrelation Corr\\(( \\phi^{(s)} , \\phi^{(s+2)})\\) is called the lag 2 autocorrelation In general the lag-\\(t\\) autocorrelation is defined as Corr\\((\\phi^{(s)}, \\phi^{(s+t)})\\) and if we consider this as a function of \\(t,~ t = 1, 2, 3, …,\\) for a typical Gibbs sampler it is always positive and (generally) decreasing toward 0 as \\(t\\) increases. The faster it zeros out the better. The “art” of MCMC is finding samplers for which the autocorrelation zeros out quickly. Student Question: If positive autocorrelation is bad in the sense that it makes the Monte Carlo less efficient might there be a way to simulate negatively correlated samples to get more efficient Monte Carlo!? Methods like that exist but to be able to implement them requires that you know quite a lot about the target distribution and the whole point of MCMC is that you can do it in situations where you know very little about the target distribution. There is sampling technique called antithetic variables or something like this with this goal but it’s pretty limited. In high-dimensional complicated Bayesian models you’re stuck with this situation where the draws will be positively correlated but MCMC is very much an art and the art is to make that correlation as little as possible. The software package Stan uses something called Hamiltonian Monte Carlo which is an MCMC method but has the goal of producing chains that are not so highly autocorrelated. 11.4.3 Sample autocorrelation function \\[ \\operatorname{acf}_{t}(\\phi)=\\frac{\\frac{1}{S-t} \\sum_{s=1}^{S-t}\\left(\\phi_{s}-\\bar{\\phi}\\right)\\left(\\phi_{s+t}-\\bar{\\phi}\\right)}{\\frac{1}{S-1} \\sum_{s=1}^{S}\\left(\\phi_{s}-\\bar{\\phi}\\right)^{2}} \\] an expression for lag-t sample autocorrelation. Denominator is variance (recall that correlation = cov / sd\\(\\times\\)sd ) numerator is a sample correlation between.. say for lag \\(t=5\\), it will be between \\(\\phi^{(1)}\\) with \\(\\phi^{(6)}\\), \\(\\phi^{(2)}\\) with \\(\\phi^{(7)}\\), \\(\\phi^{(3)}\\) with \\(\\phi^{(8)}\\), etc. 11.4.4 Effective sample size \\(\\text{Var}_{MC}(\\bar \\phi) = \\text{Var}(\\phi) / S\\) The effective sample size for MCMC is defined by \\(\\text{Var}_{MC}(\\bar \\phi) = \\text{Var}(\\phi) / S_{\\text{eff}}.\\) So what this means is that when we run the Markov chain for \\(S\\) iterations we get the same precision for our MCMC approximation as we would get from \\(S_{\\text{eff}}\\) independent samples from \\(p(\\phi)\\) S &lt;- 10000 delta.Gibbs &lt;- rep(NA, S) theta.Gibbs &lt;- rep(NA, S) delta &lt;- 3; theta &lt;- 3; # starting values for(s in 1:S) { theta &lt;- rnorm(1, mu[delta], sigma[delta]) pdgt &lt;- p * dnorm(theta, mu, sigma) delta &lt;- sample(3, 1, prob=pdgt) delta.Gibbs[s] &lt;- delta; theta.Gibbs[s] &lt;- theta; } hist(theta.Gibbs, freq=F, right=F, col=&quot;pink&quot;, xlim=c(-6,6), breaks=30, ylim=c(0, .32), xlab=&quot;theta&quot;, ylab=&quot;p(theta)&quot;, main=&quot;&quot;) theta.vals &lt;- seq(-6, 6, .01) p.theta &lt;- p[1] * dnorm(theta.vals, mu[1], sigma[1]) + p[2] * dnorm(theta.vals, mu[2], sigma[2]) + p[3] * dnorm(theta.vals, mu[3], sigma[3]) lines(theta.vals, p.theta, lwd=2) Figure 11.5: 10,000 iterations of the Gibbs sampler instead of 1,000 Even with 10,000 iterations the realized chain is not perfectly representative of the target distribution. It’s too frequently around zero and not frequently enough around -3 and +3. There are numerous formulas for computing effective sample size they give different answers and in general the estimation of \\(S_{\\text{eff}}\\) is highly unstable. But it’s still a useful quantity. So let’s find it for this example. I like the effective sample size calculator that’s in the R package mcmcse library(mcmcse) ess(theta.Gibbs) ## [1] 13.54 The autocorrelation in this Markov chain is so severe that 10,000 iterations of the Gibbs sampler yields the same precision for estimating \\(E(\\theta)\\) as would only 13 or 14 independent draws from \\(p(\\theta)\\) (using iid Monte Carlo). CRAZY! par(mfrow=c(1,2)) acf(theta.Gibbs) acf(theta.Gibbs, lag.max =2000) Figure 11.6: autocorrelation function for theta-chain from the Gibbs sampler. We have \\(S-1\\) pairs as a basis for estimating lag 1 autocorrelation. It’s the correlation between \\((\\phi^{(1)} , …, \\phi^{(S-1)})\\) and \\((\\phi^{(2)} , …., \\phi^{(S)})\\). In general we have \\(S-t\\) data points to estimate the lag-\\(t\\) autocorrelation. At some point we run out of data points. When you get something like this in a problem you care about you have two choices (1) bigger \\(S\\). Sometimes that’s not feasible (2) figure out a better way to do Monte Carlo sampling. That’s the “art” of MCMC in practice. This was a toy example chosen precisely to make this point. One of the most MCMC-confounding situations we might encounter is multimodality. "],["multivariate-normal.html", "Lecture 12 Multivariate Normal 12.1 Example: Reading comprehension 12.2 The multivariate normal density 12.3 A semiconjugate prior distribution for the mean 12.4 The inverse-Wishart distribution 12.5 Full conditional distribution of the covariance matrix", " Lecture 12 Multivariate Normal The following notes, mostly transcribed from Neath(0520,2021) lecture, summarize sections(7.1-7.4) of Hoff(2009). 12.1 Example: Reading comprehension Our notation : \\(n =\\) sample size (number of subjects on which we have collected data) \\(p =\\) dimension of observed response vector (number of measurements taken on each subject) In this example, \\(n = 22, ~p = 2\\), \\(y_1 =\\) score on pretest, \\(y_2 =\\) score on posttest. For \\(p = 2\\) there are 5 parameters of interest; \\(\\theta_1 =\\) mean pretest scores, \\(\\theta_2 =\\) mean posttest scores, \\(\\sigma_1^2 =\\) variance of pretest scores, \\(\\sigma_2^2 =\\) variance of posttest scores, \\(\\rho = \\sigma_{1,2} / (\\sigma_1 \\times \\sigma_2)\\) the correlation between pretest scores and posttest scores. And we will assume that the pairs of test scores for a given student follow a bivariate normal distribution, even if they don’t follow perfectly. 12.2 The multivariate normal density With univariate data, there’s just one measurement taken on each subject. What are the concrete examples we have done so far? \\[ \\begin{aligned} y &amp;= \\text{number of tumors a mouse got}\\\\ y &amp;= \\begin{cases}{1 \\text{ if released is reincarcerated}\\\\ 0 \\text{ otherwise}} \\end{cases} \\end{aligned} \\] With multivariate data there are \\(p\\) measurements taken on each subject. So associated with each subject is a vector value \\(\\{ y_1, …, y_p \\}\\). If there are \\(p\\) components to the random variable there are \\(p\\) mean values, there are \\(p\\) variances, there \\(p(p-1)/2\\) covariances (the number of off-diagonal entries). \\(\\text{Cov}(Y_1, Y_2),\\text{Cov}(Y_1, Y_3),...,\\text{Cov}(Y_1, Y_p)\\) that’s \\(p-1\\) then we have \\(\\text{Cov}(Y_2, Y_3), \\text{Cov}(Y_2, Y_4),...,\\text{Cov}(Y_2, Y_p)\\) that’s \\(p-2\\) eventually we get to \\(\\text{Cov}(Y_{p-1} , Y_p ).\\) So the total number of covariances is \\(1 + 2 + … + p-1 = p(p-1) / 2\\). We can use the multivariate normal model if we want to make inference about mean values, variances(standard deviations) and correlations. This is what the density function for the multivariate normal distribution looks like; \\[ p(\\boldsymbol y | \\boldsymbol{\\theta,\\Sigma} ) = (2\\pi)^{-p/2}|\\boldsymbol \\Sigma|^{1/2}\\text{exp}\\{-(\\boldsymbol y - \\boldsymbol \\theta)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol y - \\boldsymbol \\theta)/2 \\} \\] When we have a multivariate distribution the mean becomes a mean vector(there is a mean for each variable). We’ll write \\(\\boldsymbol{\\theta}\\) in a bold font. \\(\\boldsymbol{\\theta}\\) is a \\(p \\times 1\\) vector, \\(\\theta_j = E( Y_j )\\). In univariate data in addition to the mean we have the variance. For multivariate data we have a variance-covariance matrix or just covariance matrix \\(= \\boldsymbol \\Sigma = \\text{Cov}[\\mathbf Y].\\) For a covariance matrix the \\(j\\)th diagonal entry is \\(\\sigma^2_j,\\) the variance for the \\(j\\)th variable. The \\((j, k)\\) entry of the covariance matrix is the covariance between \\(Y_j\\) and \\(Y_k\\). \\(\\sigma_{j,k} = \\text{Cov}( Y_j, Y_k )\\), \\(\\sigma_{j, j} = \\text{Var}( Y_j )\\). A subscript \\(i\\) means the \\(i\\)th of the \\(n\\) observations, so \\(\\boldsymbol y_i\\) is a \\(p \\times 1\\) vector. When I write \\(Y_j\\), I’m thinking of the distribution for the \\(j\\)th of the \\(p\\) variables. The exponential term of the univariate normal density is \\(-(1/2)(y - \\theta)^2 / \\sigma^2.\\) Write this as \\((y - \\theta) \\sigma^{-2} (y - \\theta).\\) This way generalizes to \\(p\\)-variate normal density \\((\\boldsymbol y-\\boldsymbol\\theta)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol y -\\boldsymbol\\theta)\\). The bigger are the entries of a matrix the bigger is the determinant. A matrix whose determinant is zero does not have an inverse matrix. Just as with univariate normal distribution where we require \\(\\sigma &gt; 0,\\) with MVN distribution determinant of the covariance matrix will always be positive, \\(|\\boldsymbol \\Sigma| &gt; 0\\). Just as \\(x \\times 1/x = 1, ~ \\mathbf{A A}^{-1} = \\mathbf{I},\\) which is the identity matrix. It has 1’s along the main diagonal and 0’s in the off-diagonal positions. \\(\\mathbf{A I} = \\mathbf{A} = \\mathbf{IA}\\) Bold-face capital letters (greek and latin both) will indicate matrices, bold-face lower-case letters will indicate vectors. A vector will always be a column vector, hence \\(\\mathbf{b}\\) is \\(p \\times 1\\), \\(\\mathbf{b}^T\\) is \\(1 \\times p\\). If \\(\\mathbf{b}\\) is a \\(p \\times 1\\) vector and \\(\\mathbf{A}\\) is a \\(p \\times p\\) matrix, then \\(\\mathbf{b}^T \\mathbf{A} \\mathbf{b}\\) is a scalar! \\((1 \\times p) (p \\times p ) (p \\times 1)\\). Similarly, \\(\\boldsymbol y\\) and \\(\\boldsymbol \\theta\\) are both \\(p\\)-vectors, \\(\\boldsymbol{\\Sigma}\\) is \\(p \\times p\\) matrix, \\((\\boldsymbol y - \\boldsymbol \\theta)\\) that’s a \\(p\\)-vector, \\((\\boldsymbol y - \\boldsymbol \\theta)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol y - \\boldsymbol \\theta)\\) that’s a scalar! With \\(p=2\\) (bivariate normal distribution) we can “draw” the bivariate normal density using contour plots. A contour is a collection of points \\((y_1, y_2)\\) such that \\(p(y_1, y_2) =\\) same for all such \\((y_1, y_2)\\) on the same contour. The concentric circles corresponds to a set of values that are 95% of the peak, 85% of the peak, down to 0.001% of the peak. y1 &lt;- seq(0, 100, 1) y2 &lt;- seq(0, 100, 1) G &lt;- length(y1); H &lt;- length(y2); theta &lt;- c(50,50) Sigma2 &lt;- matrix(c(64, 0, 0, 144), 2, 2) Sigma1 &lt;- Sigma2; Sigma1[1,2] &lt;- -48; Sigma1[2,1] &lt;- -48; Sigma3 &lt;- Sigma2; Sigma3[1,2] &lt;- +48; Sigma3[2,1] &lt;- +48; p1 &lt;- matrix(NA, G, H) p2 &lt;- matrix(NA, G, H) p3 &lt;- matrix(NA, G, H) for(g in 1:G){ for(h in 1:H){ p1[g,h] &lt;- dmvnorm(c(y1[g],y2[h]), mean=theta, sigma=Sigma1, log=T) p2[g,h] &lt;- dmvnorm(c(y1[g],y2[h]), mean=theta, sigma=Sigma2, log=T) p3[g,h] &lt;- dmvnorm(c(y1[g],y2[h]), mean=theta, sigma=Sigma3, log=T) }} maxie &lt;- max(p1); p1 &lt;- p1 - maxie; rm(maxie); p1 &lt;- exp(p1); maxie &lt;- max(p2); p2 &lt;- p2 - maxie; rm(maxie); p2 &lt;- exp(p2); maxie &lt;- max(p3); p3 &lt;- p3 - maxie; rm(maxie); p3 &lt;- exp(p3); contours &lt;- c(.001, .01, seq(.05, .95, .10)) op &lt;- par(mfrow=c(1,3)) contour(y1, y2, p1, levels=contours, drawlabels=F, xlab=&quot;y1&quot;, ylab=&quot;y2&quot;, main=&quot;rho=-0.5&quot;) points(rmvnorm(n=1000, mean=theta, sigma=Sigma1), pch=19, cex=.5) contour(y1, y2, p2, levels=contours, drawlabels=F, xlab=&quot;y1&quot;, ylab=&quot;y2&quot;, main=&quot;rho=0&quot;) points(rmvnorm(n=1000, mean=theta, sigma=Sigma2), pch=19, cex=.5) contour(y1, y2, p3, levels=contours, drawlabels=F, xlab=&quot;y1&quot;, ylab=&quot;y2&quot;, main=&quot;rho=+0.5&quot;) points(rmvnorm(n=1000, mean=theta, sigma=Sigma3), pch=19, cex=.5) Figure 12.1: Contour plots for bivariate normal densities Take the middle density for example. The peak is in the middle. The innermost counter is the collection of all points \\((y_1, y_2)\\) such that \\(p(y_1, y_2) = 0.95 \\times\\)max-density. The outermost contour is the set of all points \\((y_1, y_2)\\) such that \\(p(y_1, y_2) = 0.001\\times\\) max-density. These three distributions have the same \\(\\theta_1 ~(\\)mean of \\(Y_1)\\) \\(\\theta_2 ~(\\)mean of \\(Y_2)\\) \\(\\sigma_1 ~(\\)sd of \\(Y_1) ~\\sigma_2 (\\)sd of \\(Y_2)\\) they only differ with respect to their correlation. If you want to describe a picture like this: Tell the person where it’s centered, how spread out the values are (refer to \\(\\theta\\) and \\(\\sigma\\) values for help with this) and how the variables are correlated. The middle plot shows an instance where the variables are uncorrelated (independent). LHS plot shows a negative correlation. RHS plot shows a positive correlation. To describe the spread you’ll have to read the labels on the axes otherwise every picture of a normal distribution looks exactly the same. Case in point, if I drew two BVN densities both have \\(\\boldsymbol \\theta = (0, 0)\\) one has \\(\\boldsymbol \\Sigma = \\begin{pmatrix} 1, 0 \\\\ 0, 1 \\end{pmatrix}\\) and one has \\(\\boldsymbol \\Sigma = \\begin{pmatrix} 100, 0\\\\ 0, 100\\end{pmatrix},\\) the two pictures will appear identical. You have to read the labels on the axes to describe the spread in a normal distribution. If \\((Y_1, Y_2)\\) are bivariate normal then marginally, \\(Y_1 \\sim \\text{Normal}(\\theta_1, \\sigma_1^2),\\) \\(Y_2 \\sim \\text{Normal}(\\theta_2, \\sigma_2^2)\\), \\(\\{Y_2 | Y_1=y_1\\} \\sim\\) Normal. We’ll save that discussion for another day. 12.3 A semiconjugate prior distribution for the mean Let’s talk about Bayesian inference about the \\(p\\)-vector \\(\\boldsymbol \\theta\\) and the \\(p\\times p\\) covariance matrix \\(\\boldsymbol \\Sigma\\) in that order. Just like we did for univariate normal let’s condition on \\(\\boldsymbol\\Sigma\\) and find the conjugate prior for \\(\\boldsymbol \\theta.\\) Just as the conjugate prior for the univariate normal sampling model was the univariate normal distribution we will see that the conjugate prior for the mean of a multivariate normal sampling model is multivariate normal. Let \\(\\boldsymbol \\theta\\) have a \\(p\\)-variate normal distribution with mean vector \\(\\boldsymbol\\mu_0\\) and covariance matrix \\(\\boldsymbol \\Lambda_0\\) and let \\(\\{Y_1, …, Y_n | \\theta\\}\\) be iid \\(p\\)-variate Normal \\((\\boldsymbol\\theta, \\boldsymbol\\Sigma)\\) Let’s look at the prior first. I’m only interested in terms that include a \\(\\theta\\) \\[ \\begin{aligned} p(\\boldsymbol\\theta) &amp;\\propto \\text{exp}\\left\\{-\\frac{1}{2} \\boldsymbol\\theta^{T} \\mathbf{A}_{0} \\boldsymbol\\theta+\\boldsymbol\\theta^{T} b_{0}\\right\\} \\end{aligned} \\] where, \\(\\mathbf{A}_0 = \\boldsymbol\\Lambda_0^{-1}\\), \\(\\mathbf b_0 = \\boldsymbol\\Lambda_0^{-1} \\boldsymbol\\mu_0\\) then \\(\\boldsymbol\\mu_0 = \\boldsymbol\\Lambda_0 \\mathbf b_0 = \\mathbf{A}_0^{-1}\\mathbf b\\) Consequence: The general form of a MVN density is \\(p(\\boldsymbol\\theta) = c \\times \\text{exp}\\{ -0.5 \\boldsymbol\\theta^T \\mathbf{A} \\theta + \\boldsymbol\\theta^T \\mathbf{b} \\}\\) for some matrix \\(\\mathbf{A}\\) and vector \\(\\mathbf b\\). Next, the sampling distribution. \\[ p(y_1, …, y_n | \\theta) \\propto \\exp \\left\\{-\\frac{1}{2} \\boldsymbol{\\theta}^{T} \\mathbf{A}_{1} \\boldsymbol{\\theta}+\\boldsymbol{\\theta}^{T} \\boldsymbol{b}_{1}\\right\\} \\] where \\(\\mathbf{A}_{1}=n \\boldsymbol\\Sigma^{-1},\\) \\(\\mathbf{b}_{1}=n \\boldsymbol\\Sigma^{-1} {\\boldsymbol{\\bar y}},\\) and \\({\\boldsymbol{\\bar y}}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} y_{i, 1}, \\ldots, \\frac{1}{n} \\sum_{i=1}^{n} y_{i, p}\\right)^T\\). Combining the above we get that \\[\\{\\boldsymbol \\theta \\mid \\boldsymbol y_{1}, \\ldots, \\boldsymbol y_{n}, \\boldsymbol \\Sigma\\} \\sim \\operatorname{Normal}_{p}\\left(\\boldsymbol \\mu_{n}, \\boldsymbol \\Lambda_{n}\\right)\\] where \\(\\boldsymbol \\Lambda_{n}=\\left(\\boldsymbol \\Lambda_{0}^{-1}+n \\boldsymbol \\Sigma^{-1}\\right)^{-1}\\) and \\(\\boldsymbol \\mu_{n}=\\left(\\boldsymbol \\Lambda_{0}^{-1}+n \\boldsymbol \\Sigma^{-1}\\right)^{-1}\\left(\\boldsymbol \\Lambda_{0}^{-1} \\boldsymbol \\mu_{0}+n \\boldsymbol \\Sigma^{-1} \\bar{\\boldsymbol y}\\right)\\) Just as in the univariate case the posterior precision, or inverse variance, is the sum of the prior precision and the data precision, and the posterior expectation is a weighted average of the prior expectation and the sample mean, weighted by their respective precisions. 12.4 The inverse-Wishart distribution For univariate data, the conjugate prior for variance was the inverse-gamma distribution. For multivariate data we have a covariance matrix. A covariance matrix has \\(p^2\\) entries however it must be symmetric, which means \\(\\sigma_{j,k} = \\sigma_{k,j}\\). So it’s not really \\(p^2\\) entries it’s something less than that because the top half = bottom half. Also it must be positive definite, which means \\(\\boldsymbol x^T \\boldsymbol x &gt; 0\\). We need a probability distribution for covariance matrices, defined on the set of all \\(p \\times p\\) symmetric positive definite matrices. This is a very complicated space, you can’t picture it in your head. We can construct such distributions from more basic things. Let’s say \\(\\boldsymbol z_1, \\boldsymbol z_2, …, \\boldsymbol z_n\\) are all \\(p\\)-vectors, then \\(\\boldsymbol {z_i z_i}^T\\) ( \\((p \\times 1)(1 \\times p)\\) that’s a \\(p \\times p\\) matrix ). \\[ \\sum_{i=1}^n \\boldsymbol z_i\\boldsymbol z_i^T = \\mathbf{Z}^T\\mathbf{Z} \\] is the sum of \\(n~~ p \\times p\\) matrices where the \\(i\\)th row of \\(\\mathbf{Z}_{n\\times p}\\) is \\(\\boldsymbol z_i^T\\). A way to construct a “random” covariance matrix is; sample \\(\\boldsymbol z_1,...,\\boldsymbol z_{\\nu_0} \\stackrel{\\text{iid}}\\sim\\) Normal\\(_p(\\boldsymbol 0, \\boldsymbol\\Phi_0)\\) calculate \\(\\mathbf{Z}^T\\mathbf{Z} =\\sum_{i=1}^{\\nu_0} \\boldsymbol z_i\\boldsymbol z_i^T\\). As long as \\(\\nu_0 &gt; p\\), the result will be a \\(p\\times p\\) covariance matrix called the Wishart\\((\\nu_0, \\Phi_0)\\) distribution \\(\\nu_0\\) is called the degrees of freedom and \\(\\Phi_0\\) is the scale matrix. The expected value, \\(E(\\mathbf{Z}^T\\mathbf{Z}),\\) is \\(\\nu_0 \\boldsymbol\\Phi_0\\) It will be symmetric and positive definite The Wishart distribution generalizes the gamma distribution (equivalently the chi-square distribution) to higher dimensions. \\(\\nu_0 &gt; p\\) guarantees that the \\(\\boldsymbol z_i\\) will be linearly independent. In this construction \\(\\nu_0\\) must be a positive integer. The bigger \\(\\nu_0,\\) is the more \\(\\boldsymbol z_i\\)’s are being added together the more there will be an “averaging out” of variation between the \\(\\boldsymbol z_i\\)’s. So if I have (sum of \\(n\\) things) the bigger \\(n\\) is the more variable this sum is. However the bigger \\(n\\) is the less variable (sum of \\(n\\) things)/\\(n\\) will be. It turns out the Wishart distribution is conjugate for the precision matrix in a multivariate normal model which means the inverse-Wishart distribution is conjugate for a covariance matrix in a multivariate normal model. We can define the inverse-Wishart distribution as follows; Let \\(\\boldsymbol z_i \\stackrel{\\text{iid}}\\sim\\) Normal\\(_p(\\boldsymbol 0, \\mathbf S_0^{-1})\\) then take the “sum of squares” of the \\(\\boldsymbol z_i\\) and invert it, so we have \\(\\boldsymbol \\Sigma = (\\mathbf{Z}^T\\mathbf{Z})^{-1}.\\) Under this simulation scheme, the precision matrix \\(\\mathbf \\Sigma^{-1}\\) has a Wishart\\((\\nu_0, \\boldsymbol S_0^{-1})\\) distribution, and the covariance matrix \\(\\boldsymbol \\Sigma\\) has an inverse-Wishart\\((\\nu_0, \\mathbf S_0^{-1})\\) where \\(\\nu_0 =\\) degrees of freedom (df) and \\(\\mathbf S_0^{-1} =\\) scale matrix. \\[ \\mathrm{E}(\\boldsymbol{\\Sigma}^{-1})=\\nu_{0} \\mathbf{S}_{0}^{-1} \\quad \\text { and } \\quad \\mathrm{E}(\\boldsymbol{\\Sigma})=\\frac{1}{\\nu_{0}-p-1} \\mathbf{S}_{0} \\] The expectation for a Wishart-distributed random matrix is df \\(\\times\\) scale matrix. The expectation for an inverse-Wishart random matrix is inverse of scale matrix divided by (df - \\(p\\) - 1) . This should help us figure out how to set a prior distribution for the covariance matrix. A sensible conjugate prior requires two things: (1) a prior best guess for the parameter value and (2) a fair assessment of your degree of confidence in that prior best guess which determines \\(\\nu_0.\\) You can’t have have \\(\\nu_0 &lt; p\\), only \\(\\nu_0 &gt; p\\). And to have a prior expectation, you need \\(\\nu_0 &gt; p+1.\\) So with very low confidence in your prior belief set \\(\\nu_0 = p+2,\\) and \\(\\mathbf S_0 = \\boldsymbol\\Sigma_0.\\) Howeber, if we are confident that the true covariance matrix is near some covariance matrix \\(\\boldsymbol\\Sigma_0\\), then set the scale matrix \\(\\mathbf S_0^{-1}\\) to be the inverse of \\(\\mathbf S_0\\) where \\(\\mathbf S_0 = (\\nu_0 -p -1) \\mathbf\\Sigma_0\\). 12.5 Full conditional distribution of the covariance matrix If \\(p(\\boldsymbol\\Sigma)\\) is an inverse-Wishart density and \\(p(\\boldsymbol y | \\boldsymbol \\theta, \\boldsymbol \\Sigma)\\) is a Normal\\(_p(\\boldsymbol \\theta , \\boldsymbol \\Sigma)\\) likelihood then \\[ p(\\boldsymbol \\Sigma | \\boldsymbol y, \\boldsymbol \\theta) \\sim \\text{inverse-Wishart}(\\nu_0+n, [\\mathbf S_0+\\mathbf S_{\\theta}]^{-1}) \\] \\(\\mathbf S_{\\theta} = \\sum_{i=1}^n (\\boldsymbol y_i - \\boldsymbol\\theta)(\\boldsymbol y_i-\\boldsymbol\\theta)^T\\\\\\) The posterior expectation of \\(\\boldsymbol\\Sigma\\) is a weighted average of the prior expectation and the sample covariance matrix (\\(1/n \\times \\mathbf S_{\\theta}\\)) conditional on mean \\(\\boldsymbol \\theta\\) being known. \\[ \\begin{array}{l} \\mathrm{E}\\left(\\boldsymbol{\\Sigma} \\mid \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{n}, \\boldsymbol{\\theta}\\right)=\\frac{1}{\\nu_{0}+n-p-1}\\left(\\mathbf{S}_{0}+\\mathbf{S}_{\\theta}\\right) \\\\ \\quad=\\frac{\\nu_{0}-p-1}{\\nu_{0}+n-p-1} \\cdot \\frac{1}{\\nu_{0}-p-1} \\mathbf{S}_{0}+\\frac{n}{\\nu_{0}+n-p-1} \\cdot \\frac{1}{n} \\mathbf{S}_{\\theta} \\end{array} \\] Since we have full conditional distributions for \\(\\boldsymbol \\theta\\) and \\(\\boldsymbol \\Sigma\\), we can do a Gibbs sampler! We just come up with a reasonable starting value and alternate between the two full conditionals. Starting values? easy enough. Start \\(\\boldsymbol \\theta\\) at \\(\\bar{\\boldsymbol y}\\) (sample mean vector) and start \\(\\boldsymbol \\Sigma\\) at sample covariance matrix. Choosing priors We want \\(\\nu_0\\) to be small relative to \\(n\\) but we need it to be at least \\(&gt; p+1\\) so set it to \\(p+2 = 4\\) and it will not get a lot of weight relative to \\(n=22\\). The relative weights in this posterior expectation are \\(n=22\\) for the sample data and \\(\\nu_0 - p - 1 = 4 - 2 - 1 = 1\\) for the prior. So the posterior expectation ( conditional on \\(\\boldsymbol \\theta\\) ) will be a \\(22/23\\) versus \\(1/23\\) weighted toward the data. # Reading comprehension example from Chapter 7 of Hoff (2009) # y1 is pretest score, y2 is posttest score y1 &lt;- c(59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 70, 34, 50, 41, 52, 60, 34, 28, 35) y2 &lt;- c(77, 39, 46, 26, 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 60, 75, 47, 48, 33) y &lt;- data.frame(cbind(y1, y2)); rm(y1, y2); # Hyperparamaters mu.0 &lt;- c(50, 50) Lambda0 &lt;- matrix(c(625, 312.5, 312.5, 625), 2, 2) S.0 &lt;- Lambda0; nu.0 &lt;- 4; # Data summaries n &lt;- dim(y)[1] p &lt;- dim(y)[2] ybar &lt;- apply(y, 2, mean) Cov.y &lt;- cov(y) ybar ## y1 y2 ## 47.18 53.86 Cov.y ## y1 y2 ## y1 182.2 148.4 ## y2 148.4 243.6 # Gibbs sampler approximation to posterior distribution S &lt;- 5000 theta &lt;- ybar # initial values Sigma &lt;- Cov.y # initial values # Calculations that will be used repeatedly Lambda0.inv &lt;- solve(Lambda0) Lam.inv.mu.0 &lt;- Lambda0.inv %*% mu.0 nu.n &lt;- nu.0 + n # Now generate the Markov chains: theta and Sigma theta.chain &lt;- matrix(NA, S, p) Sigma.chain &lt;- matrix(NA, S, p^2) for(s in 1:S) { n.Sigma.inv &lt;- n * solve(Sigma) Lambda.n &lt;- solve( Lambda0.inv + n.Sigma.inv) mu.n &lt;- Lambda.n %*% (Lam.inv.mu.0 + n.Sigma.inv %*% ybar) theta &lt;- rmvnorm(1, mu.n, Lambda.n)[1,] S.n &lt;- S.0 + (n-1)*Cov.y + n * (ybar-theta) %*% t(ybar-theta) Sigma &lt;- solve( rWishart(1, nu.n, solve(S.n))[,,1] ) theta.chain[s,] &lt;- theta Sigma.chain[s,] &lt;- Sigma } # posterior covariance matrix matrix(apply(Sigma.chain, 2, mean),2) ## [,1] [,2] ## [1,] 202.6 156.4 ## [2,] 156.4 260.9 Posterior summaries # 95% interval for theta2 - theta1 quantile(theta.chain[,2] - theta.chain[,1], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 1.484 6.600 11.795 We estimate that \\(\\theta_2 - \\theta_1\\) is between 1.5 and 11.8 # Pr(theta2 &gt; theta1 | y) mean(theta.chain[,2] &gt; theta.chain[,1]) ## [1] 0.9936 \\(Pr(\\theta_2 &gt; \\theta_1 | \\boldsymbol y) &gt; 0.99\\). So very strong evidence that the instruction program is effective. We can also draw a scatterplot of the simulated pairs, \\((\\theta_1^{(s)}, \\theta_2^{(s)}),\\) to approximate the marginal posterior \\(p(\\theta_1, \\theta_2 | \\boldsymbol y)\\) plot(theta.chain, cex=.5, xlab=expression(theta[1]), ylab=expression(theta[2])) abline(0,1, lwd=2, col=&quot;pink&quot;) Figure 12.2: Marginal posterior distribution of (theta1, theta2) 99.4% of the line is above the 45 degree line which is why we have the high believe that the treatment program is effective on average. Now let’s ask a slightly different question. What is the probability that a randomly selected child will score higher on the second exam than on the first? To answer this, we sample from the posterior predictive distribution \\(p(\\tilde y|\\boldsymbol y)\\). # Posterior predictive simulations Y.tilde &lt;- matrix(NA, S, p) for (s in 1:S){ Y.tilde[s,] &lt;- rmvnorm(1, mean=theta.chain[s,], sigma=matrix(Sigma.chain[s,],2,2))[1,] } mean(Y.tilde[,2] &gt; Y.tilde[,1]) ## [1] 0.7178 plot(Y.tilde, cex=.5, xlab=expression(tilde(y[1])), ylab=expression(tilde(y[2]))) abline(0, 1, lwd=2, col=&quot;pink&quot;) Figure 12.3: Posterior predictive distribution There is A LOT more variability in the posterior predictive distribution than in the posterior distribution of \\(\\boldsymbol \\theta\\). The posterior distribution of \\((\\theta_1, \\theta_2)\\) sits 99% above the 45-degree line, the posterior predictive distribution \\((\\tilde y_1, \\tilde y_2)\\) sits 70% above the 45-degree line. "],["group-comparisons.html", "Lecture 13 Group comparisons 13.1 Comparing two groups 13.2 Example: Math scores data 13.3 A Bayesian model", " Lecture 13 Group comparisons The following notes, mostly transcribed from Neath(0524,2021) lecture, summarize section 8.1 of Hoff(2009). What’s going on with our masterplan? We have jumped from 07a to 08a there was to be a 07b(Missing data and imputation) but we’re skipping that at least for now. We were two days behind schedule, by cancelling the 07b lesson we’re now one day behind. 13.1 Comparing two groups We have a concrete problem to motivate us 13.2 Example: Math scores data y1 &lt;- c(52.11, 57.65, 66.44, 44.68, 40.57, 35.04, 50.71, 66.17, 39.43, 46.17, 58.76, 47.97, 39.18, 64.63, 69.38, 32.38, 29.98, 59.32, 43.04, 57.83, 46.07, 47.74, 48.66, 40.80, 66.32, 53.70, 52.42, 71.38, 59.66, 47.52, 39.51) y2 &lt;- c(52.87, 50.03, 41.51, 37.42, 64.42, 45.44, 46.06, 46.37, 46.66, 29.01, 35.69, 49.16, 55.90, 45.84, 35.44, 43.21, 48.36, 74.14, 46.76, 36.97, 43.84, 43.24, 56.90, 47.64, 38.84, 42.96, 41.58, 45.96) boxplot(list(y1, y2), ylab=&quot;score&quot;, names=c(&quot;school 1&quot;, &quot;school 2&quot;), col=0) Figure 13.1: Boxplots of samples of 10th grade math scores from two schools The appearance is: mean score at school 1 is higher than that at school 2. But these 31 and 28 students are not the entire the entire school, they are just a sample from a larger population of 10th grade students. So might this difference in mean score be attributable to sampling error? i.e, might a different set of 31 and 28 students give different means? How do we test this? Let \\(\\theta_1 =\\) mean score at school 1 (that’s the population mean!) \\(\\theta_2 =\\) mean score at school 2 (population mean!) We want to test: \\(H_0: \\theta_1 = \\theta_2\\) against \\(H_a: \\theta_1 \\neq \\theta_2\\) If we assume that both populations are normal with the same variance just possibly different mean then we can conduct this test by two-sample t-test . The \\(t\\)-statistic: \\[ t(\\boldsymbol y_1, \\boldsymbol y_2) = \\frac{\\bar y_1 - \\bar y_2}{s_p\\sqrt{1/n_1 + 1/n_2}}, \\text{ where } s_p^2 = \\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1 + n_2 -2} \\] This is the classic test statistic for classical hypothesis testing (not bayesian). Our best estimate of \\(\\sigma\\) (assuming equal variance) is the pooled sample standard deviation. A test statistic measures the difference between the data you got and the data you’d have expected if \\(H_0\\)(null hypothesis) were true. The data we got had a difference in means of about 4.66 points. What we’d expect if \\(H_0\\) were true is no difference in sample means. n1 &lt;- length(y1); y1bar &lt;- mean(y1); s1 &lt;- sd(y1); n2 &lt;- length(y2); y2bar &lt;- mean(y2); s2 &lt;- sd(y2); y1bar-y2bar ## [1] 4.663 # pooled sample standard deviation sp &lt;- sqrt( ( (n1-1)*s1^2 + (n2-1)*s2^2 ) / (n1+n2-2)) # test-statistic (t.stat &lt;- (y1bar- y2bar) / (sp * sqrt(1/n1 + 1/n2)) ) ## [1] 1.742 We get t-stat \\(= 1.74\\). The difference in sample means that we observed is \\(1.74\\) standard errors greater than the difference we’d expect to observe if \\(H_0\\) were true. If \\(H_0\\) is true the \\(t\\)-statistic has a null sampling distribution that is the t-distribution with degrees of freedom \\(= n_1 + n_2 - 2=57\\). We expect the t-statistic to not be far out in the t-distribution. tvals &lt;- seq(-4, 4, 0.01) plot(tvals, dt(tvals, df=n1+n2-2), type=&quot;l&quot;, lwd=2, ylab=&quot;&quot;,xlab=&quot;&quot;) segments(1.74,0,1.74, dt(1.74, df=n1+n2-2), col=&quot;red&quot;, lwd=2, lty=3) segments(-1.74,0,-1.74, dt(1.74,df=n1+n2-2), col=&quot;red&quot;, lwd=2, lty=3) abline(h=0) Figure 13.2: The null distribution for testing equality of the population means. The red dashed line indicates the observed value of the t-statistic. P-value is the measure of the strength against the null hypothesis. The smaller the p-value the more unlikely would be the data you got if the null hypothesis were true. # two-sided p-value 2 * (1 - pt(abs(t.stat), df=n1+n2-2)) ## [1] 0.08693 If the two populations indeed follow the same normal population, then the pre-experimental probability of sampling a dataset that would generate a value of \\(t(\\mathbf Y_1, \\mathbf Y_2)\\) greater in absolute value than 1.74 is 8.7% For a two-sided alternative hypothesis the p-value counts both tail probabilities. P-value \\(= 0.087\\) is generally considered (by convention) to be not very strong evidence against \\(H_0.\\) The data we observed would have been not entirely unexpected with \\(H_0\\) being true so it’s only weak evidence that \\(H_0\\) is false. # Or just go t.test(y1, y2, var.equal=T) ## ## Two Sample t-test ## ## data: y1 and y2 ## t = 1.7, df = 57, p-value = 0.09 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.6977 10.0234 ## sample estimates: ## mean of x mean of y ## 50.81 46.15 If our job is to estimate \\(\\theta_1\\) and \\(\\theta_2,\\) the mean scores at the two schools, we have a decision to make. Should we combine the information at both schools and report pooled estimates for both? \\(\\hat \\theta_1 = \\hat \\theta_2 = (n_1 \\bar y_1+ n_2\\bar y_2) / (n_1 + n_2)\\) Or report separate estimates? \\(\\hat \\theta_1 = \\bar y_1\\) \\(\\hat \\theta_2 = \\bar y_2\\) Model selection (deciding between single mean or separate-means model) based on p-values: One proposed recipe for making this decision is to do it based on the result of the significance test. Conduct a test of \\(H_0: \\theta_1 = \\theta_2\\). Reject if p-value \\(&lt; \\alpha\\), take \\(\\alpha = 0.05\\) If you reject this \\(H_0\\) then you’ll want to report separate estimates. If you fail to reject interpret this to mean we “accept” the null and report the the pooled estimate. This is not entirely satisfactory. In this example (following this recipe) our conclusion would be; Do not reject the null. So take the average score of all \\(31 + 28 = 59\\) students as the estimated average score for both schools. On the other hand the data suggest \\(\\theta_1 &gt; \\theta_2.\\) It wasn’t conclusive statistical proof but it’s what the data indicated. So maybe it would be better to use separate estimates. On the other hand, suppose we get a p-value of 0.051 or 0.049? In general, if you are following a methodology that will do one thing if you get a p-value = 0.049 and something substantially different if you get a p-value of 0.051 you need to rethink your life choices. Another way to think about this: We are saying that we will estimate \\(\\theta_1\\) (mean score in school 1) by a weighted average of the two school averages \\(w\\bar y_1 + (1-w) \\bar y_2\\). It makes sense that \\(w\\) is closer to 1 than 0 but maybe the observed scores of students at school 2 contain valuable info about the average score at school 1. That’s the big insight of hierarchical statistical models, the idea that that data from different (but similar) sources can contribute to our estimation at the current source. This is obviously a very Bayesian idea. What that naïve testing-based recipe says is: either take \\(w = 1\\) or take \\(w \\approx 1/2\\). What if there’s a better answer somewhere in between? Wouldn’t it be cool to have a method that would return the estimate \\(\\hat \\theta_1 = w\\bar y_1 + (1-w) \\bar y_2\\) and would tell us what \\(w\\) should be based on say sample size and variance?! The method that does this is the Bayesian hierarchical model. 13.3 A Bayesian model Let’s make a hierarchical Bayesian model for estimating two related means. Consider the sampling model: \\[ \\begin{aligned} Y_{i, 1} &amp;=\\mu+\\delta+\\epsilon_{i, 1} \\\\ Y_{i, 2} &amp;=\\mu-\\delta+\\epsilon_{i, 2} \\\\ \\epsilon_{i, j} &amp; \\sim \\text { iid } \\operatorname{Normal}\\left(0, \\sigma^{2}\\right) \\end{aligned} \\] This model says: The \\(i\\)th score at school \\(j\\) is \\(\\mu + \\delta\\) for \\(j=1\\) or \\(\\mu -\\delta\\) for \\(j=2\\) plus \\(\\epsilon_{i,j}\\) (student random effect). \\(\\mu\\) is the overall mean at both schools \\(i\\) is the unit, \\(j\\) is the group(school effect) \\(\\mu\\) and \\(\\delta\\) are model parameters. The third is \\(\\sigma^2,\\) the variance of the random error term. A reparameterization of the two-means model is; \\(\\theta_1 = \\mu + \\delta,\\) \\(\\theta_2 = \\mu - \\delta\\), \\(\\mu = (\\theta_1+\\theta_2) / 2\\) \\(\\delta = (\\theta_1 - \\theta_2) / 2\\). Hence \\(2\\times\\delta\\) is the difference in means \\(\\sigma^2 =\\) variance among scores by students at the same school (within group variance) The conjugate prior for this model: \\(\\mu \\sim \\text{Normal}(\\mu_0, \\gamma_0^2)\\) \\(\\delta \\sim \\text{Normal}(\\delta_0, \\tau_0^2)\\) \\(\\sigma^2 \\sim\\) Inverse-gamma\\((\\nu_0/2,~\\nu_0\\sigma_0^2/2)\\) The posterior full conditionals: \\(\\{\\mu|\\boldsymbol y_1, \\boldsymbol y_2,\\delta,\\sigma^2\\} \\sim \\text{Normal}(\\mu_n,\\gamma_n^2)\\) where \\[ \\begin{array}{l} \\gamma_{n}^{2}=\\left[\\frac{1}{\\gamma_{0}^{2}}+\\frac{n_{1}+n_{2}}{\\sigma^{2}}\\right]^{-1}\\\\[0.1cm] \\mu_{n}=\\gamma_{n}^{2} \\times\\left[\\frac{\\mu_{0}}{\\gamma_{0}^{2}}+\\frac{\\sum_{i=1}^{n_{1}}\\left(y_{i, 1}-\\delta\\right)+\\sum_{i=1}^{n_{2}}\\left(y_{i, 2}+\\delta\\right)}{\\sigma^{2}}\\right] \\end{array} \\] What is going on above? Observe \\(E(\\bar y_1 | \\mu, \\delta) = \\mu + \\delta\\) \\(E(\\bar y_2 | \\mu, \\delta) = \\mu - \\delta\\) \\(E(\\mu | y_1, \\delta) = \\bar y_1 - \\delta\\) \\(E(\\mu | y_2, \\delta) = \\bar y_2 + \\delta\\) \\(1/\\gamma_n^2 = 1/\\gamma_0^2 + n_1/\\sigma^2 + n_2/\\sigma^2\\) Posterior expectation \\(\\mu_n\\) is a weighted average of \\(\\mu_0\\) (weight is \\(1 / \\gamma_0^2\\) ), \\(\\bar y_1\\) \\(- \\delta\\) (weight \\(n_1/\\sigma^2\\)) and \\(\\bar y_2 + \\delta ~ (\\)weight of \\(n_2/\\sigma^2)\\) Next we have; \\(\\{\\delta|\\boldsymbol y_1, \\boldsymbol y_2,\\mu,\\sigma^2\\} \\sim \\text{Normal}(\\delta_n,\\tau_n^2)\\) where \\[ \\begin{array}{l} \\tau_{n}^{2}=\\left[\\frac{1}{\\tau_{0}^{2}}+\\frac{n_{1}+n_{2}}{\\sigma^{2}}\\right]^{-1} \\\\[0.1cm] \\delta_{n}=\\tau_{n}^{2}\\times\\left[\\frac{\\delta_{0}}{\\tau_{0}^{2}}+\\frac{\\sum_{i=1}^{n_{1}}\\left(y_{i, 1}-\\mu\\right)-\\sum_{i=1}^{n_{2}}\\left(y_{i, 2}-\\mu\\right)}{\\sigma^{2}}\\right] \\end{array} \\] \\(E(\\bar y_1 | \\mu, \\delta) = \\mu + \\delta\\) \\(E(\\bar y_2 | \\mu, \\delta) = \\mu - \\delta\\), so \\(E(\\delta | \\mu, \\bar y_1) = \\bar y_1 - \\mu\\) \\(E(\\delta | \\mu, \\bar y_2) = -(\\bar y_2 - \\mu)\\) \\(1/\\tau_n^2 = 1 /\\tau_0^2+n_1 / \\sigma^2+n_2 / \\sigma^2\\) The posterior expectation \\(\\delta_n\\) is a weighted average of \\(\\delta_0\\) (weight \\(1 / \\tau_0^2\\) ), \\(\\bar y_1 - \\mu\\) ( weight of \\(n_1/\\sigma^2\\) ) and \\(\\bar y_2 - \\mu\\) (weight of \\(n_2 / \\sigma^2\\) ) Finally, we have; \\(\\{\\sigma^2|\\boldsymbol y_1, \\boldsymbol y_2,\\mu,\\delta\\} \\sim \\text{Inverse-gamma}(\\nu_n/2,\\nu_n\\sigma_n^2/2)\\) where \\[ \\begin{array}{l} \\nu_{n}=\\nu_{0}+n_{1}+n_{2}\\\\[0.05cm] \\sigma_{n}^{2}=\\frac{\\sum_{i=1}^{n_{1}}\\left(y_{i, 1}-[\\mu+\\delta]\\right)^{2}+\\sum_{i=1}^{n_{2}}\\left(y_{i, 2}-[\\mu-\\delta]\\right)^{2}}{n_1+n_2} \\end{array} \\] Student question: what is \\(\\tau_0?\\) We’ve got more variances than we are used to so we’re using more greek letters than ever before. Let’s get em straight. A greek letter with no subscript is a random variable and a greek letter with a subscript is a “constant” in this notation system) \\(\\sigma^2\\) represents the variance of the observed responses as usual. Its prior is Inverse-gamma\\(( \\nu_0/2 ,~ \\nu_0\\sigma_0^2/ 2)\\). \\(\\gamma_0^2\\) and \\(\\tau_0^2\\) are the variances for the priors of \\(\\mu\\) and \\(\\delta\\) respectively. There are two unknown means \\(\\theta_1\\) and \\(\\theta_2\\). And we’ve reparameterized \\(\\theta_1 = \\mu + \\delta\\) and \\(\\theta_2 = \\mu - \\delta\\). Our uncertainty about \\(\\mu\\) is measured by \\(\\gamma_0^2\\). Our uncertainty about \\(\\delta\\) is measured by \\(\\tau_0^2\\) 13.3.1 Analysis of the math scores data We need: \\(\\mu_0\\) (best guess at combined average) \\(\\delta_0\\) (best guess at difference between the schools) \\(\\gamma_0^2\\) and \\(\\tau_0^2\\) (the corresponding variances for those two things) \\(\\sigma_0^2\\) our best guess at the within-school / between-students variance in scores \\(\\nu_0\\) Let \\(\\mu_0 = 50\\) and \\(\\sigma_0 = 10\\). That’s the designed average score and SD for this test. Let \\(\\delta_0= 0\\) because we no prior info that one school is expected to have higher average than other. Scores range from 0 to 100 so let’s make \\(\\gamma_0 = 25\\) (mean \\(\\pm 2\\)SD = \\(50\\pm2\\gamma_0\\le100,\\) covers range of possible values). Let \\(\\tau_0^2=\\gamma_0^2= 625\\). To make that prior “diffuse” for \\(\\sigma^2?\\) take \\(\\nu_0 = 1\\) How are we gonna do posterior simulation? We know the full conditionals: \\(p(\\mu | \\boldsymbol y_1,\\boldsymbol y_2, \\delta, \\sigma^2)\\) \\(p(\\delta | \\boldsymbol y_1,\\boldsymbol y_2, \\mu, \\sigma^2)\\) \\(p(\\sigma^2 | \\boldsymbol y_1,\\boldsymbol y_2, \\mu, \\delta)\\) This looks like a job for the Gibbs sampler If we want to sample \\((X, Y, Z)\\) we can do this if we know \\(p(x)\\) and \\(p(y|x)\\) and \\(p(z | x, y)\\). But, if all we know is \\(p(x | y,z)\\), \\(p(y | x,z)\\), and \\(p(z| x, y)\\), we can’t do direct simulation. That’s where the Gibbs sampler comes in, but it’s not as good as method 1 because the simulated draws may be correlated. With a Gibbs sampler: we need starting values! \\(\\phi^{(s)}\\) is generated from \\(\\phi^{(s-1)}\\), so we need a \\(\\phi^{(0)}\\) to be able to get a \\(\\phi^{(1)}\\). But remember: You only need starting values for \\(p-1\\) of the \\(p\\) parameters. We’re gonna start by sampling \\(\\sigma^{2(1)}\\sim p(\\sigma^2 | y_1, y_2, \\mu^{(0)}, \\delta^{(0)})\\). Sensible starting values are obvious! Let \\(\\mu =\\) overall average, \\(\\delta =\\) half the difference in means # Hyperparmeters mu.0 &lt;- 50 ; gamma2.0 &lt;- 625; delta.0 &lt;- 0 ; tau2.0 &lt;- 625; sigma2.0 &lt;- 100; nu.0 &lt;- 1 ; # Starting values mu &lt;- (y1bar + y2bar) / 2 delta &lt;- (y1bar - y2bar) / 2 # Now let&#39;s generate the Markov chain! S &lt;- 5000 mu.chain &lt;- rep(NA, S); delta.chain &lt;- rep(NA, S); sigma2.chain &lt;-rep(NA, S) ; for(s in 1:S) { # First update sigma2, then mu then delta sigma2 &lt;- 1/rgamma(1, (nu.0 + n1 + n2) / 2, (nu.0*sigma2.0 + sum( (y1 - mu - delta)^2 ) + sum( (2 - mu + delta)^2 ) ) / 2 ) gamma2.n &lt;- 1 / (1/gamma2.0 + (n1+n2)/sigma2) mu.n &lt;- gamma2.n * (mu.0/gamma2.0 + ( sum(y1-delta) + sum(y2+delta) ) / sigma2 ) mu &lt;- rnorm(1, mean=mu.n, sd=sqrt(gamma2.n)) tau2.n &lt;- 1 / (1/tau2.0 + (n1+n2)/sigma2) delta.n &lt;- tau2.n * (delta.0/tau2.0 + ( sum(y1-mu) - sum(y2-mu)) / sigma2 ) delta &lt;- rnorm(1, mean=delta.n, sd=sqrt(tau2.n)) mu.chain[s] &lt;- mu delta.chain[s] &lt;- delta sigma2.chain[s] &lt;- sigma2 } par(mfrow=c(1,2)) plot(mu.chain+delta.chain, mu.chain-delta.chain, xlab=xlab, ylab=ylab,pch=19,cex=0.3) plot(mu.chain, delta.chain,pch=19,cex=0.3,xlab=xlab2,ylab=ylab2) Figure 13.3: Joint distribution for (theta1,theta2) and (mu,delta). This is the scatterplot of the joint posterior distribution of \\((\\theta_1, \\theta_2).\\) They seem to be uncorrelated. \\((\\mu, \\delta)\\) are likewise uncorrelated in their posterior distribution. This makes sense since we have an orthogonal transformation of the parameters. Student question: What is orthogonal transformation of the parameters? Ans: If \\(\\theta_1\\) and \\(\\theta_2\\) are independent then \\((\\theta_1 + \\theta_2)/2\\) and \\((\\theta_1 - \\theta_2) / 2\\) are also independent because (+1 +1) and (+1 -1) are orthogonal. \\(\\begin{pmatrix} 0.5,0.5\\\\0.5,-.5 \\end{pmatrix}^T=\\begin{pmatrix} 0.5,0.5\\\\0.5,-.5 \\end{pmatrix}^{-1}\\) (I think). par(mfrow=c(1,2)) plot(density(mu.chain), xlim=c(25, 75), lwd=2, xlab=expression(mu), ylab=&quot;density&quot;, main=&quot;&quot;) mu.vals &lt;- seq(25, 75, .10) lines(mu.vals,dnorm(mu.vals, mu.0, sqrt(gamma2.0)), lwd=2, col=&quot;pink&quot;) legend(&quot;topright&quot;, inset=.05, lwd=2, col=c(&quot;pink&quot;, &quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), cex=0.6) plot(density(delta.chain), xlim=c(-25, 25), lwd=2, xlab=expression(delta), ylab=&quot;density&quot;, main=&quot;&quot;) delta.vals &lt;- seq(-25, 25, .10) lines(delta.vals, dnorm(delta.vals, delta.0, sqrt(tau2.0)), lwd=2, col=&quot;pink&quot;) legend(&quot;topleft&quot;, inset=.05, lwd=2, col=c(&quot;pink&quot;, &quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), cex=0.6) Figure 13.4: Marginal posteriors of mu and delta. These plots show posteriors and priors for \\(\\mu\\) and \\(\\delta\\). Our prior on \\(\\mu\\) was Normal(mean=50, sd=25). The posterior mean is shifted left a bit (because the combined average is a little less than 50). Our prior on \\(\\delta\\) is is Normal(0, sd=25). The posterior is shifted right a bit because we have some belief now that \\(\\theta_1 &gt; \\theta_2\\) Notice how these are very close. (y1bar+y2bar)/2; mean(mu.chain) ## [1] 48.48 ## [1] 48.47 mean(delta.chain)*2; y1bar-y2bar ## [1] 4.627 ## [1] 4.663 # Posterior belief about other quantities of interest # 2*delta = theta1 - theta2 quantile(2*delta.chain, c(.025, .975)) ## 2.5% 97.5% ## -0.4788 9.7720 We are 95% confident that the mean score at school 1 is between a half-point less and 9.75 points greater than the mean score at school 2 so this is consistent with our t-test inference were we got \\([-0.698, 10.023]\\). The data were not conclusive (no strong evidence that \\(\\theta_1 &gt; \\theta_2\\) ) but the data suggested that. The Bayesian interval is a bit shorter than the t-test interval because we used some prior information. # Posterior belief that theta1 &gt; theta2 mean(delta.chain &gt; 0) ## [1] 0.9616 \\(\\delta = (\\theta_1 - \\theta_2) / 2\\), \\(\\delta &gt; 0\\) means average score is higher at school 1. Our posterior belief that the average score is higher at school 1 is about 96%. What’s the predictive probability a school 1 student outscores a school 2 student? It should be \\(&gt; 0.5\\) but maybe not by much. Let’s see… # Posterior predictive simulation y1.tilde &lt;- rnorm(S, mean=mu.chain+delta.chain, sd=sqrt(sigma2.chain)) y2.tilde &lt;- rnorm(S, mean=mu.chain-delta.chain, sd=sqrt(sigma2.chain)) mean(y1.tilde &gt; y2.tilde) ## [1] 0.6246 About a 62% probability that the school 1 student scores higher. Looking back (right before 13.3) we said the ‘best estimate’ of \\(\\theta_1\\) should be somewhere between just \\(\\bar y_1\\) and a straight average of \\(\\bar y_1 \\&amp; \\bar y_2\\), but we didn’t actually solve that did we? No. But is there an ‘implicit’ \\(w\\)? there must be, right? Let’s see! \\(\\bar y_1 = 50.8\\) that would be the “no pooling” estimate. y1bar ## [1] 50.81 The average of the two schools \\((\\bar y_1 + \\bar y_2) / 2 = 48.5\\). That would be the “fully pooled”estimate (in Gelman’s terminology). (y1bar+y2bar)/2 ## [1] 48.48 The “hierarchical model-based estimated” (which is what we did) SHOULD be between these two; \\(\\hat \\theta_1 = \\hat\\mu+\\hat\\delta=50.78.\\) So it’s a lot closer to no pooling. It’s lowered just a bit by school 2 which makes sense. mean(mu.chain+delta.chain) ## [1] 50.78 So even though the classical model testing approach would lead to pooling the optimal answer is a lot closer to no pooling. The author (Hoff) lost me a bit on some pages from section 8.3. If you find yourself similarly lost don’t feel bad about that. Tomorrow: We will consider \\(m\\) different groups not just two. Our model will say; \\(\\{y_{i,j} | \\theta, \\sigma^2\\} \\sim\\) Normal\\((\\theta_j , \\sigma^2)\\) where \\(y_{i,j}\\) is the \\(i\\)th response from the \\(j\\)th group. \\(i\\) goes from 1 to \\(n_j\\), \\(j\\) goes up to \\(m\\). \\(\\theta_j\\) is the true mean for the \\(j\\)th group. \\(\\{\\theta_1, …., \\theta_m | \\mu, \\tau\\} \\sim\\) iid Normal\\(( \\mu, \\tau^2 )\\). Think about the data coming about this way: There are a whole bunch of schools out there. Our data include a sample of \\(m\\) of them. At each of these schools there are a whole bunch of students. We sample \\(n_j\\) students at school \\(j,\\) \\(\\theta_j\\) represents mean score at \\(j\\)th school. The hierarchical model says: \\(y_{i,j} \\sim\\) Normal\\(( \\theta_j , \\sigma^2 )\\) \\(\\theta_1, …, \\theta_m\\) are iid Normal\\(( \\mu, \\tau^2 )\\). So the \\(\\theta_j\\) are sampled from a population with mean \\(\\mu\\) and variance \\(\\tau^2\\). So the model parameters are: \\(\\sigma^2\\) (within-school variance), \\(\\tau^2\\) (between-school variance ) and \\(\\mu\\) the overall mean. Note that the \\(\\theta_j\\)’s are not model parameters. "],["the-hierarchical-normal-model.html", "Lecture 14 The hierarchical normal model 14.1 Posterior inference 14.2 Example: Math scores in U.S. public schools 14.3 Posterior approximation 14.4 MCMC diagnostics 14.5 Shrinkage 14.6 Ranking the groups", " Lecture 14 The hierarchical normal model The following notes, mostly transcribed from Neath(0525,2021) lecture, summarize sections(8.3 and 8.4) of Hoff(2009). In section 8.3 there’s some theoretical justification to the hierarchical model based on exchangeability arguments. We won’t need that. The hierarchical model is good because it makes sense and it leads to more precise yet justified inference than competitor models. Here’s why it makes sense. From last class, we know that our data consist of scores on math tests for 1993 10th grade students. \\(Y_i =\\) score of \\(i\\)th student \\(i = 1, 2, …, 1993\\), and assume that conditional on \\(\\theta\\) and \\(\\sigma^2\\) the \\(y_i\\) are independent Normal\\((\\theta, \\sigma^2).\\) Assign prior distributions to \\(\\theta\\) and \\(\\sigma^2,\\) update using Bayes rule and make inference based on the posterior distribution. That would be fine. But there’s something I didn’t tell you last class. The data is not actually 1993 randomly selected students but rather there are 100 randomly selected high schools all having a 10th grade enrollment of 400 or greater, and from any particular high school we selected between 5 and 32 students for a total of 1993. For that reason it does not makes sense to treat the 1993 responses as exchangeable. Two students from the same school are reasonably expected to be more similar to each other than would be two students from two different schools. However, within any particular school \\(j,\\) the \\(n_j\\) students in the sample at school \\(j\\) can be modeled as exchangeable. Let \\(\\theta_j =\\) mean score at school \\(j\\) and we’ll say the \\(Y_{i,j} ~(\\)score of \\(i\\)th student at school \\(j)\\) are conditionally on \\(\\theta_j\\) iid Normal\\(( \\theta_j , \\sigma^2_j)\\). What about the \\(\\theta_j?\\) The \\(m=100\\) schools are a random sample from some population of schools (there’s a lot more than 100) so it is justified to assume \\(\\theta_1, \\theta_2, …, \\theta_{100}\\) (mean scores of the 100 schools) are exchangeable. Remember de Finetti’s theorem says: If the data are exchangeable they follow a “conditional iid” model. So our model will say \\(\\{\\theta_1, …, \\theta_m | \\mu, \\tau^2\\} \\stackrel{\\text{iid}}\\sim\\) Normal\\(( \\mu, \\tau^2 )\\). That’s the first level of the hierarchy. The \\(\\theta_j\\) are not observable! what is observable is what’s happening at the next level of the hierarchy. The next level; \\(Y_{i,j} | \\theta_j \\stackrel{\\text{iid}}\\sim\\) Normal\\((\\theta_j, \\sigma^2)\\). We will assume that the variances at each school are the same. Is this assumption justified? Probably not. Does that really matter? Probably not so much. It will not negatively affect our inference about \\(\\theta_j,\\) and interpretation is more straightforward. The model says; Observed data: \\(y_{i,j}\\), \\(i = 1, …, n_j, ~~ j = 1, …., m\\) \\(\\{Y_{i,j} | \\theta, \\sigma^2\\} \\stackrel{\\text{indep}}{\\sim}\\) Normal\\((\\theta_j, \\sigma^2 )\\) \\(\\{\\theta_1, …, \\theta_m | \\mu, \\tau^2 \\}~ \\stackrel{\\text{iid}}{\\sim}\\) Normal\\(( \\mu, \\tau^2 )\\) So we have observable random quantities, unobservable random quantities, and model parameters. What’s what? The observable data are \\(y_{i,j}\\), the test scores of students in the sample. Unobservable random quantities are the \\(\\theta_j\\). The model parameters are: \\(\\sigma^2 = \\text{Var}( Y_{i,j} | \\theta_j, \\sigma^2 )\\) \\(\\mu = E( \\theta_j | \\mu, \\tau^2)\\) \\(\\tau^2 = \\text{Var}(\\theta_j | \\mu, \\tau^2 )\\) Note: The sampling distribution of \\(\\{ Y_{i,j} | \\mu, \\sigma^2 , \\tau^2\\}\\) is normal. By laws of probability we know that if \\(X | Z=z \\sim\\) Normal\\((z, \\sigma^2)\\) ( say \\(y|\\theta\\) ) and \\(Z \\sim\\) Normal\\(( \\mu, \\tau^2)\\) ( say \\(\\theta|\\mu\\) is normal ) then \\(X \\sim\\) Normal ( then \\(y\\) is normal). \\(E(Y_{i,j} | \\mu, \\sigma^2 , \\tau^2) = \\mu\\) Remember that \\(E( E[X|Z] ) = E(X),\\) so \\[ \\begin{aligned} E(Y_{i,j} | \\mu, \\sigma^2 , \\tau^2) &amp;= E(E[Y_{i,j} | \\theta_j,\\sigma^2]|\\mu, \\sigma^2 , \\tau^2)\\\\ &amp;= E(\\theta_j|\\mu, \\sigma^2 , \\tau^2)=\\mu \\end{aligned} \\] \\(\\text{Var}( Y_{i,j}| \\mu, \\sigma^2, \\tau^2 ) = \\sigma^2 + \\tau^2\\) because \\(E(\\text{Var}[X|Z] ) + \\text{Var}(E[X|Z] ) = \\text{Var}(X).\\) What do \\(\\sigma^2\\) and \\(\\tau^2\\) represent in this model? \\(\\sigma^2 = \\text{Var}( Y_{i,j} | \\theta_j, \\sigma^2)\\) measures the variation in scores between students of the same school. \\(\\tau^2 = \\text{Var}(\\theta_j | \\mu, \\tau^2)\\) measures variation between mean scores at different schools. So we write; Total variation in student scores = variation between schools + variation between students of same school (i.e., variation within school). It is an assumption of the model that this is the same for all schools. Obviously this is not the case but it is a simplifying assumption. We are interested in inference about \\(\\mu\\) as well as in estimating (“predicting”) the \\(\\theta_j\\) and also \\(\\sigma^2\\) and \\(\\tau^2\\) (the within-school and between-school variation in scores). Let’s do it! We need to talk about priors! Our model parameters are: overall mean \\(\\mu,\\) within-group variance \\(\\sigma^2,\\) between-group variance \\(\\tau^2\\). \\[ \\begin{aligned} \\sigma^{2} &amp; \\sim \\operatorname{inverse-gamma}\\left(\\nu_{0} / 2, \\nu_{0} \\sigma_{0}^{2} / 2\\right)\\\\ \\tau^{2} &amp; \\sim \\operatorname{inverse-gamma}\\left(\\eta_{0} / 2, \\eta_{0} \\tau_{0}^{2} / 2\\right)\\\\ \\mu &amp; \\sim \\operatorname{Normal}\\left(\\mu_{0}, \\gamma_{0}^{2}\\right) \\end{aligned} \\] A greek letter with a subscript is a constant (or at least being conditioned on) a greek letter with no subscript is a parameter (and hence a random quantity). 14.1 Posterior inference We will do posterior inference by the Gibbs sampler. The joint posterior \\(p(\\theta_1, …, \\theta_m, \\mu, \\tau^2, \\sigma^2 | \\boldsymbol y_1,...,\\boldsymbol y_m )\\) is not readily solvable. But what is solvable is the set of full conditional distributions. \\[ \\begin{equation} \\begin{array}{l} p\\left(\\theta_{1}, \\ldots, \\theta_{m}, \\mu, \\tau^{2}, \\sigma^{2} \\mid \\boldsymbol y_{1},\\ldots, \\boldsymbol{y}_{m}\\right)\\\\ \\propto p\\left(\\mu, \\tau^{2}, \\sigma^{2}\\right) p\\left(\\theta_{1}, \\ldots, \\theta_{m} \\mid \\mu, \\tau^{2}, \\sigma^{2}\\right) p\\left(\\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m} \\mid \\theta_{1}, \\ldots, \\theta_{m}, \\mu, \\tau^{2}, \\sigma^{2}\\right) \\\\ =p(\\mu) p\\left(\\tau^{2}\\right) p\\left(\\sigma^{2}\\right)\\left\\{\\prod_{j=1}^{m} p\\left(\\theta_{j} \\mid \\mu, \\tau^{2}\\right)\\right\\}\\left\\{\\prod_{j=1}^{m} \\prod_{i=1}^{n_{j}} p\\left(y_{i, j} \\mid \\theta_{j}, \\sigma^{2}\\right)\\right\\} \\end{array} \\end{equation} \\] Our priors on \\(\\mu\\) and \\(\\tau^2\\) and \\(\\sigma^2\\) are independent hence the first factorization. The sampling distribution of \\(\\theta_j\\) is (conditionally) iid Normal\\((\\mu,\\tau^2)\\) and independent of \\(\\sigma^2.\\) Conditionally on the \\(\\theta_j\\) the observed data are independent of \\(\\mu\\) and \\(\\tau^2\\). Attempt at Fig. 8.2. (Hoff) A representation of the basic hierarchical normal model. \\[ \\begin{array}{c} \\mu,\\tau^2 \\longrightarrow \\theta_1,...,\\theta_{m-1},\\theta_m \\longrightarrow \\mathbf{Y}_1,...,\\mathbf{Y}_{m-1}, \\mathbf{Y}_m\\\\[0.1cm] \\sigma^2 \\longrightarrow\\mathbf{Y}_1,...,\\mathbf{Y}_{m-1}, \\mathbf{Y}_m\\\\[0.3cm] \\end{array} \\] Notice that \\(\\mu\\) and \\(\\tau^2\\) describe the population of schools the \\(\\theta_j\\) are a sample from that population \\(y_j = y_{1,j} , …., y_{n_j, j}\\) are a sample from the population that \\(\\theta_j\\) describes Those variables depend on \\(\\sigma^2\\) as well. There’s a layer of separation between the data and \\((\\mu, \\tau^2)\\). Conditionally on the \\(\\theta\\)’s the \\(y_{i,j}\\) and \\((\\mu, \\tau^2)\\) are independent. We will make use of this when deriving full conditionals. You might think deriving the full conditionals would involve a lot of messy math and it would, except we don’t need it to because we’ve already done that math! Solving the full conditionals for this model is all about recognizing where what we know from univariate normal model applies to things with different names / notations. 14.1.1 Full conditional distributions of \\(\\mu\\) and \\(\\tau^2\\) \\(p(\\mu, \\tau^2 | \\theta, \\sigma^2, y) = p(\\mu, \\tau^2 | \\theta)\\) Logically: Our goal is to estimate the overall mean score at 5 schools. If we know the overall mean score for 3 of those schools then we have no use for data from those three schools. Further, \\(p(\\mu, \\tau^2 | \\theta_1, …, \\theta_m)\\) is just the univariate normal model! what we called \\(\\theta\\) and \\(\\sigma^2\\) before are now \\(\\mu\\) and \\(\\tau^2\\) and what we called \\(y_i\\) before are now the \\(\\theta_j\\) ( akin to \\(p(\\theta, \\sigma^2 | y_1, …, y_n)\\) ). \\[ \\begin{array}{l} p\\left(\\mu \\mid \\theta_{1}, \\ldots, \\theta_{m}, \\tau^{2}, \\sigma^{2}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m}\\right) \\propto p(\\mu) \\prod p\\left(\\theta_{j} \\mid \\mu, \\tau^{2}\\right) \\\\ p\\left(\\tau^{2} \\mid \\theta_{1}, \\ldots, \\theta_{m}, \\mu, \\sigma^{2}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m}\\right) \\propto p\\left(\\tau^{2}\\right) \\prod p\\left(\\theta_{j} \\mid \\mu, \\tau^{2}\\right) \\end{array} \\] We have \\[ \\mu \\mid \\theta_{1}, \\ldots, \\theta_{m}, \\tau^{2} \\sim \\text { Normal}\\left(\\frac{\\mu_{0} / \\gamma_{0}^{2}+m \\bar{\\theta} / \\tau^{2}}{1 / \\gamma_{0}^{2}+m / \\tau^{2}}, \\frac{1}{1 / \\gamma_{0}^{2}+m / \\tau^{2}}\\right) \\] and \\[ \\tau^{2} \\mid \\theta_{1}, \\ldots, \\theta_{m}, \\mu \\sim \\operatorname{inverse-gamma}\\left(\\frac{\\eta_{0}+m}{2}, \\frac{\\eta_{0} \\tau_{0}^{2}+\\sum\\left(\\theta_{j}-\\mu\\right)^{2}}{2}\\right) \\] \\(\\eta_0\\) represents our prior “degrees of freedom” for the between-schools variance \\(\\tau^2.\\) Conditional on mean for \\(m\\) of those schools we get “posterior degrees of freedom” \\(\\eta_0 + m.\\) 14.1.2 Full conditional of \\(\\theta_j\\) \\(\\theta_j : j = 1, …, m.\\) \\[ p\\left(\\theta_{j} \\mid \\mu, \\tau^{2}, \\sigma^{2}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m}\\right) \\propto p\\left(\\theta_{j} \\mid \\mu, \\tau^{2}\\right) \\prod_{i=1}^{n_{j}} p\\left(y_{i, j} \\mid \\theta_{j}, \\sigma^{2}\\right) \\] Conditionally on { \\(\\mu, \\tau^2, \\sigma^2, \\boldsymbol y_j\\) }, the \\(\\theta_j\\) are independent of each other as well as independent of the data from the other groups. \\[ \\theta_{j} \\mid \\mu, \\tau^{2}, \\sigma^{2}, \\boldsymbol{y}_{j} \\sim \\text { Normal}\\left(\\frac{\\mu / \\tau^{2}+n_{j} \\bar{y}_{j} / \\sigma^{2}}{1 / \\tau^{2}+n_{j} / \\sigma^{2}}, \\frac{1}{1 / \\tau^{2}+n_{j} / \\sigma^{2}}\\right) \\] 14.1.3 Full conditional of \\(\\sigma^2\\) It’s going to involve the sample variances within schools for all \\(m\\) different schools. We have \\(\\nu_0\\) “prior observations” with sample variance of \\(\\sigma_0^2\\). We have a total of \\(n_1 + n_2 + … + n_m\\) observations of “sample variance.” \\[ p\\left(\\sigma^{2} \\mid \\theta_{1}, \\ldots, \\theta_{m}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m}\\right) \\propto p\\left(\\sigma^{2}\\right) \\prod_{j=1}^{m} \\prod_{i=1}^{n_{j}} p\\left(y_{i, j} \\mid \\theta_{j}, \\sigma^{2}\\right) \\] \\[ \\sigma^{2} \\mid \\boldsymbol{\\theta}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m} \\sim \\text { inverse-gamma }\\left(\\frac{\\nu_{0}+\\sum n_{j}}{2}, \\frac{\\nu_{0} \\sigma_{0}^{2}+\\sum \\sum\\left(y_{i, j}-\\theta_{j}\\right)^{2}}{2}\\right) \\] This is a “semi-conjugate” model. Not fully conjugate because our priors are independent but these parameters are NOT independent in the posterior but the full conditional posteriors have the same form as the marginal priors. We have all the tools! We can do posterior approximations using the Gibbs sampler. Yay for us! 14.2 Example: Math scores in U.S. public schools Because the sample sizes are different it’s not gonna work to store the whole data set in a matrix we need a list. # Compile the school means, medians, and standard deviations m &lt;- length(unique(Data[,1])) # number of schools is m=100 y.all &lt;- list() n &lt;- rep(NA, m); ybar &lt;- ymed &lt;- s2 &lt;- n for(j in 1:m) { y &lt;- Data[Data[,1]==j, 2] n[j] &lt;- length(y) ybar[j] &lt;- mean(y) ymed[j] &lt;- median(y) s2[j] &lt;- var(y) y.all[[j]] &lt;- y; rm(y); } In this plot we have 100 different schools. The school id 1, 2, …, 100 is just an id it’s not numerically meaningful so the “scatterplot” idea isn’t really applicable here. So if order is not meaningful we can choose any order we like so let’s choose the one that corresponds to increasing sample mean. plot(c(1,m), range(y.all), type=&quot;n&quot;, ylab=&quot;math score&quot;, xlab=&quot;rank of school-specific average&quot;) for(k in 1:m) { j &lt;- order(ybar)[k] points( rep(k, n[j]), y.all[[j]], pch=19, cex=.5) segments(k, min(y.all[[j]]), k, max(y.all[[j]]) ) } abline(h=mean(ybar)) Figure 14.1: Plot of all data, arranged in ascending order of school-specific average score This plot helps us see the between-schools versus within-schools variation in the observed scores and what we see is within-school variation is much greater i.e, \\(\\sigma^2 &gt; \\tau^2.\\) var(ybar); mean(s2) # mean(sapply(y.all, var)) ## [1] 30.99 ## [1] 82.94 The between-group-means sample variance is 31. The average within-groups variance is 82. This is consistent with the picture above. Most of the groups are pretty close to each other, whereas the variability within a group can vary quite a lot. # Histogram of school averages hist(ybar, freq=F, right=F, breaks=30, main=&quot;&quot;, col=&quot;pink&quot;) Figure 14.2: Empirical distribution of sample means \\(\\bar y_j\\) vary from about 35 to about 65. c(which.min(ybar),which.max(ybar)) ## [1] 5 67 ybar[c(which.min(ybar), which.max(ybar))] ## [1] 36.58 65.02 When it comes to estimating the \\(\\theta_j,\\) taking an overall average is probably not such a great idea. Clearly the average score at school 67 is greater than the average score at school 5. Should we just estimate the school averages by the sample averages? We could do that but some of those estimates would have very high variance because some of the sample sizes are really small. E.g. school 67 has a sample size of 4. n[67] ## [1] 4 Let’s look at a visual relationship between sample size and averages. plot(ybar ~ n, xlab=&quot;sample size&quot;, ylab=&quot;sample mean&quot;) abline(h=mean(ybar)) Figure 14.3: Relationship between sample mean versus sample size for the 100 schools There’s no real trend in this plot, but there is something going on here. The schools whose sample mean tends to be farthest from the overall mean tend to be schools where there is a low sample size. This relationship between sample averages and sample size is fairly common in hierarchical datasets. Remember that variance of the sample average \\(=\\sigma^2/n_j\\) depends on the sample size so the more the sample size, the closer to the sample average. This has me thinking… School 67 has an average score of 65 but it’s only based on 4 students! so what’s your best estimate of \\(\\theta_{67}\\)? c(n[67],ybar[67],mean(ybar)) ## [1] 4.00 65.02 48.13 Is it 65(no pooling)? Is it 48(full pooling)? Should it be somewhere in between? I’m thinking it should be somewhere in between. There is reason to believe the average score is higher at this school (higher than the overall average), but it’s probably not as high as the sample average indicates being it’s only 4 students. The two-stage sampling scheme (see 14.1) is important, but the BIG IDEA is that under this model we can use information in \\(\\mathbf Y_1, \\mathbf Y_2, …, \\mathbf Y_{m-1}\\) to inform our inference about \\(\\theta_m\\). Those other \\(\\mathbf Y_j\\) won’t have as much influence on our inference about \\(\\theta_m\\) as \\(Y_m\\) will but they won’t be ignored either. 14.3 Posterior approximation We need prior distributions for \\(\\mu\\) (overall mean); we need\\((\\mu_0, \\gamma_0^2)\\) \\(\\sigma^2\\)(within-group variance); we need\\((\\nu_0, \\sigma_0^2)\\) \\(\\tau^2\\) (between-groups variance); we need\\((\\eta_0, \\tau_0^2)\\) The test is purported to have an average score of 50 and variance of 100. That 100 includes both within-school and between-school variance. So we actually have prior belief that \\(\\sigma^2 + \\tau^2\\) should be close to 100. Between school variance cannot be more than 100 so we take \\(\\tau_0^2 = 100\\). We also take \\(\\sigma_0^2 = 100\\). These are overstatements since \\(\\sigma^2 + \\tau^2 \\approx 100,\\) but it won’t affect our inference that much because we got lots of data. Now the Gibbs sampler! sample \\(\\mu^{(s+1)} \\sim p\\left(\\mu \\mid \\theta_{1}^{(s)}, \\ldots, \\theta_{m}^{(s)}, \\tau^{2(s)}\\right)\\); sample \\(\\tau^{2(s)} \\sim p\\left(\\tau^{2} \\mid \\theta_{1}^{(s)}, \\ldots, \\theta_{m}^{(s)}, \\mu^{(s+1)}\\right)\\); sample \\(\\sigma^{2(s)} \\sim p\\left(\\sigma^{2} \\mid \\theta_{1}^{(s)}, \\ldots, \\theta_{m}^{(s)}, \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{m}\\right)\\) for each \\(j \\in\\{1, \\ldots, m\\}\\), sample \\(\\theta_{j}^{(s+1)} \\sim p\\left(\\theta_{j} \\mid \\mu^{(s+1)}, \\tau^{2(s+1)}, \\sigma^{2(s+1)}, \\boldsymbol{y}_{j}\\right)\\) It doesn’t matter what order your Gibbs sampler goes in. It just matters that you use the most updated values (which is the easiest way to code, as it happens). # Prior parameters nu.0 &lt;- 1 ; sigma2.0 &lt;- 100; eta.0 &lt;- 1 ; tau2.0 &lt;- 100; mu.0 &lt;- 50; gamma2.0 &lt;- 25 ; # Starting values for the Gibbs sampler theta &lt;- ybar # group sample means sigma2 &lt;- mean(s2) # average of within-group variances mu &lt;- mean(theta); # overall average tau2 &lt;- var(theta) # between-groups variance # Now run the Gibbs sampler! S &lt;- 5000 theta.chain &lt;- matrix(NA, S, m) sigma2.chain &lt;- rep(NA, S); mu.chain &lt;- rep(NA, S); tau2.chain &lt;- rep(NA, S); for(s in 1:S) # Update theta&#39;s, then sigma2, then mu, then tau2 { V.theta &lt;- 1 / (1/tau2 + n/sigma2) m.theta &lt;- V.theta * (mu/tau2 + n*ybar/sigma2) theta &lt;- rnorm(m, mean=m.theta, sd=sqrt(V.theta)) ##### nu.n &lt;- nu.0 + sum(n) ss.n &lt;- nu.0 * sigma2.0 for(j in 1:m){ ss.n &lt;- ss.n + sum( (y.all[[j]] - theta[j])^2 ) } sigma2 &lt;- 1 / rgamma(1, nu.n/2, ss.n/2) ##### V.mu &lt;- 1 / (1/gamma2.0 + m/tau2) m.mu &lt;- V.mu * (mu.0/gamma2.0 + sum(theta)/tau2) mu &lt;- rnorm(1, mean=m.mu, sd=sqrt(V.mu)) ##### eta.m &lt;- eta.0 + m ssq.m &lt;- eta.0 * tau2.0 + sum( (theta-mu)^2 ) tau2 &lt;- 1 / rgamma(1, eta.m/2, ssq.m/2) ##### theta.chain[s,] &lt;- theta ; mu.chain[s] &lt;- mu sigma2.chain[s] &lt;- sigma2; tau2.chain[s] &lt;- tau2 } Notice that there is no loop needed for \\(\\texttt{theta = theta[1], theta[2], ….,theta[100]}\\) 14.4 MCMC diagnostics # Split the chain into 10 blocks, and make comparative boxplots by block block &lt;- rep(1:10, rep(S/10,10)) par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0)) boxplot(mu.chain ~ block) boxplot(sigma2.chain ~ block) boxplot(tau2.chain ~ block) Figure 14.4: Stationarity plots of the MCMC samples of mu, sigma2, tau2 This is an alternative to trace plots which get messy when you have a lot of iterations. These are called “stationarity plots.” Take your \\(S\\) updates. Break your chain ( which consist of \\(S\\) iterations) into \\(10\\) pieces ( each is \\(S/10\\) values ) and look at the boxplots. For each block, if the chain is stationary that means the distribution of the early iterations is the same as that of the later iterations (we should not see a trend in the boxplots). Also, if the starting value for the Gibbs chain were a draw from the target distribution then stationarity would hold exactly. No issue with stationarity here. It would have been VERY surprising to see a non-stationarity problem suggested by these plots for our example because we used a Gibbs sampler with reasonable starting values. For other more complicated plots that we will study in the upcoming weeks, these plots will become more than just a formality to check them. Stationarity was one of the two complicating factors in the use of MCMC versus ordinary Monte Carlo. The other was mixing aka autocorrelation aka the draws are NOT independent. Is that an issue in our example? Let see! par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0)) acf(mu.chain, main=&quot;&quot;);acf(sigma2.chain, main=&quot;&quot;);acf(tau2.chain, main=&quot;&quot;) Figure 14.5: Sample autocorrelation functions of mu, sigma2, tau2, respectively Not at all. There’s some autocorrelation, but not much! round(acf(mu.chain, plot=F)$acf[1:4,,1],3) ## [1] 1.000 0.147 0.021 0.000 The lag-1 autocorrelation \\(\\rho[1] = 0.15\\) for the \\(\\mu\\)-chain. Also, \\(\\mu^{(s)}\\) is somewhat correlated with \\(\\mu^{(s-1)}\\) but practically zero correlation with \\(\\mu^{(s-2)}\\). round(acf(sigma2.chain, plot=F)$acf[1:4,,1],3) ## [1] 1.000 0.037 0.001 0.028 Even better story in the \\(\\sigma^2\\) round(acf(tau2.chain, plot=F)$acf[1:4,,1],3) ## [1] 1.000 0.337 0.105 0.033 Slightly worse one in the \\(\\tau^2\\) chain. Effective sample sizes library(mcmcse) c(ess(mu.chain), ess(sigma2.chain), ess(tau2.chain)) ## [1] 3508 4210 2128 Nominal sample size was 5000, so these effective sample sizes are not bad at all. This is because there’s so little autocorrelation. Note that we got the worse autocorrelation for \\(\\tau^2\\) which is consitent with \\(\\tau^2\\) having the least effective sample size. Next, let’s use the Gibbs draws to describe our posterior beliefs about \\(\\mu\\) and \\(\\sigma^2\\) and \\(\\tau^2\\) and \\(\\theta_1, \\theta_2, …, \\theta_{100}.\\) quants.mu &lt;- quantile(mu.chain, c(.025, .5, .975)) Posterior 95% interval for \\(\\mu\\) is \\([47.1, 49.2]\\) quants.sigma2 &lt;- quantile(sigma2.chain, c(.025, .5, .975)) For \\(\\sigma^2\\) our posterior interval is \\([79.7, 90.5]\\) \\(\\sigma\\) between \\(8.9\\) to \\(9.5.\\) quants.tau2 &lt;- quantile(tau2.chain, c(.025, .5, .975)) 95% interval for \\(\\tau^2\\) is \\([17.4, 34.6]\\) par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0)) plot(density(mu.chain), lwd=2, xlab=&quot;mu&quot;, ylab=&quot;p(mu|y)&quot;, main=&quot;&quot;) abline(v=quants.mu, col=&quot;pink&quot;, lty=c(3,2,3)) plot(density(sigma2.chain), lwd=2, xlab=&quot;sigma2&quot;, ylab=&quot;p(sigma2|y)&quot;, main=&quot;&quot;) abline(v=quants.sigma2, col=&quot;pink&quot;, lty=c(3,2,3)) plot(density(tau2.chain), lwd=2, xlab=&quot;tau2&quot;, ylab=&quot;p(tau2|y)&quot;, main=&quot;&quot;) abline(v=quants.tau2, col=&quot;pink&quot;, lty=c(3,2,3)) Figure 14.6: Posterior summaries of mu, sigma2 and tau2 with 2.5%, 50% and 97.5% quantiles given by vertical lines. The sample average of the \\(j\\)th column in the \\(\\texttt{theta.chain}\\) matrix approximates the marginal posterior mean \\(E[\\theta_j|y]\\). For example the marginal posterior density for \\(\\theta_1\\) is below. hist(theta.chain[,1], main = &quot;&quot;, col = &quot;pink&quot;, freq=FALSE, xlab=expression(theta[1]), border=&quot;lightpink&quot;) lines(density(theta.chain[,1]), col=&quot;red&quot;, lwd=2) Figure 14.7: marginal posterior density for theta1 Let’s talk about school 67. Average score was 65, but it was only 4 students. round(c(n[67],ybar[67],mean(ybar)),1) ## [1] 4.0 65.0 48.1 How will our model estimate \\(\\theta_{67}?\\) Is it gonna be 65, 48, or somewhere in between? let’s see mean(theta.chain[,67]) ## [1] 57.25 14.5 Shrinkage One of the primary strength of the hierarchical model for data like this is that the hierarchical model allows us to borrow information from other groups to estimate group means. That is \\(\\hat \\theta_{73}\\) will depend mostly on \\(\\bar y_{73}\\) but will also make use of the \\(\\bar y\\)’s for all the other schools. How much use? Depends mostly on: \\(n_{73}\\). If this value is big there will be little shrinkage if \\(n_{73}\\) is low there will be more shrinkage. The sample average of the \\(j\\)th column in the \\(\\texttt{theta.chain}\\) matrix approximates the marginal posterior mean \\(E(\\theta_j | Y).\\) \\[ \\mathrm{E}\\left(\\theta_{j} \\mid \\boldsymbol{y}_{j}, \\mu, \\tau, \\sigma\\right)=\\frac{\\bar{y}_{j} n_{j} / \\sigma^{2}+\\mu / \\tau^{2}}{n_{j} / \\sigma^{2}+1 / \\tau^{2}} \\] The above is conditional on the other parameter values and that conditional expectation is a weighted average of \\(\\mu\\) (the prior mean which gets weight \\(1/\\tau^2\\) ) and \\(\\bar y_j\\) ( the group \\(j\\) sample mean which gets weight \\(n_j / \\sigma^2\\) ). Another way to say this is that the expected value of \\(\\theta_j\\) is pulled from \\(\\bar y_j\\) back towards \\(\\mu.\\) That phenomenon is called shrinkage. It “shrinks” the variability between the groups. Hence, our estimates \\(\\hat \\theta_1 , \\hat \\theta_2, …, \\hat \\theta_{100}\\) are going to be less variable than the sample means \\(\\bar y_1, \\bar y_2, …, \\bar y_{100}\\). The \\(\\hat \\theta\\)’s are going to be “shrunk” back toward the overall average. The following plot shows \\(\\hat \\theta_j = E(\\theta_j|\\boldsymbol y_1,...,\\boldsymbol y_m)\\) versus \\(\\bar y_j\\) for each group. par(mgp=c(2,0.5,0)) theta.hat &lt;- apply(theta.chain, 2, mean) plot(theta.hat ~ ybar, ylim=range(ybar), pch=19, ylab = expression(hat(theta)), xlab=expression(bar(y))) abline(0, 1, lty=2); abline(h=mean(ybar), lty=2); Figure 14.8: Study the shrinkage effect In this problem the shrinkage is not huge. The points on this plot are \\((\\bar y_j, \\hat \\theta_j ).\\) If we were doing entirely separate estimates with no pooling of information among the different schools all the points would sit right on the diagonal dashed line. If there was no between-school variability and the 1993 students were just a random sample from the population of all students we would get the dashed horizontal line is what we’d get. We don’t believe both! What we believe is somewhere in between. How much shrinkage is happening? If we define shrinkage as the magnitude of the difference between \\(\\hat \\theta\\) and \\(\\bar y\\) i.e, \\(\\bar y - \\boldsymbol {\\hat\\theta}\\), and we plot shrinkage versus the sample size we get: shrink &lt;- abs(theta.hat - ybar) plot(shrink ~ n, pch=19) lines(lowess(shrink ~ n), lty=2, lwd=2) Figure 14.9: Shrinkage as a function of sample size. In general (it’s not a perfect association because there’s other things going on too) the bigger the sample size the less shrunk the estimate is going to be. 14.6 Ranking the groups Suppose our job is to rank the schools from 1 to 100 in terms of \\(E(\\theta_j | Y )\\). What we’d find is: It is NOT the same ranking as ranking them by sample average. We’ll illustrate this by comparing two of the schools. Consider school 46 and school 82. # Reproduce Figure 8.9 on page 142 of Hoff (2009) xlim &lt;- range(c(y.all[[46]], y.all[[82]])) plot(density(theta.chain[,46], adj=2), xlim=xlim, lwd=2, main=&quot;&quot;, xlab=&quot;Math score&quot;, ylim=c(-.06, .22)) lines(density(theta.chain[,82], adj=2), lwd=2, col=&quot;pink&quot;) abline(h=0) points(y.all[[46]], rep(-.02, n[46]), pch=19) points(ybar[46], -.02, pch=19, cex=2.5) abline(h=-.02) points(y.all[[82]], rep(-.04, n[82]), col=&quot;pink&quot;, pch=19) points(ybar[82], -.04, col=&quot;pink&quot;, pch=19, cex=2.5) abline(h=-.04, col=&quot;pink&quot;) segments(mean(mu.chain), 0, mean(mu.chain), 1, lty=2, lwd=2) legend(&quot;topright&quot;, inset=.05, lwd=2, lty=c(1,1,2), col=c(&quot;black&quot;, &quot;pink&quot;, &quot;black&quot;), legend=c(&quot;School 46&quot;, &quot;School 82&quot;, &quot; E(mu|Y)&quot;) ) Figure 14.10: Compare posterior distributions of theta[46] and theta[82] The black curve is the posterior of \\(\\theta_{46}\\) the pink one is the posterior of \\(\\theta_{82}.\\) \\(E(\\theta_j|Y)\\) is bigger for \\(j=82\\) than for \\(j=46\\), hence our belief is that \\(\\theta_{82} &gt; \\theta_{46}\\). Just under these posterior densities we’ve got some dotplots with the “raw data” scores for these two groups. The black points are school 46 \\((n_{46} = 21).\\) The pink points are for school 82 \\((n_{82} = 4)\\). The biggest point is the average value. Note the big pink dot is to the left of the big black one. In other words, \\(\\bar y_{82} &lt; \\bar y_{46}.\\) c(ybar[46],ybar[82]) # sample mean ## [1] 40.18 38.76 c(theta.hat[46],theta.hat[82]) # posterior mean ## [1] 41.37 42.55 So the relation between these quantities is reversed! \\(\\theta_{82} &gt; \\theta_{46}\\) but \\(\\bar y_{82} &lt; \\bar y_{46}.\\) How can this be? Shrinkage! (Because there’s so little data for group 82 that estimate gets shrunk toward the overall mean by more than does the estimate for group 46.) Every \\(\\hat \\theta\\) is a weighted average of \\(\\bar y_j\\) (the school-specific mean) and \\(&quot;\\bar y_\\bullet&quot;\\) the grand mean over all schools. How much weight is shifted from the particular school to the overall mean depends on the school sample size. c(n[46],n[82]) #sample size ## [1] 21 5 c(s2[46],s2[82]) #sample variance ## [1] 105.0 124.5 The smaller the sample size is the greater weight is given to the overall mean rather than that specific group mean. School 46 had an average score of 40 but sample size of 21 so shrinkage back up to overall mean (dashed vertical line) is just a little bit. School 82 has only \\(n=5\\) students so while their average score was 38.8 because there’s so much variance in this estimate we shrink it back up to the point where it actually passes up the value of school 46. Interpretation: While the average score was lower at school 82 the evidence of a “low true average” is stronger for school 46 because it was based on a larger sample size. How often does this happen? Looking again at figure 14.9, if there were no reorderings going on, the trend would be monotone i.e., you wouldn’t have any jumping around. However you see some jumping around in the top right. The school with the highest \\(\\bar y\\) does not have the highest \\(\\hat \\theta\\). At least two schools are higher. What schools are those? order(ybar)[98:100];order(theta.hat)[98:100] ## [1] 79 51 67 ## [1] 67 79 51 The top performing schools in terms of \\(\\bar y\\) were 67 then 51 then 79, however, the top performing schools in terms of \\(\\hat \\theta\\) were 67 then 79 then 51. 67 has the highest \\(\\bar y\\) but the lowest \\(\\hat \\theta\\) of the three. What must be true of the sample sizes for these three schools? It must be that \\(n_{67}\\) is the smallest since it’s getting shrunk the most toward the overall mean mean(ybar)=48. Let’s see mat &lt;- round(rbind(c(ybar[79],ybar[51],ybar[67]), c(theta.hat[79],theta.hat[51],theta.hat[67]), n[c(79, 51, 67)]),1) colnames(mat) &lt;- c(&quot;school 79&quot;, &quot;school 51&quot;, &quot;school 67&quot;) rownames(mat) &lt;- c(&quot;ybar&quot;, &quot;theta.hat&quot;, &quot;sample size&quot;) knitr::kable(mat) school 79 school 51 school 67 ybar 61.7 64.4 65.0 theta.hat 58.8 61.8 57.2 sample size 13.0 19.0 4.0 While the highest observed performance was achieved by school 67 the strongest evidence of a high-achieving school exists for school 51. "],["linear-regression.html", "Lecture 15 Linear Regression 15.1 Example: Oxygen uptake 15.2 Least squares estimation 15.3 Least squares estimation for oxygen uptake data 15.4 Bayesian estimation for a regression model 15.5 Unit information prior 15.6 Zellner’s \\(g\\)-prior 15.7 Bayesian analysis using invariant \\(g\\)-prior 15.8 Bayesian analysis using semiconjugate prior", " Lecture 15 Linear Regression The following notes, mostly transcribed from Neath(0527,2021) lecture, summarize sections(9.1 and 9.2) of Hoff(2009). Linear regression modeling is an extremely powerful data analysis tool, useful for a variety of inferential tasks such as prediction, parameter estimation and data description. Estimation of \\(p(y|\\boldsymbol{x})\\) is made using the data \\(y_1,...,y_n\\) that are gathered under a variety of conditions \\(\\boldsymbol{x}_1,...,\\boldsymbol{x}_n.\\) 15.1 Example: Oxygen uptake The experiment is this: 12 men aged between 20 and 31 who were not regular exercisers but were healthy. They were recruited to take part in the study of the effects of two different exercise regimen. Six men were randomly assigned to running, six men were randomly assigned to step aerobics ( Here is the wikipedia page about step aerobics, and I believe this must be the most 90’s photo on the internet. ) The response variable is change in oxygen uptake from before to after the 12 week exercise program. Here are the data: program &lt;- c( 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1) age &lt;- c(23,22,22,25,27,20,31,23,27,28,22,24) y &lt;- c( -0.87,-10.74, -3.27, -1.97, 7.50, -7.25, 17.05, 4.96, 10.40, 11.05, 0.26, 2.51) plot(y ~ age, pch=19+2*program, bg=c(&quot;black&quot;,&quot;pink&quot;)[program+1], xlab=&quot;age&quot;, ylab=&quot;change in maximal oxygen uptake&quot;) legend(&quot;bottomright&quot;, inset=.10, pt.bg=c(&quot;black&quot;,&quot;pink&quot;), pch=c(19,21), legend=c(&quot;running&quot;, &quot;aerobics&quot;)) abline(h=0) Figure 15.1: Change in maximal oxygen uptake as a function of age and exercise program. There are six black dots (the six men who did the running program) and six pink ones (those men did the aerobics). No change at all in oxygen uptake would be \\(y=0,\\) the horizontal line. It looks like five of the six men who did the running program actually came in lower, but we don’t wanna just look at this. We want a statistical model that establishes a relationship between change in oxygen uptake and the predictor variables. This is a regression problem. The response variable is change in oxygen uptake \\(y\\). There are two predictor variables (1) exercise program which is a binary variable and (2) age. \\[ \\text{program} = \\begin{cases} 1\\text{ if aerobic}\\\\0\\text{ if running} \\end{cases} \\] We could ignore age and just compare the six men who did the running with the six men who did aerobics but that would be throwing away crucial information because the effect of the exercise program on lung function may vary by age and we want to account for that. It wouldn’t be wrong to ignore it just wouldn’t be the best analysis we can do. Remember we randomly assigned exercise program so there shouldn’t be a systematic tendency for the younger men to do one program and the older men to do another program, but there might be just by chance variation. There’s no really strong indication of a complex relationship (e.g., adding quadratic terms) so let’s not model a complex relationship. Let’s keep it simple and assume that the relationship between change in oxygen uptake and age is a straight line and that there is one line for running and a different line for aerobics. We are effectively gonna fit two regression lines. In a bayesian linear regression model we will say that that the \\(i\\)th observation is: \\[ Y_i = \\beta_1 x_{i,1} + … + \\beta_p x_{i,p} + \\epsilon_i \\quad i = 1, …, n \\] In our example we have \\[ Y_i=\\beta_1x_{i,1}+\\beta_2\\texttt{program}+\\beta_3\\texttt{age}+\\beta_4\\texttt{program:age}+\\epsilon_i \\] \\(x_{i,1} = 1\\) for every subject \\(i\\). This is the intercept. This notation is common in bayesian statistics, where intercept is \\(\\beta_1\\) rather than \\(\\alpha\\) or \\(\\beta_0\\) \\(x_{i,2}\\) is the exercise program indicator; \\(x_{i,2} = 0\\) for running, \\(x_{i,2} = 1\\) for aerobics \\(x_{i,3} =\\) age in years of subject \\(i\\) \\(x_{i,4} = x_{i,2} \\times x_{i,3}\\) is the interaction term We’re fitting separate regression lines for the two exercise programs so there are two slopes and two intercepts. We will further assume that the error term \\(\\epsilon_i \\stackrel{\\text{iid}}\\sim\\) Normal\\((0 ,\\sigma^2 )\\). This is all conditional on the parameters \\(\\boldsymbol\\beta = (\\beta_1, \\beta_2, \\beta_3, \\beta_4)\\) and \\(\\sigma^2\\). \\[ \\begin{array}{l} \\text{running}=\\mathrm{E}[Y \\mid \\boldsymbol{x}]=\\beta_{1}+\\beta_{3}\\texttt{age } \\\\ \\text{aerobics}=\\mathrm{E}[Y \\mid \\boldsymbol{x}]=\\left(\\beta_{1}+\\beta_{2}\\right)+\\left(\\beta_{3}+\\beta_{4}\\right)\\texttt{age } \\end{array} \\] \\(\\beta_3\\) is the slope for the running program, \\(\\beta_3 + \\beta_4\\) is the slope for the aerobics program. 15.2 Least squares estimation If we were doing maximum likelihood we’d write out only the likelihood part of the model, there would be no rule for a prior distribution reflecting prior belief about \\(\\boldsymbol\\beta\\) and \\(\\sigma^2,\\) there would only be a likelihood and it would be: \\[ \\begin{array}{l} p\\left(y_{1}, \\ldots, y_{n} \\mid \\boldsymbol{x}_{1}, \\ldots \\boldsymbol{x}_{n}, \\boldsymbol{\\beta}, \\sigma^{2}\\right) =\\prod_{i=1}^{n} p\\left(y_{i} \\mid \\boldsymbol{x}_{i}, \\boldsymbol{\\beta}, \\sigma^{2}\\right) \\\\ \\hfill=\\left(2 \\pi \\sigma^{2}\\right)^{-n / 2} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\boldsymbol{\\beta}^{T} \\boldsymbol{x}_{i}\\right)^{2}\\right\\} \\end{array} \\] Under the assumption of normality maximizing the likelihood is equivalent to minimizing the sum of squares! We see that \\(\\boldsymbol\\beta\\) only appears inside \\(\\text{exp()}\\) so maximizing \\(e^{-\\text{something}}\\) is equivalent to minimizing that something. So the MLE of \\(\\boldsymbol\\beta\\) is the value of \\(\\boldsymbol\\beta\\) that minimizes \\(\\sum{ (y_i - \\boldsymbol\\beta^T \\boldsymbol x_i )^2 }\\). We can also express our model in terms of a multivariate normal distribution. \\[ \\{~\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^{2}~\\} \\sim \\operatorname{Normal}_{n}\\left(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^{2} \\mathbf{I}\\right), \\] where \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix and \\[ \\mathbf{X} \\boldsymbol{\\beta}=\\left(\\begin{array}{c} \\boldsymbol{x}_{1} \\rightarrow \\\\ \\boldsymbol{x}_{2} \\rightarrow \\\\ \\vdots \\\\ \\boldsymbol{x}_{n} \\rightarrow \\end{array}\\right)\\left(\\begin{array}{c} \\beta_{1} \\\\ \\vdots \\\\ \\beta_{p} \\end{array}\\right)=\\left(\\begin{array}{c} \\beta_{1} x_{1,1}+\\cdots+\\beta_{p} x_{1, p} \\\\ \\beta_{1} x_{2,1}+\\cdots+\\beta_{p} x_{2, p} \\\\ \\vdots \\\\ \\beta_{1} x_{n, 1}+\\cdots+\\beta_{p} x_{n, p} \\end{array}\\right)=\\left(\\begin{array}{c} \\mathrm{E}\\left[Y_{1} \\mid \\boldsymbol{\\beta}, \\boldsymbol{x}_{1}\\right] \\\\ \\mathrm{E}\\left[Y_{2} \\mid \\boldsymbol{\\beta}, \\boldsymbol{x}_{2}\\right] \\\\ \\vdots \\\\ \\mathrm{E}\\left[Y_{n} \\mid \\boldsymbol{\\beta}, \\boldsymbol{x}_{n}\\right] \\end{array}\\right) \\] The vector of responses \\(\\mathbf{Y} = (Y_1, Y_2, …, Y_n)\\) has an \\(n\\)-variate normal distribution with a mean of \\(\\mathbf{X}\\boldsymbol\\beta\\). \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix. The rows of \\(\\mathbf{X}\\) correspond to the \\(n\\) units, the columns of \\(\\mathbf{X}\\) correspond to \\(p\\) regressors. The sampling density depend on \\(\\boldsymbol\\beta\\) only through the squared residuals \\(SSR(\\boldsymbol\\beta) = \\sum{ (y_i - \\boldsymbol\\beta^T \\boldsymbol x_i )^2 }.\\) So we can do vector calculus to find the value of \\(\\boldsymbol \\beta\\) that minimizes this \\(SSR\\) (sum of squared residuals) which is the same thing as \\(RSS\\) (residual sum of squares). To minimize a function \\(f:R^p \\mapsto R,\\) take all \\(p\\) partial derivatives set them to zero and solve \\[ \\frac{d}{d \\boldsymbol{\\beta}} \\operatorname{SSR}(\\boldsymbol{\\beta})=\\frac{d}{d \\boldsymbol{\\beta}}\\left(\\boldsymbol{y}^{T} \\boldsymbol{y}-2 \\boldsymbol{\\beta}^{T} \\mathbf{X}^{T} \\boldsymbol{y}+\\boldsymbol{\\beta}^{T} \\mathbf{X}^{T} \\mathbf{X} \\boldsymbol{\\beta}\\right)=-2\\mathbf{X}^T\\boldsymbol y+2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\] Therefore, \\[ \\begin{aligned} \\frac{d}{d \\boldsymbol{\\beta}} \\operatorname{SSR}(\\boldsymbol{\\beta})=\\boldsymbol 0 &amp;\\iff -2\\mathbf{X}^T\\boldsymbol y+2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}=0\\\\ &amp;\\iff \\boldsymbol{\\beta}=\\left(\\mathbf{X}^{T} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{T} \\boldsymbol{y}=\\boldsymbol{\\beta}_{MLE}=\\boldsymbol{\\beta}_{ols}\\\\[0.1cm] &amp;= \\texttt{solve(t(X) %*% X) %*% t(X) %*% y} \\end{aligned} \\] \\(d/d\\boldsymbol\\beta\\) is a \\(p\\)-vector, the \\(j\\)-th entry is the partial derivative with respect to \\(\\beta_j\\). And if the matrix \\(\\mathbf{X}\\) has full rank, \\(p &lt; n\\) (there are not more variables than observations), and the columns are linearly independent (there is not a redundant variable), then \\(\\mathbf{X}^T\\mathbf{X}\\) is invertible and \\(\\hat{\\boldsymbol\\beta}\\), the MLE of \\(\\boldsymbol\\beta\\) is unique. 15.3 Least squares estimation for oxygen uptake data # OLS estimation n &lt;- length(y) X &lt;- cbind(rep(1,n), program, age, program*age) p &lt;- dim(X)[2] rownames(X) &lt;- 1:n colnames(X) &lt;- paste(&quot;x&quot;, 1:p, sep=&quot;&quot;) head(X,3) ## x1 x2 x3 x4 ## 1 1 0 23 0 ## 2 1 0 22 0 ## 3 1 0 22 0 (beta.hat.ols &lt;- as.vector(solve( t(X) %*% X ) %*% t(X) %*% y)) ## [1] -51.2939 13.1071 2.0947 -0.3182 # using the lm function lm(y~program + age + program:age)$coeff ## (Intercept) program age program:age ## -51.2939 13.1071 2.0947 -0.3182 So we have \\(E(Y_i) = -51.29 +13.11 \\texttt{program} + 2.09\\texttt{age}-0.32\\texttt{program:age}\\) The intercept for aerobics is higher than that for running. The slope for running is \\(2.19\\) and the slope for aerobics is \\(2.095-0.318 = 1.78.\\) Let’s see.. plot(y ~ age, pch=19+2*program, bg=c(&quot;black&quot;,&quot;pink&quot;)[program+1], xlab=&quot;age&quot;, ylab=&quot;change in maximal oxygen uptake&quot;) legend(&quot;bottomright&quot;, inset=.10, pt.bg=c(&quot;black&quot;,&quot;pink&quot;), pch=c(19,21), legend=c(&quot;running&quot;, &quot;areobics&quot;)) b &lt;- beta.hat.ols; abline(b[1], b[3], lwd=2) abline(h=0) abline(b[1]+b[2], b[3]+b[4], lwd=2, col=&quot;pink&quot;); rm(b); An unbiased estimator for \\(\\sigma^2 = SSR(\\hat{\\boldsymbol\\beta}_{ols})/(n-p) = \\hat\\sigma^2_{ols}\\) (sigma2.hat &lt;- sum( (y - X %*% beta.hat.ols)^2 ) / (n-p)) ## [1] 8.542 Although we are not doing maximum likelihood and we’re not worried about unbiasedness, we will use these quantities for our bayesian inference. \\(SE(\\boldsymbol{\\hat\\beta}) = (\\mathbf{X}^{T} \\mathbf{X})^{-1}\\hat\\sigma^2_{ols}\\) # Standard errors of beta.hat terms SE &lt;- as.vector(sqrt(diag(sigma2.hat * solve(t(X) %*% X)))) beta.hat.ols / SE ## [1] -4.1865 0.8316 3.9796 -0.4898 If you take \\(\\boldsymbol{\\hat\\beta}/SE(\\boldsymbol{\\hat\\beta})\\) and this ratio is not bigger (in absolute value) than at least 2 or so then there is no compelling evidence of what sign that \\(\\beta_j\\) has. Could be negative, could be positive. That’s the case (in these data) for \\(\\beta_2\\)(program) and \\(\\beta_4\\)(program:age). The statistical evidence of an age association is fairly strong (3.9) but the statistical evidence of a program effect (difference between running and aerobics) is not so strong (0.83). This model (separate intercepts and separate slopes) is more complicated than you might at first think. In particular, it’s not justified to conclude “no program effect” just because \\(\\beta_2\\) and \\(\\beta_4\\) are both indistinguishable from zero. Maybe we can make sharper conclusions from a Bayesian analysis. 15.4 Bayesian estimation for a regression model Consider this model \\[ \\{~\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^{2}~\\} \\sim \\operatorname{Normal}_{n}\\left(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^{2} \\mathbf{I}\\right), \\] Everything in a regression analysis is conditional on \\(\\mathbf{X}\\). The mean depends on \\(\\boldsymbol\\beta\\) the variance is \\(\\sigma^2\\). If you think the conjugate priors are \\(p\\)-variate normal distribution for \\(\\boldsymbol\\beta\\) and inverse-gamma for \\(\\sigma^2\\) you would be right. \\[ \\boldsymbol\\beta \\sim \\operatorname{Normal}_{p}\\boldsymbol(\\boldsymbol\\beta_0,\\boldsymbol\\Sigma_0) \\] \\(\\boldsymbol\\Sigma_0\\) is a \\(p \\times p\\) positive definite covariance matrix. Then the posterior of \\(\\boldsymbol\\beta\\) is \\(p\\)-variate normal and satisfies: \\[ \\begin{aligned} \\operatorname{Var}\\left[\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2}\\right] &amp;=\\left(\\boldsymbol{\\Sigma}_{0}^{-1}+\\mathbf{X}^{T} \\mathbf{X} / \\sigma^{2}\\right)^{-1} \\\\ \\mathbf{E}\\left[\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2}\\right] &amp;=\\left(\\boldsymbol{\\Sigma}_{0}^{-1}+\\mathbf{X}^{T} \\mathbf{X} / \\sigma^{2}\\right)^{-1}\\left(\\boldsymbol{\\Sigma}_{0}^{-1} \\boldsymbol{\\beta}_{0}+\\mathbf{X}^{T} \\boldsymbol{y} / \\sigma^{2}\\right) \\end{aligned} \\] Variance is a matrix and mean is a vector. Posterior precision = prior precision \\((\\boldsymbol \\Sigma_0^{-1})\\) + sampling precision. Remember that the covariance matrix of \\(\\boldsymbol{\\hat\\beta}_{ols}\\) is \\(\\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\\) so the sampling precision of \\(\\boldsymbol \\beta\\) is \\((1 / \\sigma^2) (\\mathbf{X}^T \\mathbf{X})\\). The posterior expectation (the mean vector) is a weighted average of the prior mean \\(\\boldsymbol\\beta_0\\) and the OLS estimate \\(\\hat{\\boldsymbol\\beta}\\). They are weighted by their precision matrices. Wait a sec. How is \\(\\mathbf{X}^T \\boldsymbol y / \\sigma^2\\) a multiple of \\(\\hat{\\boldsymbol\\beta}?\\) \\(\\hat{\\boldsymbol\\beta}\\) times its precision matrix is \\[ \\begin{aligned} (\\sigma^2(\\mathbf{X}^T \\mathbf{X})^{-1})^{-1} \\hat{\\boldsymbol\\beta} &amp;= (1 / \\sigma^2) \\mathbf{X}^T\\mathbf{X} \\hat{\\boldsymbol\\beta}\\\\ &amp;=(1 / \\sigma^2) (\\mathbf{X}^T\\mathbf{X}) ((\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol y)\\\\ &amp;= 1 / \\sigma^2 (\\mathbf{X}^T \\boldsymbol y) \\end{aligned} \\] The variance argument is even uglier but the result is really sensible! \\[ 1/\\sigma^2 \\sim \\text{gamma}(\\nu_0/2, \\nu_0\\sigma_0^2/2) \\] and \\[ \\{\\sigma^{2}|\\boldsymbol{\\beta},\\mathbf{X},\\boldsymbol{y}\\} \\sim \\text{inverse-gamma} \\left(\\left[\\nu_{0}+n\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\operatorname{SSR}\\left(\\boldsymbol{\\beta}\\right)\\right] / 2\\right) \\] where \\(SSR(\\boldsymbol\\beta) = \\sum{ (y_i - \\boldsymbol\\beta^T \\boldsymbol x_i )^2 }.\\) Posterior sum of squares = prior sum of squares + data sum of squares. The conditional distribution of \\(\\boldsymbol\\beta\\) given \\(\\sigma^2\\) and the data is nice, the conditional distribution of \\(\\sigma^2\\) given \\(\\boldsymbol\\beta\\) and the data is nice so posterior simulation here is gonna be done by Gibbs sampler! Given { \\(\\boldsymbol \\beta^{(s)}, \\sigma^{2(s)}\\) }, we sample new values by: Updating \\(\\boldsymbol{\\beta}\\) : compute \\(\\mathbf{V}=\\operatorname{Var}\\left[\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2(s)}\\right]\\) and \\(\\mathbf{m}=\\mathrm{E}\\left[\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2(s)}\\right]\\) sample \\(\\boldsymbol{\\beta}^{(s+1)} \\sim\\) multivariate \\(\\operatorname{normal}(\\mathbf{m}, \\mathbf{V})\\) Updating \\(\\sigma^{2}\\) : compute \\(\\operatorname{SSR}\\left(\\boldsymbol{\\beta}^{(s+1)}\\right)\\) sample \\(\\sigma^{2(s+1)} \\sim\\) inverse-gamma \\(\\left(\\left[\\nu_{0}+n\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\operatorname{SSR}\\left(\\boldsymbol{\\beta}^{(s+1)}\\right)\\right] / 2\\right)\\). This is fine, but it requires coming up with a sensible prior meaning we need a guess at \\(\\boldsymbol\\beta\\) and a guess at \\(\\sigma^2\\) but not just that but also a measure of our confidence in those guesses. On your homework this week (Exercise 9.1) you are not told what prior to use. Which means you need to specify \\(\\boldsymbol \\beta_0\\)(prior expectation), \\(\\boldsymbol \\Sigma_0\\) (covariance matrix for the \\(\\boldsymbol \\beta\\)-vector), \\(\\nu_0\\) and \\(\\sigma_0^2\\) ( prior for \\(\\sigma^2\\) ). Use this semiconjugate prior. We’re told most response values are between 22 and 24 seconds. So if 95% of observations are in a window of width 2 units that means; \\[ 4SD = 2 \\implies SD=1/2 \\] Take \\(\\sigma_0^2 = 1/4\\) \\(\\nu_0\\) is the prior sample size which the guess of \\(\\sigma_0^2\\) is based on, so take \\(\\nu_0 = 1\\) \\(\\boldsymbol\\beta_0 = (\\beta_{01}, \\beta_{02}).\\) \\(\\beta_{01}\\) is the intercept which is expected time in first week set that to \\(23\\). \\(\\beta_{02}\\) represents the slope which is to say the bi-weekly expected change. I say \\(\\beta_{02} = 0\\) \\(\\boldsymbol \\Sigma_0?\\) set the covariance terms to 0. If the \\(SD\\) is 1/2 maybe be “diffuse” in the prior and set these \\(SD\\)s to double that? That makes \\(\\boldsymbol \\Sigma_0 = \\{\\{1,0\\},\\{0,1\\}\\}.\\) Some regression problems have lots of predictor variables, how could you EVER come up with a sensible prior for such a problem. In a typical multiple regression problem before we’ve analyzed the data we generally have no idea what to expect. So what are things we can do? We’ll discuss two. Unit information prior and Zellner’s \\(g\\)-prior. 15.5 Unit information prior A unit information prior is one that contains the same amount of information as would be contained in a single observation. Take: \\(\\nu_0=1\\), \\(\\sigma_0^2 = \\hat \\sigma^2_{ols}\\), \\({\\boldsymbol\\beta_0} = \\hat {\\boldsymbol\\beta}_{ols}\\), \\(\\boldsymbol\\Sigma_0 = n \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\\) because \\(\\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\\) represents the “sampling variance.” take our prior to be consistent with that but \\(n\\) times as diffuse (spread out over a large area; not concentrated). “Such a distribution cannot be strictly considered a real prior distribution, as it requires knowledge of \\(\\boldsymbol y\\) to be constructed. However, it only uses a small amount of the information in \\(\\boldsymbol y\\) , and can be loosely thought of as the prior distribution of a person with unbiased but weak prior information.” Effectively what the unit information prior does is; prior says exactly what the data says but with only \\(1/n\\)th the certainty. By setting your prior information to be consistent with what the data says and then making it weak, you are protecting yourself from coming up with a prior that changes your conclusions in a way that isn’t justified (doing something foolish). 15.6 Zellner’s \\(g\\)-prior Set \\(\\boldsymbol\\beta_0 = \\boldsymbol0\\). That’s not our prior belief at all! What does this do? Well if \\(\\boldsymbol \\Sigma_0\\) is full of big numbers to the extent that it does influence the posterior it will be in a way of nudging the \\(\\boldsymbol\\beta\\)-values back toward zero. That may not be a thing we want but it is not a terrible outcome either. Set the prior covariance matrix \\(\\boldsymbol \\Sigma_0\\) to \\(g\\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}.\\) Take \\(g\\) big, most commonly \\(g = n\\), and in that case the \\(g\\)-prior is also a version of a unit information prior. \\[ \\text{Var}[\\boldsymbol\\beta | \\sigma^2,\\boldsymbol y, \\mathbf{X}] = g/(g+1) \\times \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1} \\] \\[ E[\\boldsymbol\\beta | \\sigma^2,\\boldsymbol y, \\mathbf{X}] = g/(g+1) \\times (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\boldsymbol y \\] This is the fully conjugate prior where \\(\\sigma^2 \\sim\\) inverse-gamma and \\(\\boldsymbol\\beta | \\sigma^2 \\sim\\) Normal\\(( 0, \\sigma^2 \\Omega )\\) for some positive definite matrix \\(\\Omega.\\) In the \\(g\\)-prior, that matrix \\(\\Omega\\) is \\(g(\\mathbf{X}^T \\mathbf{X})^{-1}.\\) What is \\(g?\\) \\(g &gt; 0\\) (only requirement) but the bigger \\(g\\) is the less influential the prior is. So we’ll see commonly \\(g=n\\). In the semiconjugate prior \\(\\boldsymbol\\beta\\) and \\(\\sigma^2\\) are independent in the prior but not in the posterior so posterior approximation is based on Gibbs sampler. However, in the fully conjugate prior where \\(\\sigma^2 \\sim\\) inverse-gamma and \\(\\boldsymbol\\beta | \\sigma^2 \\sim\\) Normal, the posterior has those forms also! The posterior satisfies \\(\\{\\sigma^2 | \\boldsymbol y, \\mathbf X\\}\\) is inverse-gamma UNCONDITIONALLY on \\(\\boldsymbol\\beta\\) Here’s the recipe for posterior simulation \\((\\)a sample value of \\(\\left(\\boldsymbol{\\beta}, \\sigma^{2}\\right)\\) from \\(p(\\boldsymbol{\\beta}, \\sigma^{2} \\mid \\boldsymbol{y}, \\mathbf{X}))\\) under the \\(g\\)-prior. It’s not a Gibbs sampler it will produce independent draws. sample \\(\\sigma^{2} \\sim \\operatorname{inverse-gamma}\\left(\\left[\\nu_{0}+n\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\mathrm{SSR}_{g}\\right] / 2\\right)\\); sample \\(\\boldsymbol{\\beta} \\sim\\) Normal \\(_{p}\\left[\\frac{g}{g+1} \\widehat{\\boldsymbol{\\beta}}_{\\text {ols }}, \\frac{g}{g+1} \\sigma^{2}\\left(\\mathbf{X}^{T} \\mathbf{X}\\right)^{-1}\\right].\\) where \\[ \\operatorname{SSR}_{g}^{z}=\\boldsymbol{y}^{T}\\left(\\mathbf{I}-\\frac{g}{g+1} \\mathbf{X}_{z}\\left(\\mathbf{X}_{z}^{T} \\mathbf{X}_{z}\\right)^{-1} \\mathbf{X}_{z}^{T}\\right) \\boldsymbol{y} \\] 15.7 Bayesian analysis using invariant \\(g\\)-prior Let’s do the \\(g\\)-prior with “unit information” logic. \\(g=n=12\\), \\(\\nu_0=1\\), \\(\\sigma_0^2=\\sigma^2_{ols}\\) S &lt;- 5000 g &lt;- n; nu.0 &lt;- 1; sigma2.0 &lt;- sigma2.hat; H.g &lt;- g/(g+1) * X %*% solve( t(X) %*% X ) %*% t(X) SSR.g &lt;- t(y) %*% ( diag(n) - H.g ) %*% y sigma2.sim &lt;- 1 / rgamma(S, (nu.0+n)/2, (nu.0*sigma2.0 + SSR.g)/2) V.beta &lt;- g/(g+1) * solve( t(X) %*% X ) m.beta &lt;- as.vector( V.beta %*% t(X) %*% y ) beta.sim &lt;- matrix(NA, S, p) for(s in 1:S){ beta.sim[s,] &lt;- rmvnorm(1, mean=m.beta, sigma=V.beta*sigma2.sim[s])[1,] } As a result the posterior expected value of \\(\\boldsymbol \\beta\\) is \\[ E(\\boldsymbol \\beta|\\boldsymbol \\beta y, \\mathbf{X}, \\sigma^2) = g/(g+1)\\times \\boldsymbol{\\hat\\beta}_{ols}=\\boldsymbol{\\hat\\beta}_{Bayes} \\] round(g/(g+1) * beta.hat.ols, 2) ## [1] -47.35 12.10 1.93 -0.29 So we have: \\[ E(Y)=-47.35+12.10\\texttt{program}+1.93\\texttt{age}-0.29\\texttt{program:age} \\] The OLS result was \\[ E(Y) = -51.29 +13.11\\texttt{program}+2.09\\texttt{age}+-0.32\\texttt{program:age} \\] # Reproduce Figure 9.3 on page 160 of Hoff (2009) par(mfrow=c(1,3)) plot(beta.sim[,c(2,4)], xlab=expression(beta[2]), ylab=expression(beta[4]), cex=.5, pch=19) abline(h=0); abline(v=0) hist(beta.sim[,2], freq=F, right=F, col=&quot;pink&quot;, breaks=40, xlab=expression(beta[2]), main=&quot;&quot;); lines(density(beta.sim[,2]), lwd=2); abline(v=0, lty=2, lwd=2) hist(beta.sim[,4], freq=F, right=F, col=&quot;pink&quot;, breaks=40, xlab=expression(beta[4]), main=&quot;&quot;); lines(density(beta.sim[,4]), lwd=2); abline(v=0, lty=2, lwd=2) Both posteriors cover zero. My results don’t look exactly like the book for this example. Specifically, I’m finding much higher posterior correlation between \\(\\beta_2\\) and \\(\\beta_4\\) than the book reported. cor(beta.sim)[2,4] ## [1] -0.9931 Why the interest in \\(\\beta_2\\) and \\(\\beta_4?\\) Recall \\[ \\begin{array}{l} \\text{running}=\\mathrm{E}[Y \\mid \\boldsymbol{x}]=\\beta_{1}+\\beta_{3}\\texttt{age }\\\\ \\text{aerobics}=\\mathrm{E}[Y \\mid \\boldsymbol{x}]=\\left(\\beta_{1}+\\beta_{2}\\right)+\\left(\\beta_{3}+\\beta_{4}\\right)\\texttt{age } \\end{array} \\] Expected response to aerobics at age \\(x =\\) \\(\\beta_{1}+\\beta_{3}\\texttt{age}+\\beta_2 + \\beta_4 \\texttt{age}\\). So if we assumed that \\(\\beta_2= \\beta_4= 0\\), then we would have an identical line for both groups. The quantity that is of primary interest is not \\(\\beta_2\\) nor is it \\(\\beta_4\\). As justified above it’s \\(\\beta_2 + \\beta_4 x\\). So let’s do posterior inference about \\(\\beta_2 + \\beta_4 x\\) for \\(x = 20, 21, …, 31\\) # Compute posterior distributions of beta2 + beta4*x for x-values over the range of ages in the study xvals &lt;- 20:31 # range(age) Effect.post &lt;- beta.sim[,c(2,4)] %*% rbind(rep(1,length(xvals)),xvals) probs &lt;- c(.025, .25, .5, .75, .975) Effect.quants &lt;- apply(Effect.post, 2, quantile, prob=probs) colnames(Effect.quants) &lt;- xvals boxplot(Effect.quants, ylim=c(-10, 15), col=&quot;pink&quot;, xlab=&quot;age&quot;, ylab=expression(beta[2]+beta[4]*&quot;age&quot;)) abline(h=0) The data seem to suggest that aerobics is more effective although the difference reduces with age. The estimated effect (difference between aerobics and running) is strongest at age 20 but the evidence for an effect is strongest at age 23 or 24. Why would that be? It’s not sample sizes, it’s a leverage thing. Estimation of \\(\\beta_2 + \\beta_4x\\) is most precise for intermediate values of \\(x\\) and most variable at the endpoints of the range of \\(x\\)-values. 15.8 Bayesian analysis using semiconjugate prior Here we use a version of the ‘unit information prior’ idea. The “unit information prior” is a prior distribution that is perfectly consistent with the data. It’s not a true prior. \\(\\nu_0 = 1\\) \\(\\sigma_0^2 = \\hat\\sigma^2_{ols}\\) \\(\\beta_0 = \\hat\\beta_{ols}\\) \\(\\boldsymbol \\Sigma_0 = \\hat\\sigma^2_{ols}(\\mathbf{X}^T\\mathbf{X})^{-1}\\times n\\) (times by \\(n\\) or a big number) beta.0 &lt;- beta.hat.ols; Sigma.0 &lt;- n * sigma2.hat * solve(t(X) %*% X) nu.0 &lt;- 1 sigma2.0 &lt;- sigma2.hat S &lt;- 5000 Sigma0.Inv &lt;- solve(Sigma.0) # Invert once, not every time beta.chain &lt;- matrix(NA, S, p) sigma2.chain &lt;- rep(NA, S) # Starting values beta &lt;- beta.0 sigma2 &lt;- sigma2.0 for(s in 1:S) { # Update beta first V.beta &lt;- solve( Sigma0.Inv + t(X) %*% X / sigma2 ) m.beta &lt;- V.beta %*% (Sigma0.Inv %*% beta.0 + t(X) %*% y / sigma2) beta &lt;- rmvnorm(1, mean=m.beta, sigma=V.beta)[1,] # Now update sigma2 SSR &lt;- sum( (y - X%*%beta)^2 ) sigma2 &lt;- 1 / rgamma(1, (nu.0 + n)/2, (nu.0*sigma2.0 + SSR)/2) # Now save updated values beta.chain[s,] &lt;- beta sigma2.chain[s] &lt;- sigma2 } rbind(apply(beta.chain, 2, mean), # beta.hat = E[beta|y] beta.hat.ols) # compare with ols estimate ## [,1] [,2] [,3] [,4] ## -51.43 13.34 2.101 -0.3276 ## beta.hat.ols -51.29 13.11 2.095 -0.3182 rbind(apply(beta.chain, 2, sd), # posterior standard deviation SE) # compare with standard error of ols ## [,1] [,2] [,3] [,4] ## 12.73 16.09 0.5481 0.6660 ## SE 12.25 15.76 0.5264 0.6498 c(mean(sigma2.chain), sigma2.hat) ## [1] 10.112 8.542 median(sigma2.chain) ## [1] 8.766 The median of \\(\\texttt{sigma2.chain}\\) is closer to the ols estimate of \\(\\sigma^2\\) 15.8.1 Prediction problem Consider two 30-year-old men, one undertakes a running program and the other undertakes a step aerobics program x.run &lt;- c(1, 0, 30, 0) # beta vector for new observation x.step &lt;- c(1, 1, 30, 30) # beta vector ytilde.run &lt;- rnorm(S, mean=as.vector(beta.chain %*% x.run), sd=sqrt(sigma2.chain)) ytilde.step &lt;- rnorm(S, mean=as.vector(beta.chain %*% x.step), sd=sqrt(sigma2.chain)) Plot the posterior predictive distributions for their change in maximum oxygen uptake par(mfrow=c(1,2)) hist(ytilde.run, freq=F, right=F, breaks=50, col=&quot;pink&quot;, xlab=&quot;Change in O2 uptake after running program&quot;, main=&quot;&quot;) lines(density(ytilde.run), lwd=2) hist(ytilde.step, freq=F, right=F, breaks=50, col=&quot;pink&quot;, xlab=&quot;Change in O2 uptake after aerobics&quot;, main=&quot;&quot;) lines(density(ytilde.step), lwd=2) The runner’s posterior predictive distribution is centered around 10 or 12 so will probably be positive. The posterior predictive distribution looks a little higher for the aerobics guy What is the posterior probability that the man who does step aerobics achieves a better result? mean(ytilde.step &gt; ytilde.run) ## [1] 0.7154 Estimate about a 72% chance the man doing aerobics gets a better result than the man who does running "],["model-selection.html", "Lecture 16 Model Selection 16.1 Review 16.2 Bayesian model comparison 16.3 Example: Oxygen uptake 16.4 Gibbs sampling and model averaging", " Lecture 16 Model Selection The following notes, mostly transcribed from Neath(0601,2021) lecture, summarize section (9.3) of Hoff(2009). 16.1 Review 12 units of healthy male age 20 to 31 undertake a 12-week exercise program either running or step aerobics. Response is change in maximum oxygen uptake. Predictor variables are age in years and program (either running or aerobics). So there’s a grouping variable (two groups) and there’s a quantitative predictor variable (age). \\[ \\begin{array}{l} \\int yp(y|\\boldsymbol x)dy = E(Y|\\boldsymbol x) = \\beta_1 +\\beta_2\\texttt{program}+\\beta_3\\texttt{age}+\\beta_4\\texttt{program:age} = \\boldsymbol \\beta^T\\boldsymbol x\\\\ \\text{running}=\\mathrm{E}[Y \\mid \\boldsymbol{x}] = \\beta_{1}+\\beta_{3}\\texttt{age } \\\\ \\text{aerobics}=\\mathrm{E}[Y \\mid \\boldsymbol{x}] = \\left(\\beta_{1} + \\beta_{2}\\right) + \\left(\\beta_{3} + \\beta_{4}\\right)\\texttt{age } \\end{array} \\] Expected response is linear in age with the possibility of different lines for running versus aerobics hence we have 5 model parameters; 2 slopes, 2 intercepts and residual variance. A semiconjugate prior is the \\(p\\)-variate normal for the \\(\\boldsymbol \\beta\\) vector (regression coefficients) and inverse gamma for the variance. In that case full conditionals are; \\(p\\)-variate normal for \\(\\boldsymbol \\beta\\) it’s inverse-gamma for \\(\\sigma^2.\\) Great, we can do a Gibbs sampler. Updating \\(\\boldsymbol{\\beta}\\) : compute \\(\\mathbf{V}=\\operatorname{Var}\\left[\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2(s)}\\right]\\) and \\(\\mathbf{m}=\\mathrm{E}\\left[\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2(s)}\\right]\\) sample \\(\\boldsymbol{\\beta}^{(s+1)} \\sim\\) multivariate \\(\\operatorname{normal}(\\mathbf{m}, \\mathbf{V})\\) At each update compute the covariance matrix and the mean vector for the full conditional of \\(\\boldsymbol \\beta\\) ( that depends on the current state of \\(\\sigma^2\\) ) Updating \\(\\sigma^{2}\\) : compute \\(\\operatorname{SSR}\\left(\\boldsymbol{\\beta}^{(s+1)}\\right)\\) sample \\(\\sigma^{2(s+1)} \\sim\\) inverse-gamma \\(\\left(\\left[\\nu_{0}+n\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\operatorname{SSR}\\left(\\boldsymbol{\\beta}^{(s+1)}\\right)\\right] / 2\\right)\\). The degrees of freedom parameter increases \\(\\nu_0\\) in the prior to \\(\\nu_0 + n\\) in the posterior. The sum of squares parameter goes from \\(\\nu_0 \\sigma_0^2\\) in the prior to \\(\\nu_0\\sigma_0^2 + SSR\\) in the posterior. \\(SSR= \\sum(y_i-\\boldsymbol \\beta^T \\boldsymbol x_i)^2\\) is the sum of the squared residuals. Priors are hard to come by in regression problems that’s why default things are useful. The ‘unit information prior’ (perfectly consistent with the data) is a good default prior for Bayesian linear regression and so is Zellner’s \\(g\\)-prior. And that’s what we’re gonna be using in the rest of today’s class. The \\(g\\)-prior is motivated by a desired invariance property which requires that \\(\\boldsymbol\\beta_0 = \\boldsymbol0\\) and \\(\\boldsymbol \\Sigma_0 = g\\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}.\\) 16.2 Bayesian model comparison In your regression class you considered regression problems with lots of potential predictor variables, and the question of interest is: Which are important predictors and which are not? We have \\(p\\) potential predictors and the model says \\(y_i = \\beta_1x_{i,1} + \\beta_2x_{i,2} + … + \\beta_px_{i,p} + \\epsilon_i.\\) Some of the \\(\\beta\\)’s are zero and some are not. We write \\(\\beta_j = z_j b_j\\) where \\(z_j \\in \\{0,1\\}\\). This is strategic because a model is defined by its associated \\(z\\)-vector. Each such \\(z\\)-vector identifies a possible model. By “model” here we mean a collection of variables that are included (where others are excluded). For example in the oxygen data; \\(E[Y|\\boldsymbol{x,b,z}=(1,1,1,1)]=b_1x_1+b_2x_2+b_3x_3+b_4x_4=b_1+b_2\\texttt{program}+b_3\\texttt{age}+b_2\\texttt{program:age}\\) is the full model \\(E[Y|\\boldsymbol{x,b,z}=(1,0,1,0)]=b_1x_1+b_3x_3=b_1+b_3\\texttt{age}\\) is one intercept one slope = no group difference at all \\(E[Y|\\boldsymbol{x,b,z}=(1,1,0,0)]=b_1x_1+b_2x_2=b_1+b_2\\texttt{program}\\) means that there’s a group effect but there’s no age effect \\(E[Y|\\boldsymbol{x,b,z}=(1,1,1,0)]=b_1x_1+b_2x_2+b_3x_3=b_1+b_2\\texttt{program}+b_3\\texttt{age}\\) means separate intercepts but a common slope (parallel mean). Age effect is the same for both groups, and the group difference is the same at all ages. If there are \\(p\\) predictor variables there are \\(2^p\\) such regressions models. They may not all be relevant. For example in this model it doesn’t make sense to have a zero in the first position. Bayesian model selection works by; assign a prior distribution to all possible \\(z\\)-vectors of length \\(p\\), compute their posterior probabilities and make conclusions. posterior probability of a particular model: \\[ p(\\boldsymbol z | \\boldsymbol y,\\mathbf{X}) = p(\\boldsymbol z)p(\\boldsymbol y | \\boldsymbol z,\\mathbf{X}) / p(\\boldsymbol y)=c \\times p(\\boldsymbol z)p(\\boldsymbol y | \\boldsymbol z,\\mathbf{X}) \\] The thing we need to be able to compute is the likelihood, \\(p(\\boldsymbol y | \\mathbf{X},\\boldsymbol z).\\) We know the likelihood is dependent on \\(\\boldsymbol\\beta\\) and \\(\\sigma^2\\)! But what we need is priors on \\(\\boldsymbol\\beta\\) and \\(\\sigma^2\\) then integrate \\(\\boldsymbol\\beta\\) and \\(\\sigma^2\\) out to get \\(p(\\boldsymbol y | \\mathbf{X}, z).\\) \\[ \\begin{aligned} p(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{z}) &amp;=\\iint p\\left(\\boldsymbol{y}, \\boldsymbol{\\beta}, \\sigma^{2} \\mid \\mathbf{X}, \\boldsymbol{z}\\right)\\, d \\boldsymbol{\\beta} d \\sigma^{2} \\\\ &amp;=\\iint p\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\beta}, \\mathbf{X}, \\boldsymbol{z}, \\sigma^{2}\\right) p\\left(\\boldsymbol{\\beta} \\mid \\mathbf{X}, \\boldsymbol{z}, \\sigma^{2}\\right) p(\\sigma^{2}|\\boldsymbol z)\\, d \\boldsymbol{\\beta} d \\sigma^{2} \\end{aligned} \\] Recap: each \\(p\\)-vector of zeros and ones which we denote with \\(\\boldsymbol z\\) represents a possible model (set of active and inactive predictor variables). We assign prior probabilities to those models probably in some kind of uniform way. To get the posterior probabilities we need the marginal ‘probabilty’ of the data \\(p(\\boldsymbol y | \\mathbf{X},\\boldsymbol z).\\) It’s marginal in the sense that it’s unconditional on \\(\\boldsymbol\\beta\\) and \\(\\sigma^2\\). Notation: \\(\\mathbf{X}_z\\) is the \\(\\mathbf{X}\\)-matrix but only the columns corresponding to \\(z_j = 1.\\) \\(\\mathbf{X}\\) has \\(n\\) rows and \\(p\\) columns. so \\(\\mathbf{X}_z\\) has \\(n\\) rows and \\(p_z\\) columns ( as many columns as there are \\(z_j = 1\\) ). We’re using the fully conjugate prior. Priors on \\(\\boldsymbol \\beta\\) and \\(\\sigma^2\\) are conditional on \\(\\boldsymbol z\\). They have to be because for the \\(j\\)’s with \\(z_j = 0\\), \\(\\beta_j = 0.\\) \\[ \\begin{array}{r} \\boldsymbol{\\beta}_{z} \\mid \\mathbf{X}_{z}, \\sigma^{2} \\sim \\operatorname{Normal}_{p_{z}}\\left(\\mathbf{0}, g \\sigma^{2}\\left[\\mathbf{X}_{z}^{T} \\mathbf{X}_{z}\\right]^{-1}\\right) \\\\ \\text { Also, } \\sigma^{2} \\sim \\text { InverseGamma}\\left(\\nu_{0} / 2, \\nu_{0} \\sigma_{0}^{2} / 2\\right) \\end{array} \\] On page 165 of Hoff \\((2009)\\), it is shown that \\[ p(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{z}) =\\pi^{-n / 2} \\frac{\\Gamma\\left(\\left[\\nu_{0}+n\\right] / 2\\right)}{\\Gamma\\left(\\nu_{0} / 2\\right)}(1+g)^{-p_{z} / 2} \\frac{\\left(\\nu_{0} \\sigma_{0}^{2}\\right)^{\\nu_{0} / 2}}{\\left(\\nu_{0} \\sigma_{0}^{2}+\\mathrm{SSR}_{g}^{z}\\right)^{\\left(\\nu_{0}+n\\right) / 2}} \\] where \\[ \\operatorname{SSR}_{g}^{z}=\\boldsymbol{y}^{T}\\left(\\mathbf{I}-\\frac{g}{g+1} \\mathbf{X}_{z}\\left(\\mathbf{X}_{z}^{T} \\mathbf{X}_{z}\\right)^{-1} \\mathbf{X}_{z}^{T}\\right) \\boldsymbol{y} \\] We will set \\(g = n\\) and \\(\\nu_0 = 1.\\) \\(\\sigma_0^2\\) will depend on \\(\\boldsymbol z\\)! It will be the least squares estimate of \\(\\sigma^2.\\) Under model \\(\\boldsymbol z\\) the notation for this quantity \\(s^2_{z}.\\) The ratio of marginal probabilities for two competing models is called the “Bayes factor” \\[ \\frac{p\\left(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{z}_{a}\\right)}{p\\left(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{z}_{b}\\right)}=(1+n)^{\\left(p_{z_{a}}-p_{z_{b}}\\right) / 2}\\left(\\frac{s_{z_{a}}^{2}}{s_{z_{b}}^{2}}\\right)^{1 / 2}\\left(\\frac{s_{z_{b}}^{2}+\\mathrm{SSR}_{g}^{z_{b}}}{s_{z_{a}}^{2}+\\operatorname{SSR}_{g}^{z_{a}}}\\right)^{(n+1) / 2} \\] The Bayes factor can be interpreted as how much the data favor model \\(\\boldsymbol z_a\\) over model \\(\\boldsymbol z_b\\). Notice that it is essentially a balance between model complexity (number of parameters) and goodness of ﬁt (SSR). Bayes factor = 1 means data are equally likely under either model. Bayes factor &gt; 1 means observed data are more likely under model \\(a\\) than model \\(b\\). 16.3 Example: Oxygen uptake We have \\(p=4\\) which mean there are \\(2^p = 2^4 = 16\\) possible linear regression models. They aren’t all of interest. Here’s five that are: \\[ \\begin{array}{ll} \\boldsymbol z &amp; {\\text{ Model }} \\\\ \\hline(1,0,0,0) &amp; \\beta_{1} \\\\ (1,1,0,0) &amp; \\beta_{1}+\\beta_{2} \\times \\texttt{group}_{i} \\\\ (1,0,1,0) &amp; \\beta_{1}+\\beta_{3} \\times \\texttt{age}_{i} \\\\ (1,1,1,0) &amp; \\beta_{1}+\\beta_{2} \\times \\texttt{group}_{i}+\\beta_{3} \\times \\texttt{age}_{i} \\\\ (1,1,1,1) &amp; \\beta_{1}+\\beta_{2} \\times \\texttt{group}_{i}+\\beta_{3} \\times \\texttt{age}_{i}+\\beta_{4} \\times \\texttt{group}_{i} \\times \\texttt{age}_{i} \\end{array} \\] We will assign equal prior probabilities of one-fifth to each of these models. Let’s compute \\(\\log p(\\boldsymbol y | \\mathbf{X},\\boldsymbol z)\\) for each of the five models. For all these Bayesian regression R programs we need \\(\\boldsymbol y\\) ( \\(n\\)-vector of responses ) and \\(\\mathbf{X}\\)( \\(n \\times p\\) matrix of regressors ) # Here &#39;program&#39; is the indicator of exercise program (0 for # running and 1 for step aerobics) program &lt;- c( 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1) age &lt;- c(23,22,22,25,27,20,31,23,27,28,22,24) y &lt;- c( -0.87,-10.74, -3.27, -1.97, 7.50, -7.25, 17.05, 4.96, 10.40, 11.05, 0.26, 2.51) # OLS estimation n &lt;- length(y) X &lt;- cbind(rep(1,n), program, age, program*age) p &lt;- dim(X)[2] rownames(X) &lt;- 1:n colnames(X) &lt;- paste(&quot;x&quot;, 1:p, sep=&quot;&quot;) beta.hat.ols &lt;- as.vector(solve( t(X) %*% X ) %*% t(X) %*% y) sigma2.hat &lt;- sum( (y - X %*% beta.hat.ols)^2 ) / (n-p) # Compute log.p(y|X,z) for the five models in consideration # Formula given at bottom of page 165 lpy.X &lt;- function(y, X, g=length(y), nu.0=1, sigma2.0= try(summary(lm(y ~ 0 + X))$sigma^2, silent=T)) { n &lt;- dim(X)[1] p &lt;- dim(X)[2]; if(p==0){ H.g &lt;- 0; sigma2.0 &lt;- mean(y^2) } if(p &gt; 0){ H.g &lt;- (g/(g+1)) * X %*% solve(t(X)%*%X) %*% t(X)} SSR.g &lt;- t(y) %*% (diag(n) - H.g) %*% y ### -0.5 * ( n*log(pi) + p*log(1+g) + (nu.0+n) * log(nu.0*sigma2.0 + SSR.g) - nu.0*log(nu.0*sigma2.0) ) + lgamma((nu.0+n)/2) - lgamma(nu.0/2) } # indicators given as five rows of matrix z z &lt;- matrix(c( 1,0,0,0, 1,1,0,0, 1,0,1,0, 1,1,1,0, 1,1,1,1), byrow=T, 5, 4) colnames(z) &lt;- paste(&quot;z&quot;,1:4, sep=&quot;&quot;) rownames(z) &lt;- 1:5; log.p &lt;- rep(NA, 5) for(k in 1:5) { log.p[k] &lt;- lpy.X(y=y, X=X[,z[k,]==1,drop=F]) } cbind(z, log.p) ## z1 z2 z3 z4 log.p ## 1 1 0 0 0 -44.33 ## 2 1 1 0 0 -42.35 ## 3 1 0 1 0 -37.66 ## 4 1 1 1 0 -36.42 ## 5 1 1 1 1 -37.60 \\(\\texttt{log.p[k]}= \\log p(\\boldsymbol y|\\mathbf X,\\boldsymbol z)=\\) log of marginal probability of data given model \\(k\\) \\((k = 1, 2, 3, 4, 5)\\). These are log-probability-densities, they are not probabilities so they don’t add to anything meaningful. But we can compare. The data are most likely under model 4, than under models 3 and 5. The data are not very likely under models 1 and 2. Assuming equal prior probabilities on these 5 models we can compute the posterior probabilities. \\[ p(\\boldsymbol{z} \\mid \\boldsymbol{y}, \\mathbf{X})=\\frac{p(\\boldsymbol{z}) p(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{z})}{\\sum_{\\tilde{\\boldsymbol{z}}} p(\\tilde{\\boldsymbol{z}}) p(\\boldsymbol{y} \\mid \\mathbf{X}, \\tilde{\\boldsymbol{z}})} \\] Assuming uniform prior probabilities on the five candidate models \\(p(z)\\) are all equal. \\(p(\\boldsymbol y | \\mathbf{X},\\boldsymbol z) = \\texttt{exp(log.p)}\\). prob.z &lt;- exp(log.p) / sum(exp(log.p)) round(cbind(z, log.p, prob.z), 2) ## z1 z2 z3 z4 log.p prob.z ## 1 1 0 0 0 -44.33 0.00 ## 2 1 1 0 0 -42.35 0.00 ## 3 1 0 1 0 -37.66 0.18 ## 4 1 1 1 0 -36.42 0.63 ## 5 1 1 1 1 -37.60 0.19 Posterior probability of model 4 is 0.63, posterior probability of model 5 is 0.19, posterior probability of model 1 or model 2 is basically zero. The data are consistent with an age effect, as the posterior probabilities of the three models that include age essentially sum to 1 (so the data is practically impossible without an age effect). The data is also indicative of a group effect(though weaker than age effect), as the combined probability for the three models with a group effect is 0.00+0.63+0.19=0.82. 16.4 Gibbs sampling and model averaging what if \\(p\\) is big and \\(2^p\\) is really big and there are LOTS of models under consideration? Remember the solution to this issue in ordinary regression? The solution would be to use stepwise search methods such as forward selection / backward elimination. That’s not what we will do here. What we will do in Bayesian model selection is run a Gibbs sampler on the posterior probability distribution \\(p(\\boldsymbol{z} \\mid \\boldsymbol{y}, \\mathbf{X}).\\) Gibbs sampler proceeds by; given \\(\\boldsymbol z^{(s)}\\) generate \\(\\boldsymbol z^{(s+1)}.\\) Do this a whole bunch of times and the observed proportions of particular \\(z\\)-values approximate the posterior probability of the corresponding model. More precisely, generating values of \\(\\left\\{\\boldsymbol{z}^{(s+1)}, \\sigma^{2(s+1)}, \\boldsymbol{\\beta}^{(s+1)}\\right\\}\\) from \\(\\boldsymbol{z}^{(s)}\\) is achieved with the following steps. Given current state \\(\\boldsymbol{z}=\\boldsymbol{z}^{(s)}\\) generate new state as follows For \\(j \\in\\{1,2, \\ldots, p\\}\\) in RANDOM ORDER, replace \\(z_{j}\\) with a sample from \\(p\\left(z_{j} \\mid \\boldsymbol{y}, \\mathbf{X}, \\boldsymbol{z}_{-j}\\right)\\) set \\(z^{(s+1)}=\\boldsymbol{z}\\) Sample \\(\\sigma^{2(s+1)} \\sim p\\left(\\sigma^{2} \\mid \\boldsymbol{y}, \\mathbf{X}, \\boldsymbol{z}^{(s+1)}\\right)\\), and sample \\(\\boldsymbol{\\beta} \\sim p\\left(\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\boldsymbol{z}^{(s+1)}, \\sigma^{2(s+1)}\\right)\\) Of course we don’t just want to make inference about the model (if we were only interested in the active variables we would just do steps 1 to 3), we also want to estimate the model (steps 4 and 5). R code to sample \\(\\boldsymbol z^{(s+1)}\\) given \\(\\boldsymbol z^{(s)}\\). # code for estimating the active variables [steps 1 to 3] z &lt;- rep(1, dim(X)[2]) lpy.c &lt;- lpy.X(y=y, X=X[,z==1, drop=F]) S &lt;- 10000 Z &lt;- matrix(NA, S, dim(S)[2]) for(s in 1:S){ for(j in sample(1:dim(X)[2])){ zp &lt;- z; zp[j] &lt;- 1 - zp[j] lpy.p &lt;- lpy.X(y=y, X=X[, zp==1, drop=F]) r &lt;- (lpy.p - lpy.c)*(-1)^(zp[j]==0) # r &lt;- r + log(prior.z[j]) - log(1-prior.z[j]) z[j] &lt;- rbinom(1, 1, 1/(1+exp(-r))) if(z[j]==zp[j]) { lpy.c &lt;- lpy.p } } Z[s,] &lt;- z } You’re gonna do a problem using this code for next week’s homework. That’s all for chapter 9. "],["generalized-linear-models-the-metropolis-algorithm.html", "Lecture 17 Generalized Linear Models; the Metropolis Algorithm 17.1 Example: Song sparrow reproductive success 17.2 Poisson regression 17.3 Logistic regression 17.4 Posterior approximations 17.5 The Metropolis algorithm 17.6 Example: Normal distribution with known variance 17.7 The Metropolis algorithm for Poisson regression {sec:metpois}", " Lecture 17 Generalized Linear Models; the Metropolis Algorithm The following notes, mostly transcribed from Neath(0602,2021) lecture, summarize sections (10.1-10.3) of Hoff(2009). 17.1 Example: Song sparrow reproductive success A sample from a population of 52 female song sparrows was studied over the course of a summer and their reproductive activities were recorded. In particular, the age and number of new oﬀspring were recorded for each sparrow. This is a regression problem with just one predictor variable; age in years of female song sparrow. Response variable is \\(y =\\) number of offspring. Possible values of \\(y\\) are 0, 1, 2, 3, …, realized values in our data are 0, 1, 2, …, 7 # 52 birds, response is &#39;fledged&#39; = number of offspring # Predictor variable is age in years (1 to 6) fledged &lt;- c( 3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, 2, 2, 1, 1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, 1, 2, 1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2) age &lt;- c(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1) table(age); table(fledged) ## age ## 1 2 3 4 5 6 ## 10 9 9 16 7 1 ## fledged ## 0 1 2 3 4 5 6 7 ## 7 9 18 6 4 3 4 1 When a predictor variable is discrete like this it’s sometimes better to look at boxplots rather than a scatterplot par(mar=c(3,3,1.5,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0),mfrow=c(1,2)) boxplot(fledged ~ as.factor(age), range=0, col=&quot;pink&quot;, xlab=&quot;age&quot;, ylab=&quot;offspring&quot;) plot(fledged ~ age, cex=0.7, ylab=&quot;&quot;) Figure 17.1: Graphical summary of data: Boxplots by age What does the relationship between \\(y\\) and \\(x\\) look like? It goes from 2 down to 6 so it’s definitely a negative relationship. However, from 1 to 2 it increases and this makes sense in terms of the subject matter. What kind of model can capture this increasing then decreasing shape? Not a linear model! We’re gonna fit a quadratic model where \\(E(Y | X=x) = \\beta_1 + \\beta_2x + \\beta_3x^2.\\) What are some potential problems with this model? Stated differently, if we fit this model to these data what is likely to be the case as far as estimated value of \\(E(Y | X=7)?\\) It is likely to be negative. There’s a natural work-around for this problem. (I’m never thrilled with just saying oh there’s curvature let’s do quadratic instead of linear but I don’t have a better idea.) If the response is binary we want to model \\(E(Y ) = Pr(Y=1)\\) and we want to ensure that fitted probabilities are between 0 and 1, the most popular way of guaranteeing this is logistic regression. You don’t say \\(E(Y) = \\beta x\\) but rather \\(\\text{logit}[ E(Y) ] = \\beta x,\\) where \\(\\text{logit}( p ) = \\log ( p / (1-p) ).\\) So you’re modelling the log odds as being a linear function. In our problem we don’t have the same issue because we are not modeling a value that is between zero and one but we are trying to fit a value that only takes positive values. The “generalized linear model” approach is; Instead of saying \\(E(Y | X=x) = \\beta_1 + \\beta_2x + \\beta_3x^2,\\) we’ll say \\(g( E[Y|x] ) = \\beta_1 + \\beta_2x + \\beta_3x^2.\\) \\(g\\) is called the link function. In this problem where the response is a count variable and natural to model as following a Poisson distribution, a natural link function to use is the log because \\(E(Y | x)\\) has to be \\(&gt; 0\\) but \\(\\log( E[Y|x] )\\) can be anything. The log function \\(\\log(x)\\) maps the domain \\((0,\\infty)\\) to \\((-\\infty,\\infty).\\) Expected offspring given age \\(=x= E(Y | x) =\\theta_x\\) for \\(x=\\) { 1, 2, 3, … ,6 }. We could model the log-mean of \\(Y\\) in terms of this regression: \\[\\log E(Y|x)=\\log (\\theta_x) = \\beta_1 + \\beta_2x + \\beta_3x^2\\] which means that \\[ E(Y|x)=\\exp\\{\\beta_1+\\beta_2x+\\beta_3x^2\\} &gt;0 \\] 17.2 Poisson regression The resulting model, \\[ Y|\\boldsymbol x \\sim \\text{Poisson}(e^{\\boldsymbol\\beta^T \\boldsymbol x}) \\] In the GLM terminology this model uses the log link function. Lots of people want to say this model has an exponential link function, but no. The convention is \\(E(Y|x) = h(\\boldsymbol\\beta^T \\boldsymbol x ).\\) \\(h\\) is the inverse link the inverse of \\(h\\) is \\(g\\) satisfies \\(g( E[y|x] ) = \\boldsymbol\\beta^T \\boldsymbol x.\\) \\(g\\) is called the link function. The Poisson regression model is a type of generalized linear model, a model which relates a function of the expectation to a linear predictor of the form \\(\\boldsymbol \\beta^T\\boldsymbol x\\). 17.3 Logistic regression If \\(Y\\) is a binary variable, \\(E(Y |\\boldsymbol x) = Pr(Y=1|\\boldsymbol x) = \\theta_x\\) must satisfy \\(0 &lt; \\theta_x &lt; 1.\\) So we can’t say \\(\\theta_x = \\boldsymbol \\beta^T\\boldsymbol x.\\) If we do this we’re likely to get estimated probabilities less than 0 and/or greater than 1. Instead we’ll reparameterize as \\[ \\log\\left( \\frac{\\theta_x}{1-\\theta_x} \\right) = \\text{logit}(\\theta_x) = \\boldsymbol\\beta^T \\boldsymbol x \\] so the function \\(g(u) = \\log( u/(1-u) )\\) relating the mean to the linear predictor is called the logit function. So the “logistic regression model” is a binary regression model with the logit link. If you do a little bit of algebra, you can see the inverse of the logit function; If \\(\\boldsymbol\\beta^T \\boldsymbol x = \\log( \\theta_x/(1-\\theta_x) )\\) then \\[ \\theta_x = \\frac{1}{( 1 + \\exp(-\\boldsymbol\\beta^T\\boldsymbol x) )} = \\frac{\\exp(\\boldsymbol\\beta^T\\boldsymbol x)}{1+\\exp(\\boldsymbol\\beta^T\\boldsymbol x)} \\quad \\theta_x \\in \\{0,1\\} \\] Student question: I still don’t understand where Poisson comes in? Ans: Because the response is a count variable. There’s two different issues. To model the sparrows reproductive success data, we need (1) a relationship between \\(E(Y | x)\\) and \\(x\\) and that’s where \\(\\log( E[Y|x] ) = \\log(\\theta_x) = \\beta_1 + \\beta_2x + \\beta_3 x^2\\) came in.(2) we also need a probability model! as that’s just the expected value. We’re gonna use Bayes rule to estimate \\(\\boldsymbol \\beta = (\\beta_1, \\beta_2, \\beta_3),\\) so we need a prior distribution \\(p(\\boldsymbol \\beta)\\) and we need a sampling distribution \\(p(\\boldsymbol y | \\boldsymbol \\beta, \\boldsymbol x)\\) and that’s where the Poisson distribution comes in. Are all GLMs with log link Poisson regression? NO. Does Poisson regression always use the log link? Not by definition but in practical terms, YES, pretty much always. 17.4 Posterior approximations The Poisson regression model and logistic regression model are actually simpler than ordinary linear regression in one respect: There is no need for a variance parameter. If I tell you that \\(Y |x \\sim\\) Poisson\\((e^{\\boldsymbol\\beta^T \\boldsymbol x}).\\) I’ve specified the variance also since \\(E(Y|x)=\\text{Var}(Y|x)\\). On the other hand, if I tell you that \\(Y | x \\sim\\) Normal with mean \\(\\boldsymbol\\beta^T \\boldsymbol x\\) we still need to estimate the variance. That said, in pretty much every other way Poisson regression and logistic regression are more complicated. The math does not work out so nice for both as it does for linear regression. For linear regression one can write out \\(p(\\boldsymbol y | \\boldsymbol\\beta, \\sigma^2, X)\\) and see conjugate priors for \\((\\boldsymbol\\beta, \\sigma^2)\\) are normal and inverse gamma which is nice! On the other hand, if \\(Y | x \\sim\\) Poisson\\(( e^{\\boldsymbol\\beta^T \\boldsymbol x} ),\\) there is no conjugate prior for \\(\\boldsymbol \\beta\\). If there’s no conjugate prior then we are not restricted in what we do. Suppose \\(\\boldsymbol\\beta \\sim\\) Normal\\(_p(\\boldsymbol \\beta_0, \\boldsymbol\\Sigma_0 ),\\) we can write \\(p(\\boldsymbol \\beta | \\boldsymbol y, \\mathbf{X}) = c\\times p(\\boldsymbol \\beta) p(y | \\boldsymbol \\beta, \\mathbf{X}).\\) How are we gonna approximate this posterior distribution? In linear regression we’ve used Monte Carlo sampling (possibly Gibbs sampler) to approximate features of the posterior distribution. That’s not gonna be so straightforward for GLMs, so we need a new method. 17.5 The Metropolis algorithm Target distribution is the posterior distribution \\[ p(\\theta | y) = \\frac{p(\\theta ) p(y | \\theta)}{\\int p(\\theta&#39; ) p(y | \\theta&#39;)d\\theta&#39;} = c\\times p(\\theta ) p(y | \\theta) \\] The constant does not depend on \\(\\theta\\). In general we can’t solve this constant explicitly. But turns out that for the Metropolis algorithm we don’t have to. We want to sample random draws \\(\\theta^{(s)} \\sim p(\\theta | y)\\) and use those to approximate posterior probabilities, moments, quantiles etc. The reason Monte Carlo works is that when we take a Monte Carlo sample the condition below just kind of happens \\[ \\frac{\\#\\left\\{\\theta^{(s)} \\text { &#39;s in the collection }=\\theta_{a}\\right\\}}{\\#\\left\\{\\theta^{(s)} \\text { &#39;s in the collection }=\\theta_{b}\\right\\}} \\approx \\frac{p\\left(\\theta_{a} \\mid y\\right)}{p\\left(\\theta_{b} \\mid y\\right)} \\] Is there a way to achieve this condition other than \\(\\theta^{(s)} \\sim p(\\theta | y)?\\) That’s exactly what the Metropolis algorithm is shooting for. Suppose current value is \\(\\theta^{(s)}\\) and we need to generate \\(\\theta^{(s+1)}.\\) Suppose we have generated a proposal \\(\\theta^*\\) close to \\(\\theta^{(s)}.\\) If \\(\\theta^*\\) has higher probability under the target distribution then we want \\(\\theta^*\\) in our sample. If \\(\\theta^*\\) is a less probable \\(\\theta\\)-value than \\(\\theta^{(s)}\\) we maybe don’t want it but we don’t just throw it out either. Just because it has low probability desn’t mean it has zero probability. Define \\[ r=\\frac{p\\left(\\theta^{*} \\mid y\\right)}{p\\left(\\theta^{(s)} \\mid y\\right)}=\\frac{p\\left(y \\mid \\theta^{*}\\right) p\\left(\\theta^{*}\\right)}{p(y)} \\frac{p(y)}{p\\left(y \\mid \\theta^{(s)}\\right) p(\\theta(s))}=\\frac{p\\left(y \\mid \\theta^{*}\\right) p\\left(\\theta^{*}\\right)}{p\\left(y \\mid \\theta^{(s)}\\right) p\\left(\\theta^{(s)}\\right)} \\] This is the key to why Metropolis algorithm is usable for classes of models where direct simulation is not feasible, because this ratio \\(r\\) depends on the prior density which we can compute and the sampling probability which we can compute whereas the marginal probability \\(p(y) = \\int{p(y | \\theta)p(\\theta) d \\theta }\\) which we can’t always compute cancels out anyway! so we don’t have to! The logic goes; we have current value \\(\\theta^{(s)}\\) and we have proposed value \\(\\theta^*.\\) If \\(r &gt; 1\\) we like \\(\\theta^*,\\) so set \\(\\theta^{(s+1)} = \\theta^*.\\) If \\(r &lt; 1\\) we don’t necessarily like \\(\\theta^*\\) any better than \\(\\theta^{(s)}\\). In this case we’ll set \\(\\theta^{(s+1)}\\) to \\(\\theta^*\\) with probability \\(r\\) and \\(\\theta^{(s+1)} = \\theta^{(s)}\\) with probability \\(1 - r\\). If you’re familiar with rejection sampling this may seem familiar but it’s not that. In rejection sampling when a proposal is rejected it’s thrown out and we try again. In the Metropolis algorithm when a proposal is rejected we keep the current state. I still didn’t tell you where \\(\\theta^*\\) comes from. It comes from a proposal distribution \\(\\theta^* \\sim J( \\theta | \\theta^{(s)} )\\) where the Metropolis algorithm requires that this probability distribution be symmetric around \\(\\theta^{(s)}.\\) Symmetric means; the probability of proposing \\(\\theta^* = \\theta_b\\) given that \\(\\theta^{(s)} = \\theta_a\\) is equal to the probability of proposing \\(\\theta^* = \\theta_a\\) given that \\(\\theta^{(s)} = \\theta_b\\). For example uniform distribution centered at \\(\\theta^{(s)},\\) Normal distribution with mean \\(= \\theta^{(s)},\\) both work. \\(J(\\theta^*|\\theta^{(s)}) =\\) uniform\\((\\theta^{(s)}-\\delta,\\theta^{(s)}+\\delta)\\) \\(J(\\theta^*|\\theta^{(s)}) =\\) normal\\((\\theta^{(s)},\\delta^2)\\) Given \\(\\theta^{(s)}\\), the Metropolis algorithm generates a value of \\(\\theta^{(s+1)}\\) as follows: Sample the proposal \\(\\theta^{*} \\sim J\\left(\\theta \\mid \\theta^{(s)}\\right)\\); Compute the acceptance ratio \\(r\\) Accept or reject the proposal. i.e., set \\[ \\theta^{(s+1)}=\\begin{cases} \\theta^{*} &amp;\\text { with probability } \\min (r, 1)\\\\ \\theta^{(s)} &amp;\\text { with probability } 1-\\min (r, 1) \\end{cases} \\] Step 3 can be accomplished by sampling \\(u \\sim\\) uniform\\((0, 1)\\) and setting \\(\\theta^{(s+1)} = \\theta^*\\) if \\(u &lt; r\\) and setting \\(\\theta^{(s+1)} = \\theta^{(s)}\\) otherwise. 17.6 Example: Normal distribution with known variance This is a toy example. The value of the Metropolis algorithm is that it can work in problems where simpler methods don’t work. When we say “this example is a toy problem” we mean this is a problem where simpler methods do work. The value of a toy problem is we know the answer. Inference about a normal mean with variance known. Let \\(\\theta \\sim\\) Normal\\((\\mu, \\tau^2)\\) and \\(\\{y_1,...,y_n|\\theta\\} \\sim\\) Normal\\((\\theta, \\sigma^2),\\) the posterior distribution of \\(\\theta\\) is Normal\\((\\mu_n = 10.03, \\tau_n^2=0.20)\\) But suppose we didn’t know the above. The algorithm requires generate \\(\\texttt{theta.star} \\sim\\)Normal\\(( \\theta^{(s)}, \\delta^2 )=\\) Normal\\(( 0, 2 )\\) Compute acceptance ratio. In this case, \\(\\texttt{log.r}\\) \\[ \\log(r)=\\log \\left( \\frac{p\\left(\\theta^{*} \\mid \\boldsymbol{y}\\right)}{p\\left(\\theta^{(s)} \\mid \\boldsymbol{y}\\right)}\\right) =\\sum_{i=1}^{n} \\left[\\texttt{log dnorm}(y_{i}, \\theta^{*}, \\sigma) - \\texttt{log dnorm}(y_{i}, \\theta^{(s)}, \\sigma)\\right]\\\\ \\quad + \\texttt{log dnorm}(\\theta^{*}, \\mu, \\tau) - \\texttt{log dnorm}(\\theta^{(s)}, \\mu, \\tau) \\] The log of products and ratios is the sum and difference of logs. That’s a more numerically stable way to compute these things. It prevents “overflow / underflow.” overflow is what happens when you tell a computer to compute a number that’s bigger than the biggest number the computer knows. Accept with probability \\(r\\) = accept if \\(\\texttt{(log(runif(1)) &lt; log.r)}\\). Just like with the Gibbs sampler we still gotta start somewhere for Metropolis algorithm. For the sake of this example we start at zero. However, zero is not a good starting point. The target distribution is Normal(mean=10, sd=0.44). Zero is an utterly impossible value in this target distribution! as it’s wayyy out in the tail. So what does the algorithm do when you start it at a ridiculous value? # Metropolis algorithm for Normal mean (variance known) sigma2 &lt;- 1; tau2 &lt;- 10; mu &lt;- 5; y &lt;- c(9.37, 10.18, 9.16, 11.60, 10.33) n &lt;- length(y); ybar &lt;- mean(y) mu.n &lt;- (mu/tau2 + n*ybar/sigma2) / (1/tau2 + n/sigma2) tau2.n &lt;- 1 / (1/tau2 + n/sigma2) theta &lt;- 0 # starting value delta2 &lt;- 2 S &lt;- 10000 theta.chain &lt;- rep(NA, S) for(s in 1:S) { theta.star &lt;- rnorm(1, theta, sqrt(delta2)) log.r &lt;- ( sum(dnorm(y, theta.star, sqrt(sigma2), log=T) ) + dnorm(theta.star, mu, sqrt(tau2), log=T) ) - ( sum(dnorm(y, theta, sqrt(sigma2), log=T) ) + dnorm(theta, mu, sqrt(tau2), log=T) ) if(log(runif(1)) &lt; log.r) { theta &lt;- theta.star } theta.chain[s] &lt;- theta } par(mfrow=c(1,2)) plot(1:1000, theta.chain[1:1000], type=&quot;l&quot;, xlab=&quot;iteration&quot;, ylab=expression(theta)) # Now the next 1000 plot(1001:2000, theta.chain[1001:2000], type=&quot;l&quot;, xlab=&quot;iteration&quot;, ylab=expression(theta)) Figure 17.2: Left panel is the trace plot of first 1000 updates, second panel is the trace plot of second 1000 updates. Was 0 a good starting value? No, it was not. It looks like the target distribution has most of its probability between 9.0 and 11.0 or so. So was 0 a good starting value? No. Does it matter? Not really. It very quickly leaves that value and finds its way to the region of high posterior probability. When you’re thinking about MCMC where \\(\\theta^{(s)}\\) is the \\(s\\)th iteration of a Markov chain, to “explore” the posterior distribution \\(p(\\theta | y)\\) it helps to think of \\(\\theta\\) as a particle moving through space. What space? This is a univariate problem, so “space” is just the real line. The “particle moving through space” means a \\(\\theta\\) value moving along the horizontal axis in figure 17.4. Remember that the true mean is \\(10.03\\). Now suppose \\(\\theta^{(s)} = 9.5\\), if \\(\\theta^* = 10.0\\) then \\(\\theta^{(s+1)} = 10.0\\) (we want to move to the region with higher probability) if \\(\\theta^* = 9.0\\) then we might accept or we might reject since we want more \\(9.5\\)s in our sample than there are \\(9.0\\)s, but that doesn’t mean we don’t ever want \\(9.0\\)s. segment &lt;- rep(1:10, rep(S/10, 10)) boxplot(theta.chain ~ segment, col=&quot;pink&quot;, ylab=expression(theta~chain));rm(segment) Figure 17.3: Stationarity plot – boxplots for 10 segments of chain If the chain is stationary the boxplots in the stationarity plot should all look the same. In the first segment the chain was not stationary but by the second it was. By properties of this algorithm, once a chain is stationary it stays that way. The right way to state this conclusion is; “there’s no evidence of non-stationarity.” hist(theta.chain[-(1:50)], freq=F, right=F, breaks=40, main=&quot;&quot;, xlab=expression(theta), ylab=&quot;density&quot;,col=&quot;pink&quot;) minny &lt;- min(theta.chain) maxxy &lt;- max(theta.chain) theta.vals &lt;- seq(minny, maxxy, length=1000) lines(theta.vals, dnorm(theta.vals, mu.n, sqrt(tau2.n)),lwd=2) Figure 17.4: Histogram of all 10,000 draws. The black curve is the target distribution and it seems that indeed the sampled values match the target distribution well. So we can use \\(\\theta\\)-values generated from a Metropolis algorithm with \\(p(\\theta | y)\\) as the stationary distribution to approximate features of the posterior moments, probabilities and quantiles. What about that lousy starting value zero? Isn’t that going to mess up our approximations? Let’s see mean(theta.chain[1:1000]);mu.n ## [1] 9.93 ## [1] 10.03 The answer is yes! The true posterior mean of this distribution is 10.03, the mean of the first 1000 draws is 9.93. So what do you do? Well first of all the longer you run the chain the less the starting value matters so there’s that but there’s also a common practice of so-called “burn-in” The way burn in works is; Discard the first \\(B\\) iterations. The first \\(B\\) iterations are called the “burn-in” period what this does is reduce dependence on the starting value. If you want \\(S\\) draws from the posterior distribution run the chain for \\(B + S\\) iterations and discard the first \\(B\\). mean(theta.chain[1001:2000]);mu.n ## [1] 10.03 ## [1] 10.03 Notice how the mean for the next 1000 samples is much closer to the actual mean. 17.6.1 Output of Metropolis Algorithm Everything about the Metropolis algorithm is prescribed except \\(J\\), the proposal distribution. \\(J\\) is determined by the user and commonly \\(J(\\theta|\\theta^{(s)})\\) is Normal with mean \\(\\theta^{(s)}\\) but that still leaves the variance \\(\\delta^2\\) to be determined. How do we choose the variance? In the example we just did we set \\(\\delta^2 = 2.\\) Where did that come from? The variance in the jump proposal distribution is a tuning parameter that we can play around with to try to minimize autocorrelation in the resulting chain. In principle the algorithm should work regardless. In practice our choice for this value can have a huge impact on the performance of the algorithm Continuing with the normal toy problem Consider these 5 possibilities for \\(\\delta^2\\), \\(\\delta^2 \\in \\{1/32,1/2,2,32,64\\}\\). From a tiny variance 1/32 to a huge variance 64. ACR &lt;- NULL; ACF &lt;- NULL; theta.all.chains &lt;- NULL; for (delta2 in 2^c(-5, -1, 1, 5, 7) ){ S &lt;- 10000 theta.chain &lt;- rep(NA, S) theta &lt;- acs &lt;- 0 for(s in 1:S) { theta.star &lt;- rnorm(1, theta, sqrt(delta2)) log.r &lt;- sum( dnorm(y, theta.star, sqrt(sigma2), log=T) - dnorm(y, theta, sqrt(sigma2), log=T) ) + dnorm(theta.star, mu, sqrt(tau2), log=T) - dnorm(theta, mu, sqrt(tau2), log=T) if(log(runif(1)) &lt; log.r) { theta &lt;- theta.star; acs &lt;- acs+1 } theta.chain[s] &lt;- theta } ACR &lt;- c(ACR, acs/s) ACF &lt;- c(ACF, acf(theta.chain, plot=F)$acf[2] ) theta.all.chains &lt;- cbind(theta.all.chains, theta.chain) } acs is counting the “accepted proposals” so ACR is the acceptance rate. ACF is the lag 1 autocorrelation. We want acceptance rate to be????. We want the ACF to be as low as possible (it is necessarily positive). If we find that proposal variance \\(\\delta^2\\) is too low this means autocorrelation is high. If we find proposal variance is too high this also means autocorrelation is high. There’s a ‘sweet spot’ right in the middle. To minimize autocorrelation we want the proposal variance to be at that sweet spot not too big and not too small. How does this translate to acceptance rates? The answer is also; not too big and not too small. aa &lt;- round(cbind(ACR,ACF),2) rownames(aa) &lt;- c(&quot;1/32&quot;,&quot;1/2&quot;,&quot;2&quot;,&quot;32&quot;,&quot;64&quot;);aa ## ACR ACF ## 1/32 0.88 0.97 ## 1/2 0.58 0.76 ## 2 0.35 0.69 ## 32 0.10 0.88 ## 64 0.05 0.90 How are acceptance rates related to the proposal variance? inversely. The bigger the proposal variance the more often you’re going to propose jumps way out into the tails and the more rejections you’re going to have. This is a general property of the Metropolis algorithm. The higher the proposal variance the lower the acceptance rate. When it comes to proposal variance we don’t it to be too big or too small we want it somewhere in the middle so that must mean we want the acceptance rate somewhere in the middle, say 20% to 50%. The figure below will help illustrate this. par(mfrow=c(1,3)) for(k in c(1,3,5)){ plot(theta.all.chains[1:500, k], type=&quot;l&quot;, xlab=&quot;iteration&quot;, ylab=&quot;theta&quot;, ylim=range(theta.all.chains)); abline(h=mu.n, lty=2) } Plot on the left has a very low proposal variance so most of the jumps are getting accepted but you can see the autocorrelation is very high. We want the chain to be moving around a lot we don’t want it taking baby steps like this. The right-most plot shows what happens if the proposal variance is too big. Too many proposals are too far out in the tails and get rejected so the chain stays stuck for long stretches. The optimal case is somewhere in between, such as the middle plot. 17.7 The Metropolis algorithm for Poisson regression {sec:metpois} \\(Y =\\) number of offspring female bird has \\(\\log( E(Y|x) )= \\log( \\theta_x ) = \\beta_1 + \\beta_2x + \\beta_3 x^2\\). The quadratic is there because the relationship was not monotone. The log is there because the mean for a Poisson variable has to be positive as \\(E(Y |x) = \\boldsymbol\\beta^T x\\) might take negative values, which is not good. Instead take \\(E(Y|x) = \\exp{(\\boldsymbol\\beta^T x )}&gt;0\\). The parameters of this model are \\(\\beta_1, \\beta_2, \\beta_3\\). We don’t need the \\(\\sigma^2\\) parameter because the mean parameter describes the variance parameter in a Poisson model. We will take diffuse priors because we want the posterior to depend mostly on the data not on the prior because our prior beliefs are weak. So take \\(\\beta_j \\stackrel {\\text{indep}} {\\sim}\\) Normal(0,100). The acceptance ratio; \\[ \\begin{array}{l} r =\\frac{p\\left(\\boldsymbol{\\beta}^{*} \\mid \\mathbf{X}, \\boldsymbol{y}\\right)}{p\\left(\\boldsymbol{\\beta}^{(s)} \\mid \\mathbf{X}, \\boldsymbol{y}\\right)} =\\frac{\\prod_{i=1}^{n} \\texttt{dpois}\\left(y_{i}, \\boldsymbol{x}_{i}^{T} \\boldsymbol{\\beta}^{*}\\right)}{\\prod_{i=1}^{n} \\texttt{dpois}\\left(y_{i}, \\boldsymbol{x}_{i}^{T} \\boldsymbol{\\beta}^{(s)}\\right)} \\times \\frac{\\prod_{j=1}^{3} \\texttt{dnorm}\\left(\\beta_{j}^{*}, 0,10\\right)}{\\prod_{j=1}^{3} \\texttt{dnorm}\\left(\\beta_{j}^{(s)}, 0,10\\right)}\\\\ =\\sum_{i=1}^n \\left[ \\texttt{log dpois}(y_{i}, \\boldsymbol{x}_{i}^{T} \\boldsymbol{\\beta}^{*})- \\texttt{log dpois}(y_{i}, \\boldsymbol{x}_{i}^{T} \\boldsymbol{\\beta}^{*}) \\right]+\\\\ \\sum_{j=1}^3\\left[ \\texttt{log dnorm}(\\beta_{j}^{*},0,10)- \\texttt{log dnorm}(\\beta_{j}^{(s)},0,10) \\right] \\end{array} \\] Let the jump proposal distribution \\(J\\) be; \\(\\boldsymbol \\beta^* \\sim\\) Normal\\(_3(\\) mean \\(= \\boldsymbol \\beta^{(s)}\\) , variance \\(= \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1})\\). Hoff gives reasoning for saying set the variance equal to \\(\\hat \\sigma^2 (\\mathbf{X}^T \\mathbf{X})^{-1}\\) where \\(\\hat \\sigma^2\\) is approximated by the sample variance of the \\(\\log(\\boldsymbol y + 1/2)= \\texttt{var(log(y+1/2))}\\) We just need a thing to try first. If the accept probability &lt; 0.20 then make the proposal variance smaller if the accept probability &gt; 0.50 then make the proposal variance bigger. # Fit Bayesian Poisson regression model Y|x, beta ~ Poisson with # E(Y|x, beta) = beta1 + beta2*x + beta3*x^2 # Prior is beta_j ~ indep Normal(mn=0, sd=10) y &lt;- fledged # vector of observed responses n &lt;- length(y) X &lt;- cbind(rep(1,n), age, age^2) rownames(X) &lt;- 1:n; colnames(X) &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;) n &lt;- dim(X)[1]; p &lt;- dim(X)[2]; pmn.beta &lt;- rep(0, p) # prior expectation vector psd.beta &lt;- rep(10, p) # prior standard deviations var.prop &lt;- var(log(y+1/2)) * solve( t(X) %*% X) S &lt;- 10000 beta &lt;- rep(0, p); acs &lt;- 0; beta.chain &lt;- matrix(NA, S, p) for(s in 1:S) { beta.p &lt;- rmvnorm(1, beta, var.prop)[1,] log.r &lt;- sum( dpois(y, exp(X %*% beta.p), log=T) ) - sum( dpois(y, exp(X %*% beta ), log=T) ) + sum( dnorm(beta.p, pmn.beta, psd.beta, log=T) ) - sum( dnorm(beta , pmn.beta, psd.beta, log=T) ) ### if(log(runif(1)) &lt; log.r){ beta &lt;- beta.p; acs &lt;- acs + 1 } beta.chain[s,] &lt;- beta } acs / S ## [1] 0.4268 Acceptance rate should be between 20% and 50% so 42% is totally fine, no need to further tune the sample. It’s better to let the computer run 2 hours than to spend 4 hours tuning your chain and get your run time down to 30 minutes. # Make trace plots of the first 1000 updates par(mfrow=c(1,p)) for(j in 1:p){ plot(beta.chain[1:1000,j], type=&quot;l&quot;, xlab=&quot;iteration&quot;, ylab=paste(&quot;beta_&quot;, j, sep=&quot;&quot;), main=&quot;&quot;) } was (0,0,0) a good starting value? Not really, but not terrible either. If you want to throw away the first part of the chain as burn-in you can do that as well. # Sample autocorrelation functions par(mfrow=c(1,p)) for(j in 1:p){ acf(beta.chain[,j], main=paste(&quot;beta_&quot;, j, sep=&quot;&quot;)) } Autocorrelation is substantial # Effective sample sizes library(mcmcse) ess(beta.chain) ## [1] 889.0 785.8 599.0 If we had the goal of getting effective sample sizes of at least 1000 that goal is not met. We’d need more than 10000 iterations for that. But these effective sample sizes are fine. # Approximate posterior densites of beta1, beta2, beta3 par(mfrow=c(1,p)) for(j in 1:p){ plot(density(beta.chain[,j], adj=2), lwd=2, main=&quot;&quot;, xlab=paste(&quot;beta_&quot;, j, sep=&quot;&quot;), ylab=&quot;p(beta|y)&quot;) } apply(beta.chain,2,mean) ## [1] 0.2182 0.7217 -0.1414 We knew \\(\\hat \\beta_3\\) would be negative because we have a quadratic with a peak not a quadratic with a valley. The point of this modeling was to get estimates of expected number of offspring by age so let’s look those! The expected number of offspring at age \\(x = \\exp(\\beta_1 + \\beta_2x + \\beta_3x^2).\\) So for each value of \\(x\\), \\(x = 1, 2, 3, 4, 5, 6\\) calculate \\(\\theta_x^{(s)} = \\exp( \\beta_1^{(s)} + \\beta_2^{(s)} x + \\beta_3^{(s)}x^2 ).\\) That will give us MCMC approximations to the expected number of offspring at each age # Posterior inference for E(Y|x) = beta1 + beta2*x + beta3*x^2 theta.chain &lt;- matrix(NA, S, 6) rownames(theta.chain) &lt;- 1:S colnames(theta.chain) &lt;- paste(&quot;theta_&quot;, 1:6, sep=&quot;&quot;); for (x in 1:6) { theta.chain[,x] &lt;- exp(as.vector(beta.chain %*% c(1, x, x^2))) } (quants &lt;- apply(theta.chain, 2, quantile, probs=c(.025, .5, .975))) ## theta_1 theta_2 theta_3 theta_4 theta_5 theta_6 ## 2.5% 1.472 2.329 2.320 1.820 0.814 0.2006 ## 50% 2.243 3.007 3.045 2.328 1.349 0.5918 ## 97.5% 3.253 3.782 3.912 2.951 2.113 1.5986 matplot(t(quants), type=&quot;b&quot;, pch=19, lty=1, lwd=2, col=c(&quot;pink&quot;, &quot;black&quot;, &quot;pink&quot;), xlab=&quot;age x&quot;, ylab=&quot;E(Y|x)&quot;, main=&quot;Expected offspring by age&quot;) legend(&quot;topright&quot;, inset=.05, pch=19, lty=1, lwd=2, col=c(&quot;black&quot;,&quot;pink&quot;), legend=c(&quot;Posterior median&quot;, &quot;95% posterior interval&quot;),cex=0.7) Here’s that quadratic shape. The peak is at 2-3 years and after that it tails off. The posterior distributions of \\(\\boldsymbol\\beta\\) were pretty symmetric but these intervals are not! That’s because of the exponentiating. "],["metropolis-hastings.html", "Lecture 18 Metropolis-Hastings 18.1 Why does the Metropolis-Hastings algorithm work? 18.2 Combining the Metropolis and Gibbs algorithms 18.3 Example: Historical CO\\(_2\\) and temperature data 18.4 A regression model with correlated errors 18.5 Analysis of the ice core data", " Lecture 18 Metropolis-Hastings The following notes, mostly transcribed from Neath(0603,2021) lecture, summarize sections (10.4-10.5) of Hoff(2009). The Metropolis algorithm and, for that matter, the Gibbs sampler as well are both special cases of a more general MCMC algorithm Metropolis-Hastings. 18.1 Why does the Metropolis-Hastings algorithm work? Notation switch! we’re not doing \\(\\theta\\) with \\(p(\\theta |\\boldsymbol y)\\) as the target distribution. Now we’re doing generic \\(x\\) as the variable and \\(p_0(x)\\) as the target density. Current state of chain is \\(x^{(s)}\\) Draw proposal \\(x^{*}\\) from \\(J_{s}\\left(x^{*} \\mid x^{(s)}\\right)\\); Compute the acceptance ratio \\[ r=\\frac{p_{0}\\left(x^{*}\\right)}{p_{0}\\left(x^{(s)}\\right)} \\times \\frac{J_{s}\\left(x^{(s)} \\mid x^{*}\\right)}{J_{s}\\left(x^{*} \\mid x^{(s)}\\right)} \\] Sample \\(u \\sim \\operatorname{uniform}(0,1)\\). If \\(u&lt;r\\) set \\(x^{(s+1)}=x^{*}\\), else set \\(x^{(s+1)}=x^{(s)}\\). The Hastings innovation that generalized the Metropolis algorithm is to not require that the jump proposal distribution be symmetric around \\(\\theta^{(s)}\\). If it is then the second component of the acceptance ratio is 1.0. Because the jump proposal distribution need not be symmetric the acceptance ratio needs to be adjusted for not just the relative probability in the target distribution but the relative frequency of getting proposed. So \\(\\theta^*\\) is a value that is getting proposed a lot then we want to reduce the probability of it getting accepted. 18.2 Combining the Metropolis and Gibbs algorithms Sometimes you have a model with lots of variables for some of them the conditional distribution takes a nice form. Yay! We can do Gibbs updates for those components. But for other variables, perhaps not so nice. Can’t do Gibbs. So do Gibbs updates for the components where full conditional has a nice solution and for the components where the full conditional does not have a nice solution do a Metropolis update. 18.3 Example: Historical CO\\(_2\\) and temperature data Data set on historic earth temperatures and CO\\(_2\\) levels over 100s of thousands of years. Two variables observed every 1000 years or so are temperature and CO\\(_2\\) concentration. These will be our \\(y\\)-variable and \\(x\\)-variable respectively. Here’s a time series plot of the data dim(icecore); names(icecore) ## [1] 200 3 ## [1] &quot;year&quot; &quot;co2&quot; &quot;tmp&quot; Time between consecutive measurements is approximately 2,000 years. plot(icecore[,1], (icecore[,3]-mean(icecore[,3]))/sd(icecore[,3]) , type=&quot;l&quot;, lwd=2, xlab=&quot;year&quot;, ylab=&quot;standardized measurement&quot;, ylim=c(-1.75, 3)) lines(icecore[,1], (icecore[,2]-mean(icecore[,2]))/sd(icecore[,2]), lwd=3, col=&quot;pink&quot;) legend(&quot;topright&quot;, inset=.05, lwd=2, col=c(&quot;black&quot;,&quot;pink&quot;), legend=c(&quot;tmp&quot;, &quot;co2&quot;) ) Figure 18.1: Time series plot of temperature and carbon dioxide -4e+05 means 400,000 years ago. How do you put these two time series on the same axes? one for temperature (difference from current levels in degrees Celsius) and one for CO\\(_2\\) in parts per million volume. You have to standardize! The agreement between these two time series is quite close. The model we’re going to fit is a linear regression model with temperature as the response and CO\\(_2\\) as the predictor. Our model will say: let \\(x_t = co_2\\) at time \\(t\\) let \\(y_t =\\) temperature at time \\(t\\), \\(t = 1, 2, …., 200\\). Time is just acting as an index here since we have equally spaced measurements. We’re gonna consider a regression model in which \\(y_t = \\beta_1 + \\beta_2 x_t + \\epsilon_t = \\beta_1 + \\beta_2 \\texttt{co2}_t + \\epsilon_t\\). This is a regression model relating \\(\\texttt{temp}\\) to \\(\\texttt{co2}\\). This relationship is summarized by a scatterplot # time is not reflected on this plot plot(icecore[,2], icecore[,3], xlab=expression(paste(CO[2], &quot;(ppmv)&quot;)), ylab=&quot;temperature difference (deg C)&quot;) lines(lowess(icecore[,2], icecore[,3]), lwd=2) Figure 18.2: Scatterplot of temperature versus carbon dioxide There seems to be a positive association. Linear regression seems reasonable for this problem. Question is: The usual regression model assumes the error terms are independent. Is the assumption of independent error terms met for these data? If \\(\\epsilon_t \\stackrel{\\text{iid}}{\\sim}\\) Normal\\((0, \\sigma^2)\\) then our model has three parameters \\(\\beta_1, \\beta_2, \\sigma^2\\). These data are observed sequentially over time. We need to check for the possibility of temporal association in the error term # Fit ordinary linear regression of tmp on co2 fit &lt;- lm(tmp ~ co2, data=icecore); summary(fit)$coeff; summary(fit)$sigma ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -23.02414 0.879543 -26.18 3.272e-66 ## co2 0.07985 0.003834 20.83 8.767e-52 ## [1] 1.533 \\(\\hat\\beta_1 = -23\\), \\(\\hat\\beta_2 = 0.08\\), \\(\\hat\\sigma = 1.53\\) Normally distributed error terms is, strictly speaking, a fundamental assumption of the linear regression model but practically not the most important. hist(resid(fit), freq=F, right=F, breaks=30, main=&quot;&quot;, xlab=&quot;residual&quot;, col=&quot;pink&quot;) Histogram of residuals looks like a bell curve or close enough. This is fine. What is a key assumption is that the error terms are independent. There isn’t a go to diagnostic for non-independence of error terms. Such a diagnostic only exists if there is some structure to the data such as spatial or temporal. In this case these data are observed over time so autocorrelation is meaningful! par(mfrow=c(1,2)) ts.plot(resid(fit));acf(resid(fit)) In addition to \\(y\\) and \\(x\\) being time series, it appears that the error term has time series component also. There is positive temporal association. acf(resid(fit), plot=F)[1] ## ## Autocorrelations of series &#39;resid(fit)&#39;, by lag ## ## 1 ## 0.517 Lag-1 autocorrelation is 0.5. This is something we should not ignore. Sometimes you can have data that are observed sequentially over time but the relationship between \\(y\\) and \\(x\\) is such that the residuals are practically independent. Here we have data observed over time but the relationship between \\(y\\) and \\(x\\) is such that the residuals are not independent (there is positive temporal association). So we gotta deal with this. This means our model can’t say \\(\\epsilon_t \\stackrel{\\text{iid}}\\sim N(\\boldsymbol{0}, \\sigma^2).\\) Normality is okay, mean \\(\\boldsymbol{0}\\) is fine, constant variance is fine, the problem is the “iid” part (the error terms are not independent). There is no data transformation that’s gonna solve this problem. We need to accommodate it in the model itself. 18.4 A regression model with correlated errors For the icecore data the ordinary regression model \\[ \\boldsymbol{Y}=\\left(\\begin{array}{c} Y_{1} \\\\ \\vdots \\\\ Y_{n} \\end{array}\\right) \\sim \\operatorname{Normal}_{n}\\left(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^{2} \\mathbf{I}\\right) \\] does not hold because the off diagonal of the covariance matrix is not all zeros like the standard model says. Let’s introduce one new parameter. A covariance matrix (correlation matrix) has \\((n-1) (n-2) / 2\\) covariances? We don’t want to fit a model with this many parameters. Is there a simple model that will account for the temporal association of the error terms but not require a bunch of parameters to estimate? Yes there is. Try assuming the error terms follow the AR(1) autoregressive process which says that each error term is correlated with the predecessor. \\(\\text{cor}( \\epsilon_i, \\epsilon_{i+1} ) = \\rho=\\) lag-1 autocorrelation \\(\\text{cor}( \\epsilon_i , \\epsilon_{i+2} ) = \\rho^2=\\) lag-2 autocorrelation \\(\\text{cor}(\\epsilon_i , \\epsilon_{i+t} ) = \\rho^t\\), \\(0 &lt; \\rho &lt; 1\\) \\[ \\boldsymbol{\\Sigma}=\\sigma^{2} \\mathbf{C}_{\\rho}=\\sigma^{2}\\left(\\begin{array}{ccccc} 1 &amp; \\rho &amp; \\rho^{2} &amp; \\cdots &amp; \\rho^{n-1} \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\cdots &amp; \\rho^{n-2} \\\\ \\rho^{2} &amp; \\rho &amp; 1 &amp; &amp; \\rho^{n-3} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\rho^{n-1} &amp; \\rho^{n-2} &amp; \\rho^{n-3} &amp; \\cdots &amp; 1 \\end{array}\\right) \\] The \\(n \\times n\\) covariance matrix looks like this. It has \\(n^2\\) entries but only two parameters the variance \\(\\sigma^2\\) and the lag-1 autocorrelation \\(\\rho.\\) So the model parameters are more generally; \\(\\boldsymbol\\beta = (\\beta_1, \\beta_2, …, \\beta_p)\\) and \\(\\sigma^2\\) (the residual variance) and \\(\\rho\\) (the residual autocorrelation). We have our data we have our model we just need to fit that model! We need to propose prior distributions for \\(\\boldsymbol \\beta\\), \\(\\sigma^2\\) and \\(\\rho\\), do the Bayes rule calculation and determine the posterior distributions! For \\(\\boldsymbol \\beta\\) and \\(\\sigma^2\\) assume the normal and inverse-gamma priors. We get posteriors of those forms as well because those are conjugate priors. Note that when I say “posteriors of these forms,” I mean “full conditionals” of these forms. So let’s find the full conditionals. \\(\\{\\boldsymbol{\\beta} \\mid \\boldsymbol{y}, \\mathbf{X}, \\sigma^{2}, \\rho\\} \\sim \\operatorname{Normal}_{p}\\left(\\boldsymbol{\\beta}_{n}, \\boldsymbol{\\Sigma}_{n}\\right)\\) where \\[ \\begin{array}{l} \\boldsymbol{\\Sigma}_{n}=\\left(\\Sigma_{0}^{-1}+\\mathbf{X}^{T} \\mathbf{C}_{\\rho}^{-1} \\mathbf{X} / \\sigma^{2}\\right)^{-1} \\\\ \\boldsymbol{\\beta}_{n}=\\boldsymbol{\\Sigma}_{n}\\left(\\boldsymbol{\\Sigma}_{0}^{-1} \\boldsymbol{\\beta}_{0}+\\mathbf{X}^{T} \\mathbf{C}_{\\rho}^{-1} \\boldsymbol{y} / \\sigma^{2}\\right) \\end{array} \\] and \\(\\{\\sigma^{2} \\mid \\boldsymbol{y}, \\mathbf{X}, \\boldsymbol{\\beta}, \\rho\\} \\sim \\operatorname{InvGamma}\\left(\\left[\\nu_{0}+n\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\mathrm{SSR}_{\\rho}\\right] / 2\\right)\\) where \\[ \\operatorname{SSR}_{\\rho}=(\\boldsymbol{y}-\\mathbf{X} \\boldsymbol{\\beta})^{T} \\mathbf{C}_{\\rho}^{-1}(\\boldsymbol{y}-\\mathbf{X} \\boldsymbol{\\beta}) . \\] \\(\\hat{\\boldsymbol\\beta}\\) is the usual “Generalized Least Squares” estimate of \\(\\boldsymbol{\\beta}\\). GLS includes this correlation matrix \\(\\mathbf C_\\rho\\). What about \\(\\rho?\\) Turns out there is no conjugate prior for \\(\\rho\\) i.e., { \\(\\rho\\) | everything else } is not a nice form. So here’s what we’ll do. We’ll use the Gibbs sampler set-up. We’ll update \\(\\boldsymbol\\beta\\) and \\(\\sigma^2\\) by Gibbs. But when we get to \\(\\rho\\) we’re gonna need to do a Metropolis update. Let’s assume \\(\\rho \\sim\\) Uniform\\((0, 1)\\). Given \\(\\{~\\boldsymbol\\beta^{(s)}, \\sigma^{2(s)}, \\rho^{(s)}~\\}\\), we will do our MCMC as follows: Update \\(\\boldsymbol{\\beta}:\\) Sample \\[ \\boldsymbol{\\beta}^{(s+1)} \\sim \\text { Normal}_{p}\\left(\\boldsymbol{\\beta}_{n}, \\boldsymbol{\\Sigma}_{n}\\right) \\] where \\(\\boldsymbol{\\beta}_{n}\\) and \\(\\boldsymbol{\\Sigma}_{n}\\) depend on \\(\\sigma^{2(s)}\\) and \\(\\rho^{(s)}\\). Update \\(\\sigma^{2}\\) : Sample \\[ \\sigma^{2(s+1)} \\sim \\operatorname{InvGamma}\\left(\\left[\\nu_{0}+n\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\mathrm{SSR}_{\\rho}\\right] / 2\\right) \\] where \\(\\operatorname{SSR}_{\\rho}\\) depends on \\(\\boldsymbol{\\beta}^{(s+1)}\\) and \\(\\rho^{(s)}\\) Update \\(\\rho\\) : Propose \\(\\rho^{*} \\sim\\) Uniform \\(\\left(\\rho^{(s)}-\\delta, \\rho^{(s)}+\\delta\\right)\\). If \\(\\rho^{*}&lt;0\\) then reassign it to be \\(\\left|\\rho^{*}\\right| ;\\) if \\(\\rho^{*}&gt;1\\), reassign it to be \\(2-\\rho^{*}\\). Compute the acceptance ratio \\[ r=\\frac{p\\left(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}^{(s+1)}, \\sigma^{2(s+1)}, \\rho^{*}\\right) p\\left(\\rho^{*}\\right)}{p\\left(\\boldsymbol{y} \\mid \\mathbf{X}, \\boldsymbol{\\beta}^{(s+1)}, \\sigma^{2(s+1)}, \\rho^{(s)}\\right) p\\left(\\rho^{(s)}\\right)} \\] Sample \\(u \\sim\\) uniform \\((0,1)\\). If \\(u&lt;r\\) set \\(\\rho^{(s+1)}=\\rho^{*}\\), otherwise set \\(\\rho^{(s+1)}=\\rho^{(s)}\\). It’s not obvious that this proposal distribution described in step 3a is symmetric as required by the metropolis algorithm. Turns out it is. This is called reflecting random walk. Instead of going negative, bounce back by that amount. Instead of going above 1 bounce back by that amount Let’s run this Gibbs sampler! 18.5 Analysis of the ice core data Priors: \\(\\boldsymbol\\beta_0 = \\boldsymbol0\\), \\(\\Sigma_0 = 1000\\times \\mathbf{I}_2,\\) diagonal with big numbers on the diagonal. In other words, a diffuse prior \\(\\nu_0 = 1\\) “prior sample size” of 1 versus data sample size of 200 (because diffuse prior) starting values: start \\(\\boldsymbol\\beta\\) at the OLS estimate of \\(\\boldsymbol\\beta\\) start \\(\\sigma^2\\) at the OLS estimate of \\(\\sigma^2\\) start \\(\\rho\\) at the sample lag-1 autocorrelation of the residuals from the OLS fit # Gibbs sampler for &#39;Bayesian GLS&#39; model fit n &lt;- dim(icecore)[1]; y &lt;- icecore[,3]; X &lt;- cbind(rep(1,n), icecore[,2]); p &lt;- 2; rownames(X) &lt;- 1:n colnames(X) &lt;- c(&quot;x1&quot;,&quot;x2&quot;) # Preliminary calculations lmfit &lt;- lm(y ~ 0 + X) beta &lt;- lmfit$coef sigma2 &lt;- summary(lmfit)$sigma^2 rho &lt;- acf(resid(lmfit), plot=F)$acf[2] #tuning parameter # and prior parameters nu.0 &lt;- 1; sigma2.0 &lt;- 1; Sigma0.inv &lt;- diag(2) / 1000 # Now run the MCMC! D &lt;- abs(outer(1:n, 1:n, &quot;-&quot;)) S &lt;- 1000 ac &lt;- 0; delta &lt;- 0.1 beta.chain &lt;- matrix(NA, S, p) sigma2.chain &lt;- rep(NA, S) rho.chain &lt;- rep(NA, S); library(mvtnorm) run.time &lt;- proc.time() for(s in 1:S) { # Update beta first C.rho.inv &lt;- solve( rho^D ) V.beta &lt;- solve(t(X) %*% C.rho.inv %*% X / sigma2 + Sigma0.inv) m.beta &lt;- V.beta %*% (t(X) %*% C.rho.inv %*% y / sigma2) beta &lt;- rmvnorm(1, mean=m.beta, sigma=V.beta)[1,] # Now update sigma2 SSR &lt;- t(y - X %*% beta) %*% C.rho.inv %*% (y - X %*% beta) sigma2 &lt;- 1/rgamma(1, (nu.0+n)/2, (nu.0*sigma2.0 + SSR)/2) # Now Metropolis update of rho rho.star &lt;- abs(runif(1, rho-delta, rho+delta)) rho.star &lt;- min(rho.star, 2-rho.star) log.r &lt;- -0.5 * ( determinant( rho.star^D, log=T)$mod - determinant( rho^D, log=T)$mod + sum( diag( (y - X %*% beta) %*% t(y - X %*% beta) %*% ( solve(rho.star^D) - solve(rho^D) ) ) ) / sigma2 ) if(log(runif(1)) &lt; log.r) { rho &lt;- rho.star; ac &lt;- ac + 1; } # Save the updates! beta.chain[s,] &lt;- beta; sigma2.chain[s] &lt;- sigma2; rho.chain[s] &lt;- rho; } run.time &lt;- proc.time() - run.time ac / S # acceptance rate ## [1] 0.258 Tuning parameter is \\(\\delta\\). Remember our rule of thumb for tuning parameters and acceptance rates. A common strategy for tuning parameter is; Come up with a value for \\(\\delta\\) some way some how. If the acceptance rate &lt; 0.20 make \\(\\delta\\) smaller if accept rate \\(&gt; 0.50\\) make \\(\\delta\\) bigger (\\(\\delta\\) indicates the likely size of the proposed jump) Our accept rate was 0.26 so that should be fine! par(mfrow=c(1,2)) plot(rho.chain, type=&quot;l&quot;, xlab=&quot;scan&quot;, ylab=expression(rho)) acf(rho.chain) Figure 18.3: Trace plot and autocorrelation function for the rho chain This is not good. 30 updates of this Markov chain are NOT 30 samples from the posterior. There’s just too much correlation. library(mcmcse) ess(rho.chain) ## [1] 8.319 In fact for 1000 iterations of the “Metropolised” Gibbs sampler the effective sample size was 8. Meaning our 1000 iterations was the statistical equivalence of 8 independent observations. So here’s what we do. Don’t run 1000 iterations. Run 25,000 iterations. However, storing 25,000 sets of parameter values when consecutive values are practically the same (which they are here given this high autocorrelation) is not efficient. So what people do for chains with really high autocorrelation is to get a set of \\(S\\) samples, run the chain for \\(S \\times T\\) iterations but only save every \\(T^{\\text{th}}\\) update. Let’s do \\(S = 1000,~ T = 25\\) So we’ll run 25,000 updates(scans) of the Gibbs sampler but only save every 25th result. So we’ll get 1000 values. This is called “thinning” we thin our output from 25,000 values to just 1000. The lag-1 autocorrelation in the thinned chain will be the lag-25 autocorrelation in the unthinned chain. The lag 2 autocorrelation in the thinned chain chain will be the lag-50 autocorrelation from the unthinned chain. run.time # run time in seconds ## user system elapsed ## 38.568 1.896 40.486 # projected run time in minutes # This should take about 25 times the previous&#39; 25 * run.time / 60 ## user system elapsed ## 16.07 0.79 16.87 Set it off, then go get a cup of coffee. S &lt;- 1000 T &lt;- 25 beta.chain &lt;- matrix(NA, S, p); ac &lt;- 0; sigma2.chain &lt;- rep(NA, S); rho.chain &lt;-rep(NA, S); run.time &lt;- proc.time() for(s in 1:S) { for(t in 1:T){ ### # Update beta first ### C.rho.inv &lt;- solve( rho^D ) V.beta &lt;- solve(t(X) %*% C.rho.inv %*% X / sigma2 + Sigma0.inv) m.beta &lt;- V.beta %*% (t(X) %*% C.rho.inv %*% y / sigma2) beta &lt;- rmvnorm(1, mean=m.beta, sigma=V.beta)[1,] ### # Now update sigma2 ### SSR &lt;- t(y - X %*% beta) %*% C.rho.inv %*% (y - X %*% beta) sigma2 &lt;- 1/rgamma(1, (nu.0+n)/2, (nu.0*sigma2.0 + SSR)/2) ### # Now Metropolis update of rho ### rho.star &lt;- abs(runif(1, rho-delta, rho+delta)) rho.star &lt;- min(rho.star, 2-rho.star) log.r &lt;- -0.5 * ( determinant( rho.star^D, log=T)$mod - determinant( rho^D, log=T)$mod + sum( diag( (y - X %*% beta) %*% t(y - X %*% beta) %*% ( solve(rho.star^D) - solve(rho^D) ) ) ) / sigma2 ) if(log(runif(1)) &lt; log.r) { rho &lt;- rho.star; ac &lt;- ac + 1; } ### } # Save the current value, but only after T scans ### beta.chain[s,] &lt;- beta; sigma2.chain[s] &lt;- sigma2; rho.chain[s] &lt;- rho; } (run.time &lt;- proc.time() - run.time) ## user system elapsed ## 985.38 52.39 1041.88 ac / (S*T) # acceptance rate ## [1] 0.2762 # Let&#39;s look at trace plot and autocorrelation function for the rho chain par(mfrow=c(1,2)) plot(rho.chain, type=&quot;l&quot;, xlab=&quot;scan/T&quot;, ylab=expression(rho)) acf(rho.chain) Okay, that’s better ess(rho.chain) ## [1] 250.3 So the thinning was effective. It’s not that the thinned chain contains more information than the unthinned chain. But it’s practically as good and saves storage. We should have a good approximation to the posterior. Let’s start summarizing it. Let’s consider posterior inference about \\(\\beta_2.\\) plot(density(beta.chain[,2], adj=2), lwd=2, xlab=expression(beta[2]), ylab=expression(p(beta[2]*&quot;|&quot;*y)), main=&quot;&quot;) abline(v=quantile(beta.chain[,2], prob=c(.025, .975)), lty=2) Posterior density estimate Mostly above 0 round(quantile(beta.chain[,2], prob=c(.025,.25,.5,.75,.975)),3) ## 2.5% 25% 50% 75% 97.5% ## 0.010 0.021 0.028 0.034 0.048 95% posterior interval is [.01,.05] which is &gt; 0, so the data give pretty compelling evidence that there exists a positive association between temperature and CO\\(_2\\) over the millenia. round(c(mean(beta.chain[,1]),mean(beta.chain[,2])),3) ## [1] -11.032 0.028 Note: When we fit the model by OLS we got \\(\\hat\\beta_2 = 0.08\\). Using this “Bayesian GLS” model we get \\(\\hat \\beta_2 = 0.03\\) # fitting AR(1) to regression library(orcutt) round(summary(cochrane.orcutt(fit))$coef,3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -10.814 1.669 -6.479 0 ## co2 0.026 0.007 3.701 0 Coefficients similar to Bayesian result plot(y ~ X[,2], xlab=expression(CO[2]), ylab=&quot;temperature&quot;) abline(apply(beta.chain, 2, mean), lwd=2) abline(lmfit$coef, col=&quot;pink&quot;, lwd=2) legend(&quot;topleft&quot;, inset=.05, lwd=2, col=c(&quot;pink&quot;,&quot;black&quot;), legend=c(&quot;OLS estimate&quot;, &quot;Bayesian GLS&quot;)) Figure 18.4: Compare ‘Bayesian GLS’ estimate of regression line with the OLS estimate. This shows the raw data, the OLS line in pink and the GLS line in black. The GLS line has positive slope but A LOT less steep than the OLS line. Even though the pink line is the one that best fits these points by least squares criterion, given the correlation structure of the data the black line provides the more reliable estimate of the true association. Any Ideas as to why? Hoff provides the explanation that for OLS estimation, the small number of data points with high \\(y\\)-values have a larger amount of influence on the estimate of \\(\\boldsymbol\\beta\\). In contrast, the GLS model recognizes that many of these extreme points are highly correlated with one another and down-weights their influence. Student Question: If we directly used the 25000 unthinned values would we have gotten similar posterior conclusions?¿ Ans: Yes I believe so because \\(\\theta^{(1)}, \\theta^{(2)},...,\\theta^{(26)},\\) \\(\\theta^{(2)}\\) through \\(\\theta^{(25)}\\) don’t contribute a whole lot. They’re just a whole bunch more values that are probably between \\(\\theta^{(1)}\\) and \\(\\theta^{(26)}\\) "],["linear-mixed-effects-models-aka-hierarchical-linear-models.html", "Lecture 19 Linear Mixed-effects Models, aka, Hierarchical Linear Models 19.1 Hierarchical model review 19.2 Hierarchical linear regression model for math scores data 19.3 Bayesian hierarchical linear regression model 19.4 Bayesian analysis of the math scores data", " Lecture 19 Linear Mixed-effects Models, aka, Hierarchical Linear Models The following notes, mostly transcribed from Neath(0607,2021) lecture, summarize sections (11.1-11.3) of Hoff(2009). 19.1 Hierarchical model review The hierarchical normal model says \\(Y_{i,j} \\sim\\) Normal\\((\\theta_j, \\sigma^2 )\\) where \\(Y_{i,j} =\\) response for subject \\(i\\) in group \\(j\\). There’s a different mean value in each group. Think about data sets that arise from two stages of sampling, studying a medical procedure by having \\(m\\) different hospitals being tested. \\(Y_{i,j} =\\) outcome for the \\(i\\)th patient at the \\(j\\)th hospital. The different medical centers are gonna be similar to each other but not exactly the same. It wouldn’t be appropriate just to combine the data \\(Y_{i,j}\\) and pretend they are all independent observations from one big population because they’re not that, they are \\(m\\) different populations. It also wouldn’t be the best analysis to analyze them completely separately because some of them may be very some sample sizes that we can’t learn anything from anyway. We’re going to apply this idea to regression modeling where we have \\(m\\) different groups of observations. We believe there exists a different regression relationship within each group but the groups are themselves a sample from a population of groups. We’re going to use the same example that we used for chapter 8 where we have data on test scores of 10th graders from 100 different schools. Let \\(Y_{i,j}=\\) test score for student \\(i\\) at school \\(j\\), \\(j=1,2,...,100\\). There are 1993 students total. These 1993 students are not an independent random sample from the population of 10th graders. The 100 schools in the sample are a random sample from the population of US high schools. 1993 / 100 \\(\\approx\\) 20, so the within-school sample sizes are 20 on average. So the student on which we collect the data are a random sample from all students at that school. Our model in chapter 8 said: \\(Y_{i,j} = \\theta_j + \\epsilon_{i,j}\\), where \\(\\theta_j\\) represents the mean score at school \\(j\\) and \\(\\epsilon_{i,j}\\) is the student \\(i\\) random variation \\(\\theta_j \\stackrel{\\text{iid}}\\sim\\) Normal\\((\\mu, \\tau^2)\\) \\(\\epsilon_{i,j} \\stackrel{\\text{iid}}\\sim\\) Normal\\(( 0 , \\sigma^2 )\\) The the school means \\(\\theta_j\\) are themselves a random sample from the population of schools. Depending on how you define a parameter, there are either 3 or 103 parameters in this model. There is \\(\\mu\\) and \\(\\tau^2\\) which describe the distribution of school means. \\(\\theta_j\\) and \\(\\sigma^2_j\\) describe the \\(j\\)th school and it is a simplifying assumption that \\(\\sigma^2_j=\\sigma^2\\) is the same for all \\(j\\). Hence, with this assumption we have 3 parameters: \\(\\mu, \\tau^2, \\sigma^2\\). We attach prior distributions to these parameters \\(\\mu\\) (Normal) = overall mean \\(\\tau^2\\) (inverse-gamma) = between-schools \\(\\sigma^2\\) (inverse-gamma) = within-schools variance. The model says \\(\\theta_j \\stackrel{\\text{iid}}\\sim\\) Normal\\(( \\mu, \\tau^2 )\\). THIS IS NOT A PRIOR DISTRIBUTION. This is a sampling distribution (\\(\\theta_j\\) is not a parameter) because the probability distributions on \\(\\mu, ~\\tau^2\\) and \\(\\sigma^2\\) describe our uncertainty. However, the probability distribution on \\(\\theta_j\\) describes the random sampling that makes up the experiment. The experiment is to sample a 100 schools and then sample between 5 and 30 students from each school. But if we repeated the experiment we would have different \\(\\theta_j\\)’s but we would have the same \\(\\mu, ~ \\tau^2\\) and \\(\\sigma^2\\) 19.2 Hierarchical linear regression model for math scores data We are considering the same data but now we have an additional variable. Response variable \\(y_{i,j}\\) is test score for \\(i\\) student at school \\(j\\). Predictor variable \\(x_{i,j}\\) for the \\((i,j)\\) student is the socioeconomic status score (SES) centered to have mean zero at each school. SES score is based on parents income and education level. What we’re interested in today is estimating the regression relationship \\(y_{i,j} = \\beta_{1,j} + \\beta_{2,j} x_{i,j} + \\epsilon_{i,j},\\) and how these relationships differ across schools. \\(\\beta_1\\) is the intercept term \\(\\beta_2\\) is the slope (the relationship between SES and test score) and it’s probably \\(&gt; 0\\). Note there’s a \\(j\\)-subscript on those \\(\\beta\\)’s! because we are allowing that this regression relationship is different at different schools. We will need to group these data by schools for the modeling we want to do. For grouped data like these it’s convenient to store them in a list ids &lt;- sort(unique(nels$sch_id)) m &lt;- length(ids) # number of schools = 100 y &lt;- list();# list of response variables X &lt;- list();# regressor matrices for the 100 schools n &lt;- NULL; for(j in 1:m) { y[[j]] &lt;- nels[nels$sch_id==ids[j], 4] n[j] &lt;- sum(nels$sch_id==ids[j]) x.j &lt;- nels[nels$sch_id==ids[j], 3] x.j &lt;- (x.j - mean(x.j)) #centering X[[j]] &lt;- cbind(rep(1, n[j]), x.j) } The \\(\\texttt{x.j}\\) variable here is \\(x_j-E(x_j)\\) so that \\(\\texttt{x.j}\\) have a mean of zero at each school. Regression quiz: When the \\(x\\)-variable in regression model is centered to have mean zero what is the interpretation of the intercept term? \\(\\beta_{1,j} = E(Y | X=0)\\), “\\(x=0\\)” means \\(x\\) is the mean value. So centering the \\(x\\)-variable makes the intercept term \\(\\beta_{1,j}\\) equal the expected test score for a student with exactly average socioeconomic status score (SES). In other words, intercept is mean of \\(Y\\) at the mean value of each of the predictor variables. There are \\(m = 100\\) schools in this data set so we are going to fit \\(m=100\\) separate regressions. # Separate OLS fits at each school beta.hat.OLS &lt;- matrix(NA, m, 2) sigma2.hat &lt;- rep(NA, m) for(j in 1:m) { fit &lt;- lm(y[[j]] ~ 0 + X[[j]]) # no intercept beta.hat.OLS[j,] &lt;- fit$coef sigma2.hat[j] &lt;- summary(fit)$sigma^2 rm(fit) } plot(range(nels[,3]), range(nels[,4]), type=&quot;n&quot;, xlab=&quot;SES&quot;, ylab=&quot;Math score&quot;) for(j in 1:m){ abline(beta.hat.OLS[j,], col=&quot;gray&quot;) } abline(apply(beta.hat.OLS, 2, mean), lwd=2) Figure 19.1: 100 separate OLS fits in light gray, and the average slope / average intercept in solid black This display has 100 gray lines “\\(\\hat y = \\hat\\beta_{1j} + \\hat\\beta_{2j}\\texttt{SES}\\) for \\(j = 1, …, 100\\).” They are the 100 OLS regression lines for the 100 schools in the data set. The black line is the “average” of those 100 (the full pooled model estimate) where intercept = average of the intercepts and slope = average of the slopes. There are a few negative slopes which is not expected, but there are mostly positive associations. The fully pooled model is wrong because the data are not iid. The no pooling model is not wrong but does not give the most precise inference. hist(n, col = &quot;pink&quot;) School specific sample size range from 4 to 32. Fitting a regression line to four points is certainly something we can do, but don’t expect a very accurate estimate. par(mfrow=c(1,2)) plot(n, beta.hat.OLS[,1], xlab=&quot;sample size&quot;, ylab=&quot;intercept&quot;) abline(h=mean(beta.hat.OLS[,1]), lwd=2) plot(n, beta.hat.OLS[,2], xlab=&quot;sample size&quot;, ylab=&quot;slope&quot;) abline(h=mean(beta.hat.OLS[,2]), lwd=2) We have 100 pairs of \\(\\boldsymbol{\\hat\\beta}\\)’s (estimates) corresponding to the 100 schools and 100 sample sizes, and these plots of estimates versus group sample size. Are these values associated? From the plots, not really. However, there is a tendency for the extreme \\(\\beta\\)-values (both high and low) to occur at schools where sample size is low. Recall the normal hierarchical model; the “best estimate” of \\(\\theta_j\\) (the group \\(j\\) mean) was not \\(\\bar y_j\\)(group mean) nor was it \\(\\bar y_\\bullet\\) (grand mean), it was somewhere in between. We’re gonna see something similar here. The best estimate of the school \\(j\\) regression relationship is not that school’s gray line and it’s not the black line either it’s somewhere in between. The bigger the sample size the closer it will be to gray line. Two extreme approaches to analyzing these data (1) At one extreme conduct 100 separate regression analyses and that’s our analysis! That’s not wrong it’s just not optimal since these estimates are subject to very high uncertainty (owing to the low sample sizes) and this does not make the best use of all the information we have available (2) At the other extreme combine all 100 schools into a single sample of size \\(n=\\) 1993. The result would be the solid black line from that earlier plot. This would be wrong because it would way overstate our confidence because it pretends our data are a random sample of size \\(n=\\) 1993 which they’re not. Also, it gives no basis for studying the between-school differences which might be of interest. The analysis that sits between these two extremes that optimally uses all the information available is the hierarchical regression model. 19.3 Bayesian hierarchical linear regression model We will describe this model in a hierarchical way i.e, at multiple levels. There’s the within-group level and between-group level. The within-group sampling model says: \\[ \\mathbf{Y}_j \\sim \\text{Normal}_{nj}(\\mathbf{X}_j\\boldsymbol\\beta_j, \\sigma^2\\mathbf{I}) \\] we assume \\(\\sigma^2_j\\) is the same for all \\(j\\). The between-group sampling model says: \\[ \\boldsymbol\\beta_1,...,\\boldsymbol\\beta_m \\stackrel{\\text{iid}}\\sim \\text{Normal}_p(\\boldsymbol\\theta, \\boldsymbol\\Sigma) \\] We will assume (mostly for convenience) that \\(\\boldsymbol\\beta_j\\) for \\(j = 1, …, m\\) are a random sample from a \\(p\\)-variate normal distribution. \\(\\mathbf Y_j\\) are observable \\(\\boldsymbol\\beta_j\\) are unobservable but both are modeled by sampling distributions. The unknown model parameters consist of \\(\\sigma^2\\) (scalar) \\(\\boldsymbol\\theta\\) ( \\(p\\)-vector ) and \\(\\boldsymbol\\Sigma\\) ( \\(p \\times p\\) positive-definite matrix ). We have the sampling distributions that will drive the likelihood part of our Bayesian model. We need prior distributions to describe our beliefs about \\(\\{~\\boldsymbol\\theta, \\boldsymbol\\Sigma, \\sigma^2~\\}\\) before observing the data. Attempt at Fig. 11.2. (Hoff) A representation of the hierarchical normal regression model. \\[ \\begin{array}{c} \\boldsymbol\\theta,\\boldsymbol\\Sigma \\longrightarrow \\boldsymbol\\beta_1,...,\\boldsymbol\\beta_{m-1},\\boldsymbol\\beta_m \\longrightarrow \\mathbf{Y}_1,...,\\mathbf{Y}_{m-1}, \\mathbf{Y}_m\\\\[0.1cm] \\sigma^2 \\longrightarrow\\mathbf{Y}_1,...,\\mathbf{Y}_{m-1}, \\mathbf{Y}_m\\\\[0.3cm] \\end{array} \\] \\(\\boldsymbol\\theta\\) and \\(\\boldsymbol\\Sigma\\) are the mean and variance for the population of regression lines. In the hierarchical normal model there’s a whole population of school averages (the true group means). In this model there’s a whole population of regression parameters \\((\\boldsymbol\\beta_j , \\sigma^2_j ).\\) We observe a random sample of \\(\\boldsymbol\\beta_j\\)’s from that population except we don’t exactly observe it. What actually happens is; within each sampled group we observe a random sample of units \\(Y_{i,j}\\) within that group and from those observations we can make inference about the \\(\\boldsymbol\\beta_j\\). So how does \\(\\mathbf Y_1, \\mathbf Y_2, …., \\mathbf Y_{m-1}\\) inform our inference about \\(\\boldsymbol\\beta_m?\\) Well \\(\\boldsymbol\\beta_m\\) depends on \\(\\boldsymbol\\theta\\) and \\(\\boldsymbol\\Sigma,\\) and \\(\\mathbf Y_{m}\\) depends on \\(\\boldsymbol\\beta_m\\). Meanwhile \\(\\mathbf Y_1, …., \\mathbf Y_{m-1}\\) depend on \\(\\boldsymbol\\beta_1, …., \\boldsymbol\\beta_{m-1}\\) which in turn depend on \\(\\boldsymbol\\theta\\) and \\(\\boldsymbol\\Sigma\\) and therefore \\(\\mathbf Y_1, …., \\mathbf Y_{m-1}\\) contain information about \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\Sigma\\) and that in turn informs our inference about \\(\\boldsymbol\\beta_m\\). Arrows in this picture represent sampling distributions. The sampling distribution of \\(\\boldsymbol\\beta_j\\) depends on \\(\\boldsymbol\\theta\\) and \\(\\boldsymbol\\Sigma\\) the sampling distributions of \\(\\mathbf{Y}_j\\) depend on \\(\\boldsymbol\\beta_j\\) and \\(\\sigma^2\\). There are no arrows pointing at \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\Sigma\\) \\(\\sigma^2\\) which means we need to assign a prior distribution to them. BUT WHAT PRIORS? The usual priors! The semiconjugate prior distributions are used for \\(\\boldsymbol\\theta, ~\\boldsymbol\\Sigma\\) and \\(\\sigma^2\\) \\[ \\begin{aligned} \\boldsymbol{\\theta} &amp; \\sim \\operatorname{Normal}_{p}\\left(\\boldsymbol{\\mu}_{0}, \\boldsymbol{\\Lambda}_{0}\\right)\\\\ \\boldsymbol{\\Sigma} &amp; \\sim \\text { Inverse-Wishart}_{p}\\left(\\eta_{0}, \\mathbf{S}_{0}^{-1}\\right)\\\\ \\sigma^{2} &amp; \\sim \\text { Inverse-Gamma}\\left(\\nu_{0} / 2, \\nu_{0} \\sigma_{0}^{2} / 2\\right) \\end{aligned} \\] \\(\\eta_0\\) is the degrees of freedom (df), \\(\\mathbf{S}_0\\) is the prior scale matrix, \\(\\nu_0\\) is the prior df for \\(\\sigma^2\\) and \\(\\sigma^2_0\\) prior scale matrix. We do not need to do metropolis-hastings in this model because all these full conditionals have a nice form. So we can do the Gibbs sampler. What makes up the full conditionals? 19.3.1 Full conditionals \\(\\left\\{\\boldsymbol{\\beta}_{j} \\mid \\boldsymbol{y}_{j}, \\mathbf{X}_{j}, \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}, \\sigma^{2}\\right\\}\\) has a \\(p\\) -variate normal distribution with \\[ \\begin{aligned} \\operatorname{Var}\\left(\\boldsymbol{\\beta}_{j} \\mid \\boldsymbol{y}_{j}, \\mathbf{X}_{j}, \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}, \\sigma^{2}\\right) &amp;=\\left(\\boldsymbol{\\Sigma}^{-1}+\\mathbf{X}_{j}^{T} \\mathbf{X}_{j} / \\sigma^{2}\\right)^{-1} \\\\ \\mathbf{E}\\left(\\boldsymbol{\\beta}_{j} \\mid \\boldsymbol{y}_{j}, \\mathbf{X}_{j}, \\boldsymbol{\\theta}, \\boldsymbol{\\Sigma}, \\sigma^{2}\\right) &amp;=\\left(\\boldsymbol{\\Sigma}^{-1}+\\mathbf{X}_{j}^{T} \\mathbf{X}_{j} / \\sigma^{2}\\right)^{-1}\\left(\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\theta}+\\mathbf{X}_{j}^{T} \\boldsymbol{y}_{j} / \\sigma^{2}\\right) \\end{aligned} \\] Posterior precision = prior precision + sampling precision Posterior expectation is a weighted average of the prior expectation \\(\\boldsymbol{\\beta}\\) and sample estimate \\(\\hat{\\boldsymbol{\\beta}}\\) weighted by their precisions. \\(\\{~\\boldsymbol{\\theta} \\mid \\boldsymbol{\\beta}_{1}, \\ldots, \\boldsymbol{\\beta}_{m}, \\boldsymbol{\\Sigma}~\\} \\sim \\operatorname{Normal}_{p}\\left(\\boldsymbol{\\mu}_{m}, \\boldsymbol{\\Lambda}_{m}\\right)\\) where \\[ \\begin{aligned} \\boldsymbol\\Lambda_{m} &amp;=\\left(\\boldsymbol\\Lambda_{0}^{-1}+m \\boldsymbol\\Sigma^{-1}\\right)^{-1} \\\\ \\boldsymbol\\mu_{m} &amp;=\\left(\\boldsymbol\\Lambda_{0}^{-1}+m \\boldsymbol\\Sigma^{-1}\\right)^{-1}\\left(\\boldsymbol\\Lambda_{0}^{-1} \\boldsymbol\\mu_{0}+m \\boldsymbol\\Sigma^{-1} \\bar{\\boldsymbol\\beta}\\right) \\end{aligned} \\] where \\(\\overline{\\boldsymbol{\\beta}}\\) is the vector average \\(\\frac{1}{m} \\sum \\boldsymbol{\\beta}_{j}\\). \\(\\{~\\boldsymbol{\\Sigma} \\mid \\boldsymbol{\\theta}, \\boldsymbol{\\beta}_{1}, \\ldots, \\boldsymbol{\\beta}_{m} ~\\} \\sim\\) Inverse-Wishart\\(_{p}\\left(\\eta_{0}+m,\\left[\\mathbf{S}_{0}+\\mathbf{S}_{\\theta}\\right]^{-1}\\right)\\) where \\[ \\mathbf{S}_{\\theta}=\\sum_{j=1}^{m}\\left(\\boldsymbol{\\beta}_{j}-\\boldsymbol{\\theta}\\right)\\left(\\boldsymbol{\\beta}_{j}-\\boldsymbol{\\theta}\\right)^{T} \\] \\(\\mathbf{S}_\\theta\\) is the “sum of squares matrix” for the variability of the \\(\\boldsymbol{\\beta}_{j}\\) around \\(\\boldsymbol{\\theta}\\). Often we encounter \\(\\sum ( \\mathbf{a}^T\\mathbf{a}).\\) This here is \\(\\mathbf{aa}^T.\\) In mathematics this is called an outer product and it will give us a \\(p \\times p\\) matrix assuming \\(\\mathbf{a}\\) is \\(p \\times 1\\). \\(\\{~\\sigma^{2} \\mid \\boldsymbol{y}, \\mathbf{X}, \\boldsymbol{\\beta}~\\} \\sim \\operatorname{Inverse-Gamma}\\left(\\left[\\nu_{0}+\\sum n_{j}\\right] / 2,\\left[\\nu_{0} \\sigma_{0}^{2}+\\mathrm{SSR}\\right] / 2\\right)\\) where \\[ \\mathrm{SSR}=\\sum_{j=1}^{m} \\sum_{i=1}^{n_{j}}\\left(y_{i, j}-\\boldsymbol{\\beta}_{j}^{T} \\boldsymbol{x}_{i, j}\\right)^{2} \\] depends on the variability of the \\(y_{i,j}\\) around their means. 19.4 Bayesian analysis of the math scores data First we need priors. Following Hoff Chapter 11, we will use unit information priors. Such a prior distribution represents the information of someone with unbiased but weak prior information. Take the prior expectation of \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\mu_0\\), to be the average of the \\(\\boldsymbol{\\hat\\beta}_{j_{ols}}\\) and prior variance \\(\\boldsymbol\\Lambda_0\\) to be their sample covariance. For the prior distribution of \\(\\boldsymbol\\Sigma\\), by properties of the Wishart distribution the expectation matrix exists provided df is at least \\(p + 2\\) so take prior df \\(\\eta_0 = p+2 = 4\\) and then take the prior sum of squares matrix to be the covariance matrix of \\(\\boldsymbol{\\hat\\beta}_{j_{ols}}\\). For \\(\\sigma_0^2\\) take the average of the \\(\\hat\\sigma^2\\) for the 100 different OLS fits. # Set prior parameters here p &lt;- dim(X[[1]])[2] mu.0 &lt;- apply(beta.hat.OLS, 2, mean) Lambda.0 &lt;- cov(beta.hat.OLS) S.0 &lt;- cov(beta.hat.OLS) eta.0 &lt;- p + 2; nu.0 &lt;- 1; sigma2.0 &lt;- mean(sigma2.hat) \\(S =\\) number of saved values, \\(T =\\) thinning parameter. We run \\(S \\times T\\) total scans but save every \\(T\\)-th update and get \\(S\\) parameter values in the end. The reason thinning is a good idea in this problem is because there are \\(m\\times p = 100\\times2=200\\) different \\(\\beta\\) parameters hence storage costs are non trivial in this problem. S &lt;- 1000 T &lt;- 10; sigma2.chain &lt;- rep(NA, S); theta.chain &lt;- matrix(NA, S, p) Sigma.chain &lt;- matrix(NA, S, p^2) beta.chain &lt;- matrix(NA, S, m*p) # Compute inverse matrix once, not S*T times Lambda0.inv &lt;- solve(Lambda.0); # Starting values theta &lt;- mu.0; sigma2 &lt;- sigma2.0; beta &lt;- beta.hat.OLS; # beta is m x p Sigma.inv &lt;- solve(S.0) Be mindful about how you store these values. If you look at the full conditionals you’ll see that \\(\\boldsymbol\\Lambda_0^{-1}\\) appears a couple of places. Don’t do matrix inversion 10,000 times, do it once! # Now run it! library(mvtnorm) run.time &lt;- proc.time() for(s in 1:S) { for(t in 1:T) { ### # Update the beta_j ### for(j in 1:m) { V.j &lt;- solve( Sigma.inv + t(X[[j]]) %*% X[[j]] / sigma2 ) m.j &lt;- V.j %*% ( Sigma.inv %*% theta + t(X[[j]]) %*% y[[j]] / sigma2 ) beta[j,] &lt;- rmvnorm(1, mean=m.j, sigma=V.j)[1,] } ### # Update theta ### Lambda.m &lt;- solve( Lambda0.inv + m*Sigma.inv ) mu.m &lt;- Lambda.m %*% (Lambda0.inv %*% mu.0 + Sigma.inv %*% apply(beta,2,sum) ) theta &lt;- rmvnorm(1, mean=mu.m, sigma=Lambda.m)[1,] ### # Update Sigma matrix ### m.theta &lt;- matrix(theta, m, p, byrow=T) S.theta &lt;- t(beta - m.theta) %*% (beta - m.theta) Sigma.inv &lt;- rWishart(1, eta.0+m, solve(S.0 + S.theta))[,,1] ### # Update sigma2 ### SSR &lt;- 0 for(j in 1:m){ SSR &lt;- SSR + sum( (y[[j]] - X[[j]] %*% beta[j,])^2 ) } sigma2 &lt;- 1 / rgamma(1, (nu.0 + sum(n))/2, (nu.0*sigma2.0 + SSR)/2 ) ### } # at every t-th scan we&#39;ll save the results sigma2.chain[s] &lt;- sigma2; theta.chain[s,] &lt;- theta; Sigma.chain[s,] &lt;- solve(Sigma.inv); for(j in 1:m){ beta.chain[s, seq(j, j+(p-1)*m, m)] &lt;- beta[j,] } } run.time &lt;- proc.time() - run.time; run.time/60 #in minutes ## user system elapsed ## 5.3892 0.0356 5.4441 In the \\(t = 1: T\\) loop the values \\(\\boldsymbol{\\beta ,~\\theta,~ \\Sigma},~ \\sigma^2\\) are getting updated in each iteration but then they’re just overwritten after the \\(T\\)-th such iteration then we save the current values in \\(\\texttt{beta.chain, theta.chain, Sigma.chain, sigma2.chain}\\) 19.4.1 MCMC diagnostics par(mfrow=c(1,3)) acf(sigma2.chain) acf(theta.chain[,1]) acf(theta.chain[,2]) c(acf(theta.chain[,1], plot=F)$acf[2], acf(theta.chain[,2], plot=F)$acf[2]) ## [1] 0.06982 -0.01460 Happily the autocorrelation in the resulting chain is practically nil. Zero autocorrelation means your sample has equivalent information to an independent sample. library(mcmcse) ess(sigma2.chain) ## [1] 1000 ess(theta.chain) ## [1] 1000 1000 19.4.2 Posterior summaries Consider the \\(\\theta_2 = E(\\beta_{2j})\\) parameter, which represents the average slope (expected additional points per SD of SES score). It is the population mean slope parameter. quantile(theta.chain[,2], c(.025, .25, .5, .75, .975)) ## 2.5% 25% 50% 75% 97.5% ## 1.822 2.193 2.380 2.573 2.910 95% posterior interval for \\(\\theta_2\\) is [1.82, 2.91] plot(density(theta.chain[,2], adj=2), lwd=2, main=&quot;&quot;, xlab=expression(E(beta[&quot;2j&quot;])*&quot;=&quot;*theta[2]),ylab=expression(p(theta[2]*&quot;|&quot;*y))) abline(v=quantile(theta.chain[,2], c(.025, .975)), lty=2) \\(\\theta_2\\) is surely greater than one. This means that in the population in general there is positive association between SES and score on a math test score. 19.4.3 Posterior predictive simulation Consider a 101st school not in the sample. What can we say about our posterior predictive distribution \\(p(\\boldsymbol{\\tilde\\beta}| \\boldsymbol y) = \\int\\int{ p(\\boldsymbol{\\tilde\\beta} | \\boldsymbol\\theta, \\boldsymbol\\Sigma ) p( \\boldsymbol{\\theta, \\Sigma} | \\boldsymbol y)\\, d\\boldsymbol\\theta d\\boldsymbol\\Sigma }\\) To sample from this posterior predictive distribution we take our samples \\(\\boldsymbol\\theta^{(s)}\\) , \\(\\boldsymbol\\Sigma^{(s)}\\) which are drawn from \\(p(\\boldsymbol\\theta, \\boldsymbol\\Sigma | \\boldsymbol y )\\) For each saved scan, sample \\(\\boldsymbol{\\tilde\\beta}^{(s)} \\sim\\) Normal\\(_2( \\boldsymbol\\theta^{(s)} , \\boldsymbol\\Sigma^{(s)})\\) for \\(s = 1, …, S\\) # Regression line for an as-yet-unsampled school beta.tilde &lt;- matrix(NA, S, p) for(s in 1:S) { beta.tilde[s,] &lt;- rmvnorm(1, mean=theta.chain[s,], sigma = matrix(Sigma.chain[s,], 2, 2) ) } max(density(theta.chain[,2], adj=2)$y ) ## [1] 1.306 plot(density(beta.tilde[,2], adj=2), lwd=2, ylim=c(0, 1.31), xlab=&quot;slope parameter&quot;, ylab=&quot;posterior density&quot;, main=&quot;&quot;) lines(density(theta.chain[,2],adj=2), lwd=2, col=&quot;gray&quot;) legend(&quot;topright&quot;, inset=.05, lwd=2, col=c(&quot;gray&quot;, &quot;black&quot;), legend=c(expression(theta[2]), expression(tilde(beta[2])))) In this figure the black curve represents the distribution of possible slopes at the as yet unsampled school. The gray curve is our posterior belief about the mean of this distribution. These two curves should have the same mean. In this case it looks like they don’t due to Monte Carlo error. The sampling variability (i.e., sampling of \\(\\boldsymbol{\\tilde\\beta}\\) ) dominates the posterior uncertainty. The probability of a negative association between SES and score on math test at this to-be-sampled school i.e., \\(Pr(\\tilde\\beta_2 &lt; 0 | \\boldsymbol y, \\mathbf X),\\) mean(beta.tilde[,2] &lt; 0) ## [1] 0.085 Small. but not negligible beta.hat &lt;- apply(beta.chain, 2, mean) plot(range(nels[,3]), range(nels[,4]), type=&quot;n&quot;, xlab=&quot;SES&quot;, ylab=&quot;Math score&quot;) for(j in 1:m){ abline(beta.hat[c(j,j+m)], col=&quot;gray&quot;) } abline(apply(theta.chain, 2, mean), lwd=2) Figure 19.2: Summary plot of school-specific estimated regression lines, with overall average regression line overlaid Finally, to summarize our analysis, recall from figure 19.1 that we had a very noisy set of estimated regression lines. We believe an improved set of estimates is given above based on the notion of “borrowing information” across schools. mean(beta.hat[101:200] &lt; 0) ## [1] 0.02 Only 2 of the 100 schools have negative slope. "],["stan-poisson-regression.html", "Lecture 20 Stan; Poisson regression 20.1 Intro to Stan 20.2 Song sparrows reproductive success example", " Lecture 20 Stan; Poisson regression The following notes are mostly transcribed from Neath(0607,2021) lecture. 20.1 Intro to Stan Do you agree with this statement: Bayesian statistics is cool? Do you agree with this statement: Bayesian model-fitting is a lot more work than just fitting likelihood methods using \\(\\texttt{lm()}\\) and \\(\\texttt{glm()}\\) and such? In recent years (beginning around 2000 or so) there is an effort under way to change this. The question is: Is it possible to create user-friendly software with reasonable defaults that will allow users who aren’t programming experts to still fit such models? The first serious effort in this movement was a package called BUGS “Bayesian Inference using Gibbs Sampler.” There’s been others such as JAGS “just another Gibbs sampler” and NIMBLE (created and maintained by a group at UC Berkeley). The current state of the art among such programs is a program called Stan. It’s very general and very powerful. The output of Stan is a set of simulations from a posterior distribution in other words everything we need to do Bayesian inference. As an instructor for a course in Bayesian Statistics, there’s a question I struggle with a lot in the same way that if I were teaching Calculus, a question I would struggle with is do I allow graphing calculators? The art of Calculus is learning how to analyze functions from studying but how much is that relevant today when we hold machines in the palm of our hands that do those things for us? As an instructor for Bayesian Statistics, I want for us to get a good understanding of bayesian statistical models and how the simulation is done. Is Stan user-friendly? Simple? No. It takes a considerable investment of effort to become a proficient Stan user. The big news for today is that for pretty much everything we’ve done in this course there’s an easier way Or at least a way that is potentially easier if you’re willing to invest some effort in becoming proficient in Stan. I am more than willing to invest the effort to become proficient. I just haven’t yet. Some of you are not going to be doing much of these types of analyses after this course is over. For those of you who are it pays to become proficient in Stan. How does Stan work? You have to input the data (obviously), you have to describe your model to stan e.g., the prior, the sampling models etc. But that’s it. You do not have to derive a posterior distribution. You do not have to try a whole bunch of different values of tuning parameters in your Metropolis samplers. Stan effectively does that stuff for you. What’s the down side? You gotta learn it. It’s a new language. On this front the situation is better today than even just a few years ago in that it runs quite nicely within R and not just R, there’s Python Stan and others. Stan was developed at and is maintained at Columbia! Professor Gelman is the project leader. There are a whole community of contributors and there are 3 or 4 or 5 or so full time employees of Stan. They regularly meet in Watson Hall and if you become a Stan user you become part of this community and you can pose questions to these people and participate in different conferences and activities and such. Where Stan is lagging right now; there does not exist at this time at a level appropriate for our course a textbook with a large number of worked Stan examples. But there very likely will in a couple of years. 20.2 Song sparrows reproductive success example Remember the Poisson regression problem we did in section 17.2. # Section 10.1 of Hoff (2009) # 52 birds, response is &#39;fledged&#39; -- number of offspring # Predictor variable is age in years (1 to 5) fledged &lt;- c( 3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, 2, 2, 1, 1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, 1, 2, 1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2) age &lt;- c(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1) # Graphical summary of data: Boxplots by age boxplot(fledged ~ as.factor(age), range=0, col=&quot;pink&quot;, xlab=&quot;age&quot;, ylab=&quot;offspring&quot;) \\[ \\log E(Y_i|x_i) = \\beta_1+\\beta_2x_i+\\beta_3x_i^2 \\] where \\(Y_i\\) is a sample from a Poisson distribution. In this model \\(x =\\) age of female bird, \\(Y =\\) number of offspring that bird had. We observed two obvious things about this association (1) it’s positive then negative i.e., it’s not monotone. So it will require a quadratic term so we have \\(\\beta_1 + \\beta_2x + \\beta_3x^2.\\) (2) Expected number of offspring cannot be negative so maybe we don’t say \\(E(Y|x) = \\beta x\\) because that could be negative but rather \\(E(Y|x) = \\exp( \\beta x ).\\) So that’s how we got to our model. Also, a probability distribution is not completely determined by its mean hence we need a distribution assumption also. So we’ll use Poisson. Let’s see if we get similar results as before using Stan. You need to install the R package \\(\\texttt{rstan}\\). library(rstan) # description of our model y &lt;- fledged n &lt;- length(y) X &lt;- cbind(rep(1,n), age, age^2) rownames(X) &lt;- 1:n; colnames(X) &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;) n &lt;- dim(X)[1] p &lt;- dim(X)[2] # Fit model using Stan # Prior is beta_j ~ indep Normal(mn=0, sd=10) run.time &lt;- proc.time() stan_model &lt;- &quot; data{ int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; int&lt;lower=0&gt; y[n]; matrix[n,p] X; } parameters{ vector[p] beta; } model{ beta ~ normal(0, 10); y ~ poisson_log(X*beta); } &quot; data &lt;- list(n=n, p=p, y=y, X=X); fit_stan &lt;- stan(model_code=stan_model, data=data, chains=1, iter=6000, warmup=1000, refresh = 0) (proc.time() - run.time)/60 # elapsed time in minutes ## user system elapsed ## 0.4337 0.0276 0.4857 A stan model requires three components: (1) data statement (Tell stan what values are known), (2) parameters statements (tell stan what’s unknown) and (3) the model statement \\(\\texttt{y ~ Poisson_log(X*beta)}\\) \\(y\\) is Poisson with log expected value of \\(\\beta x\\). “warmup” in Stan is the same thing we called “burn-in” before. summary(fit_stan)$summary ## mean se_mean sd 2.5% 25% 50% 75% ## beta[1] 0.2234 0.014585 0.43475 -0.67380 -0.06354 0.2371 0.5271 ## beta[2] 0.7190 0.011342 0.33310 0.09595 0.48920 0.7101 0.9465 ## beta[3] -0.1412 0.001888 0.05707 -0.25690 -0.17946 -0.1395 -0.1030 ## lp__ -12.6975 0.036345 1.17219 -15.85503 -13.26755 -12.4025 -11.8346 ## 97.5% n_eff Rhat ## beta[1] 1.02925 888.5 1 ## beta[2] 1.39610 862.6 1 ## beta[3] -0.03493 913.3 1 ## lp__ -11.35609 1040.2 1 We have 5,000 samples for the posterior distribution \\(p(\\beta | y, X)\\) \\(\\texttt{se_mean = sd/sqrt{n_eff}}\\). \\(\\texttt{Rhat}\\) is a thing called the Gelman-Rubin statistic it’s a diagnostic of a not-yet-converged markov chain. If it’s bigger than 1.10 or so indicates you haven’t run the chain long enough. The last row of this table is a thing we haven’t talked about (and won’t) Results &lt;- summary(fit_stan)$summary[1:3, 4:8] round(Results, 2) ## 2.5% 25% 50% 75% 97.5% ## beta[1] -0.67 -0.06 0.24 0.53 1.03 ## beta[2] 0.10 0.49 0.71 0.95 1.40 ## beta[3] -0.26 -0.18 -0.14 -0.10 -0.03 If you want the samples themselves easy enough beta.sims &lt;- extract(fit_stan)$beta;head(beta.sims,3) ## ## iterations [,1] [,2] [,3] ## [1,] -0.1908 0.8781 -0.1561 ## [2,] 0.1824 0.6834 -0.1295 ## [3,] 0.2664 0.6443 -0.1268 The rows are iterations of the sampler the columns are the three components of \\(\\boldsymbol\\beta\\). # Make trace plots of the first 1000 updates par(mfrow=c(1,p)) for(j in 1:p){ plot(beta.sims[1:1000,j], type=&quot;l&quot;, xlab=&quot;iteration&quot;, ylab=paste(&quot;beta_&quot;, j, sep=&quot;&quot;), main=&quot;&quot;) } # Sample autocorrelation functions par(mfrow=c(1,p)) for(j in 1:p){ acf(beta.sims[,j], main=paste(&quot;beta_&quot;, j, sep=&quot;&quot;)) } Whatever Stan is doing is better than the Metropolis algorithm. the autocorrelation is basically zero! Stan is doing a thing called Hamiltonian Monte Carlo which is not unrelated to Metropolis-Hastings but it’s not Metropolis-Hastings. # Approximate posterior densites of beta1, beta2, beta3 par(mfrow=c(1,p)) for(j in 1:p){ plot(density(beta.sims[,j], adj=2), lwd=2, main=&quot;&quot;, xlab=paste(&quot;beta_&quot;, j, sep=&quot;&quot;), ylab=&quot;p(beta|y)&quot;) } Posterior inference for \\(E(Y|x) = \\beta_1 + \\beta_2x + \\beta_3x^2\\) S &lt;- dim(beta.sims)[1] theta.sims &lt;- matrix(NA, S, 6) rownames(theta.sims) &lt;- 1:S; colnames(theta.sims) &lt;- paste(&quot;theta_&quot;, 1:6, sep=&quot;&quot;); for (x in 1:6) { theta.sims[,x] &lt;- exp(as.vector(beta.sims %*% c(1, x, x^2))) } quants &lt;- apply(theta.sims, 2, quantile, probs=c(.025, .5, .975)) quants ## theta_1 theta_2 theta_3 theta_4 theta_5 theta_6 ## 2.5% 1.498 2.370 2.362 1.831 0.8006 0.1942 ## 50% 2.251 2.999 3.042 2.326 1.3426 0.5873 ## 97.5% 3.156 3.764 3.845 2.915 2.1010 1.5463 Do the results agree with what we did in a previous lecture – ??? matplot(t(quants), type=&quot;b&quot;, pch=19, lty=1, lwd=2, col=c(&quot;pink&quot;, &quot;black&quot;, &quot;pink&quot;), xlab=&quot;age x&quot;, ylab=&quot;E(Y|x)&quot;, main=&quot;Expected offspring by age&quot;) legend(&quot;topright&quot;, inset=.05, pch=19, lty=1, lwd=2, col=c(&quot;black&quot;,&quot;pink&quot;), legend=c(&quot;Posterior median&quot;, &quot;95% posterior interval&quot;),cex=0.7) They sure do. Pretty much perfect agreement! "],["summary.html", "Lecture 21 Summary 21.1 Missing data and imputation; 7b 21.2 Generalized linear mixed eﬀects models; 11b 21.3 Improper priors", " Lecture 21 Summary The following notes are mostly transcribed from Neath(0610,2021) lecture. We are done in the sense that no new course material will be presented today. Reminder: Next Monday is a day of mandatory real-time participation. This will be our “in-class final exam.” It will be interactive and collaborative. My goal for Monday is a learning experience more than final assessment. Attendance is mandatory. Be prepared to re-visit some homework problems and to answer questions posed directly to you ( if you’re chosen by the \\(\\texttt{sample()}\\) function in \\(\\texttt{R}\\) ). Given this is the last day and I said no new material let’s spend a few minutes talking about the things we didn’t cover in our course. 21.1 Missing data and imputation; 7b We went over “7a” multivariate normal(MVN) models but we did not go over “7b” which would have been about missing data. Pity because that’s a really cool application of the Gibbs sampler. Multivariate normal data: \\(Y_{i,j}\\) is response for variable \\(i\\) for unit \\(j\\). What if we observe \\(Y_{j} =\\) (missing, observed, missing, observed). Suppose \\(p=4\\) (there are 4 variables), and there’s a case in our data set where we have observed \\(y_2\\) and \\(y_4\\) but are missing \\(y_1\\) and \\(y_3\\). We don’t have to throw that case out! We can use the information that we have. The way we would do this: Recall for the MVN model we have a Gibbs sampler that works by sampling from the full conditionals \\(\\{~\\boldsymbol\\theta | \\Sigma, \\boldsymbol y~\\}\\) then \\(\\{~\\Sigma | \\boldsymbol\\theta, \\boldsymbol y~\\}\\). Now write complete data \\(= \\boldsymbol y = (\\boldsymbol y_{obs}, \\boldsymbol y_{mis}).\\) A way to handle missing data is just by adding a step to the Gibbs sampler. We don’t have to solve \\(\\{~\\boldsymbol\\theta | \\boldsymbol\\Sigma, y_{obs}~\\}\\), \\(\\{~\\Sigma |\\boldsymbol\\theta, \\boldsymbol y_{obs}~\\}\\). We’ve solved \\(\\{~\\boldsymbol\\theta | \\Sigma, \\boldsymbol y~\\}\\) and \\(\\{~\\boldsymbol\\Sigma | \\boldsymbol{\\theta, y}~\\}\\) already. So add a step to the Gibbs sampler where \\[ \\boldsymbol y_{mis}^{(s)} \\sim p( \\boldsymbol y_{mis} | \\theta^{(s)}, \\Sigma^{(s)},\\boldsymbol y_{obs}) \\] and then use those sampled values for the missing cases for the next update of \\(\\boldsymbol\\theta\\) and \\(\\boldsymbol\\Sigma\\). This is particularly convenient in the MVN model because; If \\(\\mathbf Y = (\\mathbf Y_1, \\mathbf Y_2)\\) has the MVN normal distribution, \\(\\mathbf Y_1 | \\mathbf Y_2 = \\boldsymbol y_2\\) is MVN and \\(\\mathbf Y_2 | \\mathbf Y_1 = \\boldsymbol y_1\\) is MVN. So the ‘data imputing’ step of the Gibbs sampler is particularly straightforward. The result is \\(\\boldsymbol\\theta^{(s)} , \\boldsymbol\\Sigma^{(s)}\\) are a Gibbs sample from \\(p(\\boldsymbol\\theta , \\boldsymbol\\Sigma | \\boldsymbol y_{obs} )\\) 21.2 Generalized linear mixed eﬀects models; 11b We did not work on “11b,” but we built the foundation for this. In “10a” we talked about Generalized Linear regression Models when the response is something other than normal e.g., Poisson regression and Logistic regression(for binary responses). In 11a we talked about hierarchical regression. Our data are obtained by a two-stage sampling plan and there’s a different regression model (possibly different regression parameters) within the different groups of data. What our “11b” unit would have been about is putting those ideas together. That is, hierarchical regression models for non-normal regression. This is called generalized linear mixed models GLMM. 21.3 Improper priors We never did any work with improper priors largely because we were following the text by Hoff and Hoff does not mess with improper priors at all. The Gelman text uses improper priors as a default. There’s a brief discussion in Hoff page 78. The idea is this; Recall that in the case of the normal distribution we could characterize the conjugate prior for \\(\\sigma^2\\) and \\(\\theta\\) by \\(\\theta\\sim\\) Normal\\((\\mu_0, \\sigma^2/\\kappa_0)\\) \\(\\sigma^2 \\sim\\) inverse-gamma\\((\\nu_0/2, ~ \\nu_0\\sigma^2_0/2)\\) \\(\\sigma^2_0 =\\) prior best guess at \\(\\sigma^2\\) and \\(\\nu_0 =\\) “sample size on which that’s based.” Also \\(\\mu_0 =\\) prior best guess at \\(\\theta\\) and \\(\\kappa_0 =\\) “sample size on which that’s based.” What happens if we let \\(\\kappa_0 = 0\\) and \\(\\nu_0 = 0\\). This is a logical question: “What if we have no prior information?” If you look at a chi-square density with degrees of freedom = 0 you get \\(f(x) = c \\times 1 / x\\) for \\(x &gt; 0\\). This is not a probability density because although it’s non-negative for all \\(x\\), it does not have a finite integral. Similarly consider a normal distribution and let the prior variance go to infinity. Suppose I said I want my prior density for \\(\\theta\\) (the mean of a normal distribution) to be \\(p(\\theta) = 1\\) for all \\(\\theta\\), i.e., uniform on \\(-\\infty\\) to \\(+\\infty\\). You can’t do that! that’s not a probability distribution because it’s not finite! But if we do it anyway and say \\(p(\\theta | y) = c \\times p(\\theta) p(y | \\theta),\\) it still works out. Because of this, this has become a popular way to do Bayesian statistics. To routinely employ noninformative priors and if the parameter space is unbounded they’ll generally be improper priors. Def: A prior density \\(p(\\theta)\\) is proper if \\(\\int { p(\\theta) d\\theta } &lt; \\infty\\). A prior distribution is improper if the prior density \\(p(\\theta)\\) has a divergent integral; \\(\\int { p(\\theta) d\\theta }\\) is not finite. However, you can still do this as long as the posterior density \\(p(\\theta | y) = c \\times p(\\theta) p(y | \\theta)\\) as is a proper density. The difficulty is that there’s no guarantee that it is. There are certain models where improper priors lead to proper posteriors. There are other situations where they don’t. It’s not always so easy to know the difference. Example: For most Generalized Linear Models, if we take the improper prior distribution \\(p(\\beta) = constant\\), generally the posterior distribution will still be proper and therefore lead to valid conclusions. Let’s look at the Poisson regression example we did where \\(x =\\) age of female bird and \\(y =\\) number of offspring and see if this is the case. Fitting the model with a normal(0,10) prior. # 52 birds, response is &#39;fledged&#39; -- number of offspring # Predictor variable is age in years (1 to 5) fledged &lt;- c( 3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, 2, 2, 1, 1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, 1, 2, 1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2) age &lt;- c(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1) library(rstan) # description of our model y &lt;- fledged n &lt;- length(y) X &lt;- cbind(rep(1,n), age, age^2) rownames(X) &lt;- 1:n; colnames(X) &lt;- c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;) n &lt;- dim(X)[1] p &lt;- dim(X)[2] stan_model &lt;- &quot; data{ int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; int&lt;lower=0&gt; y[n]; matrix[n,p] X; } parameters{ vector[p] beta; } model{ beta ~ normal(0, 10); // prior on beta y ~ poisson_log(X*beta); } &quot; data &lt;- list(n=n, p=p, y=y, X=X); fit_stan &lt;- stan(model_code=stan_model, data=data, chains=1, iter=5000, warmup=1000, refresh = 0) Results &lt;- summary(fit_stan)$summary[1:3, 4:8]; Results_normal_prior &lt;- round(Results, 2) Let’s fit another Bayesian model, but this time with the prior commented out. Think about what this model says: There is a probability distribution for \\(\\boldsymbol y | \\beta,\\) there are numeric values of \\(y\\) but there are no values of \\(\\beta\\) provided there is no prior distribution specified! What stan does in the absence of a prior distribution is assign a uniform distribution. In this case it’s a uniform distribution on an unbounded space so it’s an improper distribution. What is stan going to do? is it going to crash? Let’s see! stan_model2 &lt;- &quot; data{ int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; int&lt;lower=0&gt; y[n]; matrix[n,p] X; } parameters{ vector[p] beta; } model{ // no prior specified y ~ poisson_log(X*beta); } &quot; data &lt;- list(n=n, p=p, y=y, X=X); fit_stan &lt;- stan(model_code=stan_model2, data=data, chains=1, iter=5000, warmup=1000, refresh = 0) Results &lt;- summary(fit_stan)$summary[1:3, 4:8]; Results_improper_prior &lt;- round(Results, 2) It didn’t crash. Results_improper_prior; Results_normal_prior ## 2.5% 25% 50% 75% 97.5% ## beta[1] -0.75 -0.06 0.25 0.56 1.16 ## beta[2] 0.00 0.47 0.69 0.93 1.46 ## beta[3] -0.27 -0.18 -0.14 -0.10 -0.02 ## 2.5% 25% 50% 75% 97.5% ## beta[1] -0.70 -0.07 0.23 0.53 1.04 ## beta[2] 0.09 0.49 0.71 0.94 1.40 ## beta[3] -0.26 -0.18 -0.14 -0.10 -0.04 They are pretty much the same. Using a vague but proper prior like Normal(0 , 100) and using an improper prior lead to identical conclusions. I would argue that the former is safer because this can happen; You have a model where an improper prior does not lead to a proper posterior but MCMC may still work. You’ll get output, but it won’t be meaningful output and you may not even know it. Cheers! "]]
