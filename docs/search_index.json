[["index.html", "Bayesian Statistics notes from summer 2021 Lecture 1 Belief and Probability 1.1 Example: 1.2 Random Variables 1.3 Binomial distribution 1.4 Poisson distribution 1.5 Continuous Random Variables", " Bayesian Statistics notes from summer 2021 Chisom Onyishi 2021-06-01 Lecture 1 Belief and Probability The following notes, mostly transcribed from Neath(0503,2021) lecture, summarize sections(2.1-2.4) of Hoff(2009). I like to use the chat I will post lecture slides to Courseworks before 9am each class day I will share the slides as I talk and I will type in the chat as I talk My hope is that by typing in the chat as we go along I will sort of reign myself in from going too fast. Also, there’s a printed record of the chat which is nice Gelman is on the third edition and there’s no reason to be looking at the 2nd or 1st edition of Gelman. However, for this course, I recommend get your hands on the book by Peter D. Hoff (2009) “A First Course in Bayesian Statistical Methods.” I will occasionally reference stuff in the Gelman text but the Hoff text is what we’re gonna follow. If you access “Springer link” using the Columbia network you can get a free copy of Hoff’s book by legitimate means! Gelman’s book is also (legitimately) free online because he has posted it! Find a link on Prof Gelman’s Columbia web page! Again, lecture will follow Hoff’s text. We’re starting in Chapter 2 and we’ll finish it or come very close (maybe skipping a few things along the way). We’ll leave some time at the end to talk about a few things that aren’t in Hoff’s book e.g. Stan software (which we’ll introduce toward the end of the course). But we’ll be doing computing from the get-go. Your first assignment (due next week) (not posted yet) will involve computing. I like R, but you may use Python if you know it. The Hoff book has some examples given in R. You know all about probability from the ‘relative frequency’ interpretation. If I roll a die a gazillion times it will land on the 3-side one-sixth of those rolls \\(Pr(3) = 1/6\\). In Bayesian statistics we use probability more generally than that. We use probability in a way that’s consistent with the informal use of probability. E.g., the probability that there will be criminal charges brought on former president is x%. In a strict frequency-based sense it doesn’t even make sense to speak of ‘probability’ for something like this. In a Bayesian sense it does and that is because in Bayesian statistics we use a different (more general) interpretation of probability where the probability of an event represents our degree of belief in that event. In Bayesian statistics any statement about the world can have a probability attached to it. E.g., The probability that climate change is a hoax brought by China and the Deep State. Though we might say this is absurd, some might assign this a probability other than zero. The only rule we’ll have for probabilities in our course is that they be internally consistent i.e., coherent – that they follow the rules of probability (mathematical laws of probability). We will use probability as a measure of our degree of belief in a statement about the world. Forget about probability for a second and just think about this idea of Belief. Let \\(F\\) be a statement about the world \\(Be(F)\\) is our degree of belief in that statement as measured by a numerical value. What’s a reasonable set of requirements on \\(Be()\\) for it to be a reasonable belief function? The higher the value, the higher the degree of belief. Let’s make this more concrete by thinking about bets \\(Be(F) &gt; Be(G)\\) means prefer betting on \\(F\\) to betting on \\(G\\) \\(Be(F | H) &gt; Be(G | H)\\) means if we know \\(H\\) to be true we prefer betting on \\(F\\) to betting on \\(G\\) \\(Be(F | G) &gt; Be(F | H)\\) means if forced to bet on \\(F\\) we prefer to do it under the condition that \\(G\\) is true than that \\(H\\) is true. The following Axioms of belief have been proposed as a set of conditions that any rational belief function must satisfy B1. \\(Be(\\text{not } H | H) \\le Be(F|H) \\le Be(H | H)\\). If \\(H\\) is known to be true there is no other function that I have higher belief in than \\(H\\) itself, and there is no statement that I have lower belief in other than not \\(H\\). B2. \\(Be(F \\text{ or } G | H) \\ge Be(F | H), ~~ Be(F \\text{ or } G | H) \\ge Be(G | H)\\) B3. \\(Be(F \\text{ and } G | H)\\) can be derived from \\(Be(G | H)\\) and \\(Be(F | G \\text{ and } H)\\) We propose as Bayesian statisticians to use probability as our measure of belief. Probability has its own set of axioms. Here they are! P1. \\(0 \\le Pr(F | H) \\le 1\\), where \\(Pr(\\text{ not } H | H) = 0, ~~ Pr( H | H ) = 1\\) so that takes care of B1 P2. If \\(F\\) and \\(G\\) are disjoint events then \\(Pr(F \\text{ or } G | H) = Pr(F | H) + Pr(G | H)\\) P3. \\(Pr(F \\text{ and } G | H) = Pr(G | H) \\times Pr(F | G \\text{ and } H)\\) You can verify that any probability function that satisfies the axioms of probability also satisfies the axioms of belief. So using probability as a language for measuring our belief in statements about the world is justified. And that’s what we do in Bayesian statistics. 1.1 Example: \\(H_j =\\) the event that { randomly selected person is in quartile \\(j\\) of income } \\(j = 1, 2, 3, 4\\) \\(H_1 =\\) { lower \\(25\\%\\) }, \\(H_4\\) = { upper \\(25\\%\\) } Let \\(E\\) = event that { randomly selected person has college degree } From survey data (a very large survey; the General Social Survey for a particular year). We have \\(Pr(E | H_j)\\) for each of \\(j = 1, 2, 3, 4\\) \\(.11 + .19 + .31 + .53 = 1.14\\). You might think uh oh. However, not uh oh at all. These numbers aren’t expected to add to \\(1.\\) These numbers don’t add to anything particularly meaningful. Using Bayes rule, we can obtain \\(Pr(H_j | E)\\) for each \\(j = 1, 2, 3, 4\\) and those had better add to \\(1\\). Let’s do \\(Pr(H_3 | E)\\); the probability that a person is in the 3rd quartile of income (between \\(50\\)th and \\(75\\)th percentile) given that they have a college degree. Using Bayes’ rule \\[ \\begin{aligned} Pr(H_3 | E) &amp;= \\frac{Pr(H_3 \\text{ and } E)}{Pr(E)}\\\\ &amp;=\\frac{Pr(H_3)Pr(E | H_3)}{Pr(E)}\\\\ &amp;= \\frac{Pr(H_3)Pr(E | H_3)}{Pr(H_1 \\text{ and } E) + Pr(H_2 \\text{ and } E) + Pr(H_3 \\text{ and } E) + Pr(H_4 \\text{ and } E)}\\\\ &amp;= \\frac{.25 \\times .31}{0.28} = \\frac{0.0775}{0.28} = 0.272 \\end{aligned} \\] \\[ \\begin{aligned} Pr(H_1 \\text{ and } E) = Pr(H_1 ) Pr(E | H_1) = .25 \\times .11\\\\ Pr(H_2 \\text{ and } E) = Pr(H_2 ) Pr(E | H_2) = .25 \\times .19\\\\ Pr(H_3 \\text{ and } E) = Pr(H_3) Pr(E | H3) = .25 \\times .31\\\\Pr(H_4 \\text{ and } E) = Pr(H_4) Pr(E | H_4) = .25 \\times .53 \\end{aligned} \\] \\(Pr(E) =\\) the sum of these four products. I get \\(Pr(E) = 0.285\\) This is a problem in Bayesian inference! Did you think this was just a fun little probability exercise? No no no no no, this was serious business. This was our first real Bayesian learning problem. I tell you I have a randomly selected person from this survey. What is your belief about their income? You think they’re in 1st, 2nd, 3rd or 4th quartile? Your belief is { \\(.25 , .25 , .25, .25\\) }. Now I tell you they have a college degree so you will update your belief! You will update your belief which is measured by a probability which is updated using Bayes rule. Bayesian inference: is the discipline of updating our belief about the world based on further observation of the world. If we know they have college degree our belief is not { \\(.25, .25, .25, .25\\) } anymore it’s skewed more toward the higher income groups { \\(.09, .17, .27, .47\\) }. The \\(H_k\\)’s in this set-up are usually ‘states of nature’ and the \\(E\\) in this set-up is the observed data 1.2 Random Variables In Bayesian statistics a random variable is any numerical quantity whose value is uncertain that includes things like experimental results (before the experiment is conducted) survey results (before the sample is taken). But it also includes model parameters states of nature. Let \\(Y\\) be a random variable \\(\\mathcal{Y}\\) is the set of possible values. If the set of possible values is a countable set { \\(y_1, y_2, \\ldots\\) }, then \\(Y\\) is a discrete random variable. We can compute \\(Pr(Y = y)\\) for any value of \\(y\\). We’ll define the pdf \\(p(y) = Pr(Y = y)\\). \\(Y\\) is the random variable, \\(y\\) is a possible realized value for \\(Y\\). Note we are using the pdf (density) terminology even for a discrete r.v. (Hoff, 2009). If you know the pdf you know \\(p(y) = Pr(Y = y)\\) for every possible value of \\(y\\) then you know the whole probability distribution. Two key properties for the pdf of a discrete r.v. \\(0 \\le p(y) \\le 1\\) They sum to \\(1\\) The two most important discrete probability distributions are the Binomial and Poisson distributions. Here are their definitions: 1.3 Binomial distribution \\[ Pr(Y=y|\\theta) = \\texttt{dbinom}(y,n,\\theta)=\\left(\\begin{array}{l}n\\\\y \\end{array}\\right)\\theta^y(1-\\theta)^{n-y} \\] \\(Y\\) counts the number of successes in \\(n\\) independent trials where the probability of success on each trial is \\(\\theta\\). Then \\(Pr(Y = y | \\theta)\\) is the probability of \\(y\\) successes in \\(n\\) trials = probability of \\(y\\) successes and \\(n-y\\) failures. Well the probability of \\(y\\) successes followed by \\(n-y\\) failures is \\(\\theta^y \\times (1-\\theta)^{n-y}\\). But \\(Pr(Y = y| \\theta)\\) is the probability of ANY possible sequence of \\(y\\) successes and \\(n-y\\) failures. There are “\\(n\\) choose \\(y\\)” such sequences; there are “\\(n\\) choose \\(y\\)” ways to arrange a sequence of \\(y\\) successes and \\(n-y\\) failures. Each sequence has the same probability \\(\\theta^y \\times (1-\\theta)^{n-y}\\). The probability that it’s one of these sequences is the sum of those probabilities \\(\\left(\\begin{array}{l}n\\\\y\\end{array}\\right)\\theta^y(1-\\theta)^{n-y}\\). We’ll use this dbinom notation for the binomial probability function which is also the R function to calculate these! Calculating binomial probabilities in R is easy. Suppose \\(n = 60\\) and \\(\\theta = .20\\). This is the binomial distribution; the probability distribution for number of successes in \\(60\\) trials where the success probability is \\(0.20\\). 1.4 Poisson distribution \\[ Pr(Y = y|\\theta) = \\texttt{dpois}(y,\\theta) = e^{-\\theta} \\frac{\\theta^y}{y!} \\] \\(\\theta =\\) mean (expected value of \\(Y\\)). \\(Y\\) counts the number of events. \\(\\theta\\) is the expected number. \\(y = \\{0, 1, 2, \\ldots\\}\\) In R dpois does this! Let’s do a Poisson distribution with expected value of \\(12\\) (will look not so different from that binomial distribution. That’s the Poisson distribution with expected value of \\(12\\). They’re pretty similar. But careful on plots like these. When we make comparative plots like this we should take care to put them on the same scale! You can see that the binomial distribution assigns higher probabilities close to the expected value whereas the Poisson distribution gives more probability mass farther away from the expected value. par(mfrow = c(1,2)) y &lt;- 0:30 p &lt;- dbinom(y, size = 60, prob = 0.2) plot(y, p, pch=19, main = &quot;Binomial(60, 0.2)&quot;, ylim = c(0, 0.13)) segments(y,0,y,p) p &lt;- dpois(y, 12) plot(y, p, pch = 19, main = &quot;Poisson(12)&quot;, ylim = c(0, 0.13)) segments(y,0,y,p) Figure 1.1: Binomial and Poisson Distributions They’re applicable to different situations. The binomial distribution is most appropriate when we have a fixed number of trials. However, if you’re not clear on what \\(n\\) and \\(\\theta\\) should be but you know what \\(n \\times \\theta\\) should be then you can’t do Binomial\\((n, \\theta)\\) but you can do Poisson\\((n\\times\\theta)\\). If there’s no upper bound then the binomial distribution is not appropriate (you can’t have \\(150\\) successes in \\(100\\) trials - in this case there’s no upper bound, and a Poisson model is more appropriate). 1.5 Continuous Random Variables Where the set of possible values is not a countable set and it’s, say, the whole real line. \\(F(y)\\) is the cumulative probability up to and including \\(y\\) \\(F(y) = Pr(Y \\le y)\\). If I know the cdf (cumulative distribution function) of a random variable, I know the whole distribution. Because I can find \\(Pr( a &lt; Y \\le b) = F(b) - F(a).\\) If \\(Y\\) is a discrete random variable it’s cdf is a step function. If \\(F\\) is a monotone continuous function then \\(Y\\) is a continuous random variable. In that case (maybe) there exists a function \\(p()\\) such that for any set \\(A,\\) \\(Pr(Y \\in A) = \\int_A { p(y) dy}.\\) \\(p(y)\\) is called the density function or pdf. If \\(p()\\) is the density function of a continuous random variable then \\(p(y) \\ge 0\\) for all \\(y\\) and \\(\\int_{-\\infty}^\\infty p(y) dy = 1\\). This is slightly different from the conditions on the pdf of a discrete random variable. If \\(p(y\\)) is the pdf of a continuous random variable, it is possible that \\(p(y) &gt; 1\\). \\(p(y)\\) does not represent a probability. The areas under the density curve are probabilities as long as that total area under the curve is \\(1\\) then it’s a valid probability distribution. The most important continuous distribution is the Normal distribution. Let’s look at the pdf and cdf of a Normal dist! In R these are computed by dnorm (for the density) and pnorm (for the cdf). 1.5.1 Example: Normal(\\(\\mu = 10.75, \\sigma=0.8\\)) par(mfrow = c(1,2)) mu &lt;- 10.75 sigma &lt;- 0.8 y &lt;- seq(7.5, 14, 0.05) # pdf p &lt;- dnorm(y, mu, sigma) plot(y, p, type = &quot;l&quot;, lwd = 2, main = &quot;Normalpdf(10.75, 0.8)&quot;) segments(y,0,y,p) #cdf F &lt;- pnorm(y, mu, sigma) plot(y, F, type = &quot;l&quot;, lwd = 2, main = &quot;Normalcdf(10.75, 0.8)&quot;) segments(y,0,y,F) Figure 1.2: Normal pdf and cdf The “bell curve” describes the density. The cdf is an S-curve. The mean of this distribution is \\(10.75\\). The density is symmetric about that value, the mode of this distribution is \\(\\mu = 10.75\\) and the median of this distribution is \\(\\mu = 10.75\\). Speaking of means and modes and medians, let’s define these things! The mean is the center of mass. It’s the weighted average of the possible values weighted by their probabilities. The mode is the value with the highest probability (or highest probability density for a continuous rv). The median is the \\(.50\\) quantile. The point where half the probability is to the left and half the probability is to the right. These are all measures of “location.” If we want a measure of the “spread” of a distribution we can use the standard deviation. Def: The variance of a random variable is defined by \\(\\text{Var}(Y) = E{ ( Y -E(Y) )^2 } = E(Y^2) - E(Y)^2\\) expected squared distance from the mean value. Take the square root of this (to back to the original scale) and call that quantity the standard deviation. We can also measure location and spread both using quantiles! For discrete distributions quantiles are weird. The \\(\\alpha\\)-quantile of a distribution is the value \\(y_{\\alpha}\\) such that \\(F(y_{\\alpha}) = \\alpha\\). Consider the normal cdf below. Where is the \\(.75\\) quantile? plot(y, F, type = &quot;l&quot;, lwd = 2, main = &quot;Normalcdf(10.75, 0.8)&quot;) abline( h = 0.75) abline(v = qnorm(0.75, mu, sigma)) Figure 1.3: Normal cdf with quantile In R, I can find normal quantiles using the qnorm function. With discrete distributions the cdf is a step function it jumps, so there’s not a uniquely defined point where the cdf curve passes through \\(.75\\) say. A \\(50\\%\\) probability interval for the distribution of \\(Y\\) is \\((y_{.25},~y_{.75}).\\) A \\(95\\%\\) probability interval is \\((y_{.025}, ~ y_{.975}),\\) where \\(y_\\alpha\\) represents \\(\\alpha\\) quantile of \\(y\\). "],["exchangeability.html", "Lecture 2 Exchangeability 2.1 Discrete joint distributions 2.2 Bayes’ rule and parameter estimation 2.3 Independent random variables 2.4 Exchangeability", " Lecture 2 Exchangeability The following notes, mostly transcribed from Neath(0504, 2021) lecture, summarize sections(2.5-2.8) of Hoff(2009). 2.1 Discrete joint distributions We will talk about joint probability distributions. \\(Y_1\\) and \\(Y_2\\) are two discrete random variables. We can define a joint probability density function (joint pdf) \\(p_{Y_1Y_2}(y_1, y_2) = Pr(Y_1 = y_1 \\text{ and } Y_2 = y_2)\\). We will also be interested in the marginal distributions e.g., the pdf of \\(Y_1\\) only. We will use subscripts on the \\(p\\) to be clear what density we’re talking about. I get the marginal of \\(Y_1\\) from the joint of \\((Y_1, Y_2)\\) by summing over the values that \\(Y_2\\) can take. We can also speak of the conditional density of \\(Y_2\\) given \\(Y_1\\) which follows immediately from the definition of conditional probability! Recall that \\(Pr( B | A ) = Pr(A \\text{ and } B) / Pr(A)\\). Conditional density of \\(Y_2 | Y_1=y_1\\) equal the joint density of \\(Y_1\\) and \\(Y_2\\) divided by the marginal of \\(Y_1\\). Given the joint density one can derive both marginals and both conditionals. Given the marginal of \\(Y_1\\) and the conditional of \\(Y_2\\) given \\(Y_1\\) one can construct the joint density of \\((Y_1, Y_2)\\). Given the two marginals, marginal of \\(Y_1\\), marginal of \\(Y_2\\), it is not possible to construct the joint density (knowing the row and columns totals doesn’t mean you know how to complete a contingency table). The joint distribution contains all the information the marginals do not! All the joint probabilities, all the marginal probabilities, and all possible conditional probabilities. We will not always use these subscripts on our \\(p\\)’s. We will just use a lower case \\(p\\) to denote a probability density. We’ll know the ‘density of what’ by the argument to the function! If we are talking about a pair of continuous random variables then the pdf’s really are probability DENSITY functions they’re not actually probabilities at all. We get probabilities from them by solving integrals. Proposition: \\(Y_1\\) and \\(Y_2\\) are two arbitrary rvs (random variables) with a joint distribution. That joint distribution is completely specified by the joint cdf. The ‘rigorous’ definition of a joint distribution starts with the joint cdf. The joint pdf is the function whose integral gives the joint cdf. Given a joint distribution of (\\(Y_1, Y_2\\)), the marginal distribution of \\(Y_1\\) is found by (summing over \\(Y_2\\) values in the discrete case) (in the continuous case it’s integrating over all the \\(Y_2\\) values). Although probability densities are not strictly probabilities, conditional densities can be defined analogously. The conditional density of \\(Y_2 | Y_1 = y_1\\) is found by joint density of \\(Y_1\\) and \\(Y_2\\) divided by marginal density of \\(Y_1\\). We can define a pair of rvs so that \\(Y_1\\) is discrete and \\(Y_2\\) is continuous. We still have \\(p(y_1, y_2) = p(y_1) p(y_2 | y_1)\\) and \\(p(y_1, y_2) = p(y_2) p(y_1 | y_2)\\). Probabilities are found by summing over values of the discrete one and integrating over values of the continuous one. 2.2 Bayes’ rule and parameter estimation Let \\(\\theta =\\) proportion of people who have a certain characteristic. E.g., public opinion poll: Approve of performance of President Biden? Yes or No? \\(\\theta\\) would be the proportion of people who would answer yes, \\(0 &lt; \\theta &lt; 1\\) Is \\(\\theta\\) discrete or continuous? If there are 300 million people in the population of interest then the possible values are 0, 1/300,000,000, 2/300,000,000, …, 299,999,999/300,000,000, 1. But that’s kind of stupid. It’s continuous. It makes most sense to treat \\(\\theta\\) as continuous, \\(0 &lt; \\theta &lt; 1\\). What if \\(Y =\\) number of people sampled who answer YES to “Approve of President Biden?” The population size is millions the sample size is hundreds? Here it does make sense to use a discrete probability model for \\(Y\\). \\(\\theta\\) is the value we want to know, \\(Y = y\\) is the quantity we’re able to observe. Statistical inference is making induction about \\(\\theta\\) based on observation of \\(Y = y\\). For a classical (non-Bayesian) analysis this requires a probability model for \\(Y\\) that depends on \\(\\theta\\). In Bayesian statistics it requires a joint probability model for \\((Y , \\theta)\\)! Where frequentist statistics treats \\(\\theta\\) as fixed (unknown but fixed) Bayesian statistics treats it as a random variable. The justification for this? Our belief about the value of \\(\\theta\\) is measured by a probability dist! Start with what we believe about \\(\\theta\\) before we’ve observed any data. Those beliefs determine the marginal distribution \\(p(\\theta)\\). We call this the prior distribution. Then we have (just as in non-Bayesian statistics) probability distribution for \\(Y\\) that depends on \\(\\theta\\). In Bayesian statistics we make it explicit that this is a CONDITIONAL probability, conditional on the value of \\(\\theta\\). After data are observed we want to update our beliefs about \\(\\theta\\). Use Bayes’ rule! \\(p(\\theta | y) = p(\\theta, y) / p(y) = p(\\theta) p(y | \\theta) / p(y)\\). This is called the posterior distribution (because it comes after observing the data). If \\(\\theta_a\\) and \\(\\theta_b\\) are two possible values of \\(\\theta\\) our belief in \\(\\theta_a\\) versus \\(\\theta_b\\) is defined by the ratio of their posterior probabilities (or posterior densities). That ratio depends on \\(p(\\theta_a) / p(\\theta_b)\\) ratio of ‘prior probabilities’ and \\(p(y | \\theta_a) / p(y | \\theta_b)\\) which is the ratio of the ‘likelihoods.’ Note! One need not calculate the marginal probability \\(p(y)\\) to calculate this ratio! That’s an important observation! Bayes’ rule tells us \\(p(\\theta | y) = p(\\theta) p(y | \\theta) / p(y)\\). Thinking of this thing as a function of \\(\\theta\\) we can write \\(p(\\theta | y) = c \\times p(\\theta) \\times p(y | \\theta)\\). Are you familiar with this “proportional to” symbol? You will be! We say \\(f(x) \\propto g(x)\\) if there exists a constant \\(c &gt; 0\\) such that \\(f(x) = c \\times g(x) ~\\forall ~x\\). \\(p(\\theta | y)\\) is a probability density. It integrates to 1. The function \\(p(\\theta) p(y | \\theta)\\) (a function of \\(\\theta\\)) integrates to something positive. Dividing by that positive thing would give us the normalized posterior density. Not dividing by that thing gives us an unnormalized posterior density which is just as good. Often solving for \\(p(y)\\) explicitly is impossible (or nearly impossible) :( But we’ll see in this course, turns out that doesn’t really matter the important information about the posterior is contained in the numerator of the posterior expression that is \\(p(\\theta ) p(y | \\theta)\\) which are not hard to solve :) 2.3 Independent random variables Continuing with probability review. \\(Y_1, Y_2, \\ldots , Y_n\\) are independent if their joint probability density factors into the product of their marginal probability densities. If these densities are specified conditionally on the value of \\(\\theta\\) (where \\(\\theta\\) is a random variable) we say \\(Y_1, Y_2, \\ldots, Y_n\\) are conditionally independent given \\(\\theta\\). If \\(Y_1, Y_2, \\ldots, Y_n\\) are generated from a common process, then the marginal densities of the \\(Y_i\\) (conditional on \\(\\theta\\)) are all the same. If \\(\\theta =\\) Biden’s true approval rate \\(Pr(Y_1 = 1 | \\theta) = Pr(Y_2 = 1 | \\theta) = … = \\theta\\). In such a case we’ll say \\(Y_1, Y_2, …,\\) are conditionally iid. 2.4 Exchangeability In Bayesian statistical models we generally assume conditional independence which does not imply independence it implies a different condition called exchangeability. \\[ Y_i= \\begin{cases} 1 \\text{ if subject answers yes }\\\\ 0 \\text{ if subject answers no (or doesn&#39;t answer) } \\end{cases} \\] We will pick 10 survey respondents out of very very many (thousands) at random \\(Y_i = 1\\) if the \\(i\\)th one answers yes. There are \\(2^{10} = 1024\\) possible realizations of \\((y_1, y_2, …, y_{10})\\). What’s the probability for each? Here are 3 of those 1024 possible sequences \\[ \\begin{array}{l} p(1, 0, 0, 1, 0, 1, 1, 0, 1, 1) = ?\\\\ p(1, 0, 1, 0, 1, 1, 0, 1, 1, 0) = ?\\\\ p(1, 1, 0, 0, 1, 1, 0, 0, 1, 1) = ?\\\\ \\end{array} \\] I’m thinking their probabilities should all be equal. Note that each of these sequences contains six \\(1&#39;\\)s and four \\(0&#39;\\)s. Does it seem reasonable to assume Pr(yes, yes, no) = Pr(yes, no, yes) = Pr(no, yes, yes)? I’d say it does! The technical term for this condition is exchangeability. \\(y_1, y_2, …, y_n\\) are a sequence of values then the probability of observing \\(Y_1 = y_1, Y_2 = y_2, … , Y_n = y_n\\) and the probability of observing \\(Y_1 = y_2, Y_2 = y_1\\), etc are the same. This is called exchangeability. Exchangeability holds in models where the subscript label contains no information about the outcome. Does exchangeability imply independence? NO. \\(Y_1, Y_2 , …\\) are conditionally independent given \\(\\theta\\), but unconditionally they’re not independent. Think about \\(Pr(Y_{10} = 1).\\) My answer to this would be whatever is my best guess of the overall success proportion \\(\\theta\\). What about \\(Pr(Y_{10} = 1 | Y_1 = Y_2 = … = Y_9 = 1)?\\) Suppose first probability is \\(a\\), second probability is \\(b\\). I don’t think we want \\(a = b\\)! In fact, \\(a&lt;b\\). What about conditionally on \\(\\theta\\), given that \\(\\theta\\) is the true success probability (for the population)? \\(Pr(Y_{10} = 1 | \\theta) \\approx \\theta?\\) yes \\(Pr(Y_{10} = 1 | Y_1, …, Y_9 , \\theta) \\approx \\theta?\\) yes These are 10 independent draws from a large population. Imagine a huge bowl of jelly beans, green ones and red ones. I know the proportion that are green is \\(\\theta.\\) \\(Pr(10\\)th pick is green | \\(\\theta) = \\theta,~ Pr(10\\)th pick is green | first \\(9\\) picks, \\(\\theta)\\) is about \\(\\theta\\). It’s a huge bowl so sampling without replacement is essentially the same thing as sampling with replacement. Given that 80% of this population is happy. If only 4 of the first 9 answered yes that doesn’t change my probability for the 10th, it’s still going to be .80. True or false: \\(Y_1\\) and \\(Y_2\\) and … and \\(Y_9\\) and \\(Y_{10}\\) are independent. We’re gonna see in a minute this statement is false. The true statement is \\(Y_1, Y_2, … , Y_9, Y_{10}\\) are CONDITIONALLY independent given \\(\\theta.\\) \\(Pr(Y_i = 1 | \\theta , Y_j,~ j \\neq i) = \\theta.\\) Probability of zero is \\(1-\\theta.\\) That’s the joint probability conditionally on \\(\\theta.\\) Unconditionally it’s… You take the conditional probability multiply by the marginal of \\(\\theta\\) integrate \\(\\theta\\) out of it! So we see \\(Y_1, Y_2, ... Y_n\\) are exchangeable. They are not iid because they’re not independent, because unconditionally on \\(\\theta,\\) \\(Y_{10}\\) is not marginally independent of \\(Y_1, Y_2, …, Y_9\\) because \\(Y_{10}\\) depends on \\(\\theta\\) and \\(Y_1, Y_2, …, Y_9\\) contain information about \\(\\theta\\). We do not have \\(a=b\\) above, but we do have exchangeability. That is, joint distribution is preserved under re-labeling of the subscripts (permutation of labels). Proposition: If \\(\\theta \\sim p(\\theta),\\) \\(Y_1, … Y_n | \\theta\\) are iid then \\(Y_1, …, Y_n\\) are exchangeable. The proof is straightforward it’s given in Section 2.7 or 2.8 of Hoff. The converse is also true. i.e., If \\(Y_1, …, Y_n\\) are an exchangeable sequence of rvs then there exists a prior distribution \\(\\theta\\) and conditional distribution \\(Y_i | \\theta\\) such that \\(Y_1, …, Y_n | \\theta\\) are iid \\(p(y_i | \\theta)\\) where \\(\\theta \\sim p(\\theta)\\). That result is known as di Finetti’s theorem. Why is this important? This means that any situation where modeling observed values as being exchangeable is appropriate then a Bayesian statistical analysis is also appropriate. "],["binomial-1.html", "Lecture 3 Binomial 3.1 Example - Exchangeable binary data 3.2 The beta distribution 3.3 Binomial distribution 3.4 Combining information 3.5 Prediction", " Lecture 3 Binomial The following notes, mostly transcribed from Neath(0504, 2021) lecture, summarize section 3.1 of Hoff(2009). 3.1 Example - Exchangeable binary data The quantity of interest is the proportion of women that rate themselves as generally happy. Denote this quantity as \\(\\theta\\), \\(0 &lt; \\theta &lt; 1\\). Our data is \\(n=129\\) women surveyed. \\[ Y_i = \\begin{cases} 1 \\text{ if } i\\text{th woman answered yes}\\\\ 0 \\text{ otherwise} \\end{cases} \\] \\(\\sum{y_i} =118\\)(number of \\(1\\)s). We describe our prior belief (before observing the data) by a prior probability distribution \\(p(\\theta)\\). In this example we’re saying \\(\\theta \\sim \\text{Uniform}(0, 1)\\) prior density \\(\\implies p(\\theta) = 1\\). Let \\(\\theta =\\) proportion of ALL women age 65+ who would answer yes. Binary means same thing as Bernoulli \\((1\\) with probability \\(\\theta\\), \\(0\\) with probability \\(1-\\theta)\\). Conditonal on \\(\\theta\\) the probability of any particular sequence of \\(0\\)s and \\(1\\)s is \\(\\theta^{\\sum{y_i}} \\times (1-\\theta)^{n-\\sum{y_i}}.\\) What do we know about \\(\\theta?\\) Our belief about \\(Y_1, …, Y_n\\) are determined by our beliefs about \\((1)~\\theta = \\sum_{i=1}^NY_i/N \\text{ and }~ (2) ~ p(y_1, \\ldots, y_n|\\theta)\\). The second one would be the bernoulli distribution! The first one is not resolved. What do we believe about \\(\\theta\\) prior to observing data? We know \\(0 &lt; \\theta &lt; 1.\\) Our belief about where between \\(0\\) and \\(1\\) is most likely to be is correctly described by some probability distribution on the interval \\([0, 1]\\). So Let’s say \\(\\theta \\sim \\text{Uniform}(0, 1).\\) This would say \\(Pr(a &lt; \\theta &lt; b) = b-a\\) for any \\(0 &lt; a &lt; b &lt; 1.\\) Okay fine. The prior density is \\(p(\\theta) = 1, ~ 0 &lt; \\theta &lt; 1.\\) Do Bayes’ rule to get the posterior \\(p(\\theta | y_1, y_2, \\ldots, y_n) =p(y | \\theta)p(\\theta)/p(y) = c \\times p(y | \\theta)\\). The denominator is constant because it doesn’t depend on \\(\\theta\\)! We have a general result here: Under a uniform prior distribution the posterior is proportional to the likelihood. Let’s proceed. The data are: \\(n = 129, ~118\\) answered Yes I am generally happy \\(11\\) did not. The probability of the specific sequence of values observed \\((\\)conditional on the value of \\(\\theta)\\) i.e., \\(p(y|\\theta)= \\theta^{118} \\times (1 - \\theta)^{11}.\\) Note here \\(y_1, y_2, …, y_{129}\\) represents a particular sequence of 118 1s and 11 0s. So we have \\(p(\\theta | y) = c \\times p(y | \\theta) = c \\times \\theta^{118} \\times (1 - \\theta)^{11}\\) so a plot of \\(p(y | \\theta)\\) versus \\(\\theta\\) tells us what the posterior distribution looks like! n &lt;- 129 sum.y &lt;- 118 theta &lt;- seq(0, 1, .001) x = expression(theta) y = expression(p(y*&#39;|&#39;*theta)) plot(theta, theta^sum.y * (1-theta)^(n-sum.y), type=&quot;l&quot;, lwd=2, col=&quot;red&quot;, ylab=y, xlab = x); grid(); abline(v = 118/129) #118 yeses -- the mode Figure 3.1: unnormalized posterior Our belief about \\(\\theta\\) based on 118 yeses out of 129 asked is; \\(0.80 &lt; \\theta &lt; 1\\) Note: The shape here is correct for the posterior distribution, but the values on the y-axis is not as we are missing the constant factor in \\(p(\\theta | y) = c \\times p(y | \\theta).\\) This curve does not integrate to 1 so it’s not a probability density but it does uniquely define a probability density by its shape and THAT probability density is indeed the posterior of \\(\\theta.\\) If the prior is uniform, we get a similar conclusion as a frequentist statistician although the interpretations are not the same. 3.2 The beta distribution The Beta distribution is a probability distribution on \\([0, 1].\\) Hey! \\(\\theta\\) lives on \\([0, 1]\\). We can use the beta distribution as our prior for \\(\\theta\\) ! Remember the Gamma function? For integer - value \\(x,~\\text{Gamma}(x) = (x-1)!\\) For noninteger it effectively interpolates between those factorials . 3.2.1 Properties of the beta distribution It has two parameters \\(a\\) and \\(b, ~a\\) describes tendency toward 1, \\(b\\) describes tendency toward 0, mean of this distribution is \\(a / (a+b)\\) variance is \\(ab/(a+b+1)(a+b)^2.\\) If \\(a\\) and \\(b\\) are big, the beta distribution is has a low variance (has a high peak). If \\(a\\) and \\(b\\) are not big then the beta distribution is more spread out. Mode is the value that maximizes our posterior belief! So if you want to report a single - number best guess at \\(\\theta\\) this would be a sensible choice. A plot of the posterior, \\(\\theta | y_1, …, y_n \\sim \\text{Beta}(119, 12)\\) post &lt;- dbeta(theta, sum.y+1, n-sum.y+1) plot(theta, post, type=&quot;l&quot;, lty=1, lwd=2, ylab=&quot;&quot;, xlab = x); lines(theta, rep(1, length(theta)), col=&quot;gray&quot;, lwd=2); legend(&quot;topleft&quot;, inset=.05, lwd=2, col=c(&quot;gray&quot;,&quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;));grid() Figure 3.2: the posterior density, with uniform(0,1){ Beta(1,1) } prior also shown This curve is the same as the first one above. But this one is a probability density. It has area under curve = 1. The light gray curve (the flat line) is the uniform density. That’s our prior belief about \\(\\theta\\). So we purposely chose a prior that did not make any super strong assumptions. We don’t want to pretend we know more than we do. With a uniform prior, the posterior will depend mostly on the data. In the prior our belief about \\(\\theta\\) was weak, in the posterior we have pretty strong beliefs! Based on the data observed! Our posterior belief about \\(\\theta\\) is entirely contained in ratios \\(p(\\theta_a | y_1,\\ldots,y_n) / p(\\theta_b | y_1,\\ldots,y_n)\\), because you can do this for any two values, if you have a way to compare any two possible values of \\(\\theta\\) you have a way to do complete inference about \\(\\theta\\). \\[ \\frac{p(\\theta_a | \\boldsymbol{y})}{p(\\theta_b | \\boldsymbol{y})} = \\bigg(\\frac{\\theta_a}{\\theta_b}\\bigg)^{\\sum y_i} \\bigg(\\frac{1-\\theta_a}{1-\\theta_b}\\bigg)^{n-\\sum y_i} \\frac{p(\\theta_a)}{p(\\theta_b)} \\] Look what happens to this ratio. It depends on the data \\(y_1, …, y_n\\) only through their sum, \\(\\sum{ y_i }\\). Any posterior belief about \\(\\theta\\) depends on the data only through the total value \\(\\sum{y_i}\\), This means that \\(\\sum{y_i}\\) is a sufficient statistic for the bernoulli sampling model (likelihood) and uniform prior. Another way to say this; If you observe \\(y = (1, 0, 0)\\) and I observe \\(y = (0, 0, 1)\\) and we use the same prior and likelihood our inference will be exactly the same because these two data sets have the same \\(\\sum{y_i}\\). If inference about \\(\\theta\\) depends on the data only through the total value let’s just call that our data. Notation: \\(Y_i = 1\\) or \\(0\\) for the \\(i\\)th observation, \\(Y = Y_1 + … + Y_n\\). Then conditionally on \\(\\theta, ~~ Y | \\theta \\sim \\text{Binomial}( n, \\theta)\\). 3.3 Binomial distribution \\(Pr(\\sum{Y_i} = y) = \\binom{n}{y} \\times \\theta^y \\times (1-\\theta)^{n-y}\\) Without the \\({n \\choose y}\\) we have the probability of any particular sequence of \\(y\\) 1s and \\(n-\\)y 0s. But what we want is just the probability of one of those. \\(E(Y | \\theta) = n \\times \\theta\\) var\\((Y | \\theta) = n \\times \\theta \\times (1-\\theta)\\) 3.3.1 Posterior inference for a binomial sampling model So we have a prior density \\(p(\\theta)\\) we have a sampling model \\(Pr(Y = y | \\theta) =\\) dbinom\\((y , n , \\theta)\\). From these we use Bayes’ rule to compute the posterior! \\[ \\begin{aligned} p(\\theta \\mid y) &amp;=\\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} \\\\ &amp;=\\frac{\\left(\\begin{array}{l} n \\\\ y \\end{array}\\right) \\theta^{y}(1-\\theta)^{n-y} p(\\theta)}{p(y)} \\\\ &amp;=c(y) \\theta^{y}(1-\\theta)^{n-y} p(\\theta) \\end{aligned} \\] \\(c(y) = {n \\choose y} / p(y)\\),which is a function of \\(y\\). Can we evaluate this? The answer is yes. But it would involve hard calculus. But we don’t need to. The key point here is: \\(c(y)\\) does not depend on \\(\\theta, \\implies p(\\theta | y) \\propto \\theta^y (1-\\theta)^{n-y} p(\\theta).\\) We know the shape of the posterior density! we don’t know it exactly i.e, the scale but we know the shape. Let’s take the uniform prior distribution \\(p(\\theta) = 1\\). We now have \\(c(y)\\) in an explicit form involving gamma functions( 03a slide 15 ) therefore we have the exact form of \\(p(\\theta | y)\\). We have just demonstrated \\(\\theta | y \\sim \\text{Beta}( y+1, n-y+1)\\). Recall what we noted about the Beta\\((a, b)\\) dist. \\(a\\) measures a tendency toward 1, \\(b\\) measures a tendency toward \\(0 ~(\\)or more accurately, \\(a/(a+b)\\) and \\(b/(a+b))\\). We’ve seen that this posterior results from the “independent Bernoulli’s” model it also results from the Binomial model. Thus confirming the sufficiency result. Intuitively speaking, you can make a note of who answered yes and who answered no in the happiness question but it doesn’t matter because all we need is the sum of the yeses. Uniform\\((0, 1) = \\text{Beta}(1,1)=\\) dbeta\\((\\theta, 1, 1) = 1, ~~ \\forall ~ 0 &lt; \\theta &lt; 1\\). We’ve demonstrated that if \\(\\theta \\sim \\text{Beta}(1,1), ~~ Y | \\theta \\sim \\text{Binomial}(n, \\theta),\\) then \\(\\theta | y \\sim \\text{Beta}(1 + y , 1 + n - y)\\). It looks like the Beta\\((1, 1)\\) prior combines with \\(y\\) successes and \\((n-y)\\) failures to give a Beta posterior with \\(1+y\\) and \\(1+n-y\\) parameters \\(\\implies p(\\theta|y) \\sim \\text{Beta}(a+y, b+n-y) =\\) dbeta\\((\\theta, a+y, b+n-y).\\) 3.3.2 Conjugacy Key result: If \\(\\theta \\sim \\text{Beta}(a, b)\\) and \\(Y | \\theta \\sim \\text{Binomial}(n , \\theta)\\) then \\(\\theta | y \\sim \\text{Beta}(a + y, b + n-y)\\). This situation where the posterior distribution is of the same family of distributions as the prior is called conjugacy. Specifically we say the conjugate prior for a Binomial sampling model is the Beta distribution. Conjugate distributions are characterized by nice math. Things work out nicely for conjugate priors, things cancel out etc. Priors should reflect our prior belief. If that’s well described by a conjugate distribution GREAT otherwise no reason to restrict attention to conjugate priors in Bayesian analyses. 3.4 Combining information The prior \\(p(\\theta)\\) is what we believed before we saw the data. The likelihood \\(p(y|\\theta)\\) contains what we learn from the data. Therefore the posterior should in some sense combine what we knew before with what we just learned. It does! Posterior combines prior and likelihood; \\(p(\\theta | y) = c \\times p(\\theta) \\times p(y | \\theta)\\). Let’s demonstrate this for the “beta-binomial” model where \\(\\theta | y \\sim \\text{Beta}( a + y , b + n-y).\\) \\[\\begin{aligned} \\text{Prior mean } = E(\\theta) &amp;= \\frac{a}{a+b}\\\\ \\text{Sample average is } &amp; y/n \\end{aligned}\\] \\[\\text{Posterior mean } = \\frac{a+y}{a + b + n} = \\frac{a+b}{a+b+n} \\cdot\\frac{a}{a + b} + \\frac{n} {a+b+n} \\cdot \\frac{y}{n}\\] What this equation demonstrates is that posterior expectation = weighted average of prior expectation and sample average. \\(a + b\\) is the weight given to the prior, \\(n\\) is the weight given to the data. Data contributes \\(y\\) successes and \\(n-y\\) failures (in \\(n\\) trials). \\(a\\) plays the exact same role as \\(y, ~b\\) plays the exact same role as \\(n-y.\\) Thus we can say the Beta\\((a, b)\\) prior contributes \\(a\\) “successes” and \\(b\\) “failures” in \\(a + b\\) “prior trials.” So the bigger is \\(n\\) relative to \\(a+b\\) the less the prior matters. 3.4.1 Example Happiness example with a uniform prior. \\(a + b = 2, ~ n = 129\\). Thus the posterior distribution mostly depends on the data. # Reproduce Figure 3.4 in Hoff (2009) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(2,2)) theta &lt;- seq(0, 1, .01) a &lt;- 1; b &lt;- 1; n &lt;- 5; y &lt;- 1; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b1, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) legend(&quot;topright&quot;, inset=.05, lty=1, lwd=2,col=c(&quot;gray&quot;,&quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), bty=&quot;n&quot;) a &lt;- 3; b &lt;- 2; n &lt;- 5; y &lt;- 1; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b2, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) a &lt;- 1; b &lt;- 1; n &lt;- 100; y &lt;- 20; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b3, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) a &lt;- 3; b &lt;- 2; n &lt;- 100; y &lt;- 20; plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;,lwd=2, ylab=ylab, main=mtext(b4, side=3,line=.1), xlab = x) lines(theta, dbeta(theta, a, b), lwd=2, col=&quot;gray&quot;) Figure 3.3: Beta posterior distributions under two diﬀerent sample sizes and two different prior distributions. Look across a row to see the effect of the prior distribution, and down a column to see the eﬀect of the sample size In the top row \\(n=5\\) prior matters! In the bottom row, \\(n=100\\) prior does not matter. 3.5 Prediction Suppose we have \\(n+1\\) trials \\(Y_1, Y_2, …, Y_n, Y_{n+1}.\\) We will observe \\(n\\) of them and make our best prediction for the \\((n+1)\\)st. The model says \\(\\{Y_1, …, Y_n, Y_{n+1} | \\theta\\} \\sim \\text{iid Binary}(\\theta)\\). Under Bayesian, we have a distribution for our predictions. \\[ \\begin{aligned} Pr(\\tilde Y = 1|y_1,...,y_n)=E(\\theta|y_1,...,y_n)&amp;=\\frac{a+\\sum y_i}{a+b+n}\\\\ Pr(\\tilde Y = 0|y_1,...,y_n)=1-E(\\theta|y_1,...,y_n)&amp;=\\frac{b+\\sum(1-y_i)}{a+b+n} \\end{aligned} \\] \\(a\\) and \\(b\\) are called hyperparameters. The predictive distribution depends on hyperparameters which are known, and on the data which is observed. If the predictive distribution did not depend on \\(y_1, …, y_n\\) that would suggest \\(\\tilde Y\\) was independent of \\((Y_1, .., Y_n)\\). Also note that the predictive distribution does not depend on any unknown quantities. The Beta(1,1) prior \\(\\equiv\\) Uniform(0, 1) is equivalent to two prior observations (one success one failure). Does this seem right? Is this a ‘noninformative’ prior? Wouldn’t it be even more uninformative to take \\(a = b = 0.5?\\) What does this density look like? teta &lt;- seq(0, 1, 0.01) prior &lt;- dbeta(teta, 0.5, 0.5) plot(teta, prior, type = &quot;l&quot;, xlab = x, ylab = expression(p(theta)), lwd = 2) Figure 3.4: Beta(0.5, 0.5) density It is more uninformative to take Beta(0.5, 0.5) So although the 1 extra success and 1 extra failure in the Beta(1,1) feels like something kind of noninformative to some people it leads to a more sensible prediction as we will illustrate here; Under uniform prior, the posterior of the predictive probability for \\(\\tilde Y\\) has the posterior mean \\((y+1)/(n+2)\\). Suppose \\(y=0\\) then the posterior mean becomes \\(1/(n+2)\\) which is reasonable. Suppose \\(y=n\\), then it is \\((n+1)/(n+2)\\) which is also reasonable. Whereas the posterior mode of \\(y/n\\) doesn’t make sense as a predictive probability. Just because you observed \\(n\\) successes in \\(n\\) trials doesn’t mean you are 100% certain the next trial will be a success! Hence the posterior mean makes more sense as a predictive probability than the posterior mode and those two extra trials coming from the uniform prior seem sensible also. "],["CI.html", "Lecture 4 Confidence Intervals 4.1 Confidence Intervals 4.2 How do we compute intervals?", " Lecture 4 Confidence Intervals The following notes, mostly transcribed from Neath(2021) lecture, summarize sections(3.1.2 and 3.2) of Hoff(2009). In classical statistics you assume a probability model for a population of values based on the observation of a sample from that population. You estimate parameters of that probability model. A point estimate represents our best guess at that value, but we know it’s not right. Same thing is true with Bayesian inference. We can formulate a prior distribution, collect data, and compute the posterior using Bayes’ rule and from that take the posterior mean, \\(E(\\theta | y)\\), or posterior mode, \\(p(\\theta | y)\\), or posterior median as a point estimate and that value would represent our best guess. But if for example \\(\\theta\\) is a probability, \\(0 &lt; \\theta &lt; 1\\), whatever our best guess for \\(\\theta\\) is, our confidence in its correctness is 0%. \\(E(\\theta | y), p(\\theta | y)\\) are point estimates of \\(\\theta\\), NOT \\(\\theta\\) itself! There is a true \\(\\theta\\)! When we speak of \\(\\theta\\) as a random variable and give it a prior distribution and update that using the data and from that get a posterior distribution. We only involve probability in this discussion because we don’t know what \\(\\theta\\) is. Let’s use the percent of ‘happy women’ as an example. \\(\\theta =\\) proportion of women age 65+ who would call themselves “generally happy” \\((0 &lt; \\theta &lt; 1).\\) In classical statistics we resolve this issue (point estimate is our best guess but we know it’s wrong) by computing a standard error as well as a point estimate and reporting a 95% confidence interval. In Bayesian inference we’re gonna do the same thing. 4.1 Confidence Intervals Observe the data \\(Y = y\\). Compute \\(l(y) &lt; \\theta &lt;u(y)\\), \\(l\\) and \\(u\\) stand for lower and upper. The interval \\([l(y), u(y)]\\) is a 95% (Bayesian) confidence interval if \\(Pr[ l(y) &lt; \\theta &lt; u(y) | Y = y ] = 0.95\\). This is not the usual (frequentist) definition of 95% confidence interval. What does “95% confident” mean to a frequentist? It doesn’t mean true with 95% probability because a frequentist doesn’t allow “probability” to be interpreted like that. To a frequentist the statement \\(l(y) &lt; \\theta &lt; u(y)\\) is either true or false and we don’t know which it is but we know that prior to the data collection based on the likelihood(sampling model \\(p(y | \\theta)\\)), this is true: \\(Pr[ l(Y) &lt; \\theta &lt; u(Y) | \\theta] \\ge .95\\) for all values of \\(\\theta ~ (\\)this is a random interval, also notice conditioning on \\(\\theta)\\) So before we collect the data, we agree on a data collection mechanism, we agree on a formula for computing the interval based on the observed data, and we can say that there is a 95% chance that the interval will contain the value of \\(\\theta\\) and a 5% chance that is misses. Once you observe \\(Y = y\\) and you plug this data into your conﬁdence interval formula \\([l(y), u(y)]\\), then \\[ \\operatorname{Pr}(l(y)&lt;\\theta&lt;u(y) \\mid \\theta)=\\left\\{\\begin{array}{ll} 0 &amp; \\text { if } \\theta \\notin[l(y), u(y)] \\\\ 1 &amp; \\text { if } \\theta \\in[l(y), u(y)] \\end{array}\\right. \\] This statement makes sense: \\(Pr[ l(Y) &lt; \\theta &lt; u(Y) | \\theta] = 0.95.\\) We observe the data \\(Y= y\\) and \\(y\\) is a number and suppose our CI is \\(l(y) = .58, ~u(y) = .63\\). Then to say: \\(Pr[ .58 &lt; \\theta &lt; .63 ] = 0.95\\) ??????? That doesn’t make any sense! After we have observed the data, \\(\\theta\\) is either between 0.58 and 0.63 or it’s not. But to a Bayesian this makes perfect sense. Again, What does the frequentist 95% confidence mean? When a frequentist says: I’m 95% confident that \\(.58 &lt; \\theta &lt; .63\\) it means this interval [.58, .63] was computed using a method that would, in the long run, give correct results 95% of the time. This particular instance may be one of the 95% of cases we’re right or one of the 5% of cases where we get it wrong and we don’t which it is. We summarize this by saying we’re 95% confident in this particular interval. But that “95% confidence” statement to a frequentist is a verbal shorthand for this much more complicated interpretation. To a Bayesian 95% confidence is a perfectly direct statement. 4.2 How do we compute intervals? That’s easy! (conceptually easy). I have a probability distribution for \\(\\theta,\\) \\(p(\\theta | y)\\) is its density. I find numbers \\(l\\) and \\(u\\) such that \\(Pr( \\theta &lt; l | y) + Pr( \\theta &gt; u | y) = .05\\). That means there’s a 95% chance that \\(\\theta\\) is between those limits and thus \\([l, u]\\) give a 95% Bayesian confidence interval. There is not a unique solution. The most straightforward thing to do is to put \\(\\alpha/2\\) probability to to the left of \\(l\\) and \\(\\alpha /2\\) probability to the right of \\(u,\\) leaving \\(1 - \\alpha\\) probability in the middle (eg. for 95% confidence, \\(\\alpha = .05\\)). Our notation for quantiles is \\(\\theta_p, ~ \\theta_a, ~ \\theta_q\\) means \\(Pr[ \\theta &lt; \\theta_q | y ] = q\\) and \\(Pr[ \\theta &gt; \\theta_q | y ] = 1-q\\) (I’m assuming our parameter spaces are all continuous). 4.2.1 Example 2 successes in 10 trials. Find a 95% posterior interval for \\(\\theta\\) (posterior interval, Bayesian confidence interval, credible interval, credible set, posterior probability interval) all mean the same thing. \\[\\theta | Y = 2 \\sim \\text{Beta}(1+2, 1+8)\\] a &lt;- 1; b &lt;- 1; # prior n &lt;- 10; y &lt;- 2; # data qbeta(c(.025, .975), a+y, b+n-y) ## [1] 0.06021773 0.51775585 These quantiles are 0.06 and 0.52, respectively, and so the posterior probability that \\(\\theta \\in [0.06, 0.52]\\) is 95%. theta &lt;- seq(0, 1, .001) plot(theta, dbeta(theta, a+y, b+n-y), type=&quot;l&quot;, ylab=expression(p(theta*&#39;|&#39;*y)), lwd=2, xlab = expression(theta)) CI &lt;- qbeta(c(.025, .975), a+y, b+n-y) abline(v=CI, lty=2, lwd=2) legend(&quot;topright&quot;, inset=.05, lty=c(1,2), lwd=2, legend=c(&quot;Posterior dist&quot;, &quot;95% CI&quot;)) Figure 4.1: Posterior interval There is 95% posterior probability that \\(.06 &lt; \\theta &lt; .52\\). A thing to note about this interval. Putting .025 in the left tail and .025 in the right tail is not the only way to get .95 in the middle. In this case we could move the lower bound down a little and get to move the upper bound down by a little more. Some people (quite reasonably) would like to report the shortest possible 95% interval. In general the equal-tail interval that we just described is not that. There is a way to get a shorter interval. The term for this is the HPD(highest posterior density) interval. In most problems it does not make a huge difference (I think I heard there’s an R package that makes it pretty easy). But ok, look into this if you’re interested. In our case we’ll make it our standard to always report equal-tailed posterior probability intervals \\(\\alpha/2\\) to the left \\(\\alpha/2\\) to the right leaving \\(1-\\alpha\\) in the middle. "],["poisson-model.html", "Lecture 5 Poisson model 5.1 Posterior inference for the Poisson model 5.2 Posterior predictive distribution 5.3 Example: Birth rates 5.4 Explaining the parameters of the gamma distribution", " Lecture 5 Poisson model The following notes are transcribed from Neath(2021) lecture which summarizes Sections(3.1.2 and 3.2) of Hoff(2009). Suppose \\(Y =\\) number of facebook friends your neighbor has. There are a large number of people in the world. Some of them are your neighbors facebook friends and some are not. \\[ Y_i = \\begin{cases} 1 \\text{ if the }i\\text{th person in the world is a facebook friend of your neighbors}\\\\ 0 \\text{ otherwise} \\end{cases} \\] \\(Y_i \\sim ~\\)Binomial(size = number of people in the world). probability = proportion of those people who are facebook friends with your neighbor. This is not a good probability model for this variable, because the size is too big and the probability is too small. What works better in cases like this is the Poisson distribution. Sample space is { \\(0, 1, 2, ...\\) }. probability function is \\(e^{-\\theta} \\times \\theta^y / y!, ~ y \\in \\{~0, 1, 2,...\\},\\) \\(\\theta\\) is the mean, i.e., expected value of \\(Y\\). The Poisson model is a good first thing to think of and first thing to try for count data. A property of the Poisson distribution is; variance = \\(\\theta\\) = mean. When the variance is a little bigger than the mean, Poisson probabilities may still match the observed proportions reasonably closely but when variance is a lot bigger than the mean Poisson distribution is not such a good model. Let \\(Y_i =\\) count for \\(i\\)th unit, { \\(i = 1, 2, …, n\\) }. If \\(Y_i | \\theta\\) are iid Poisson(\\(\\theta\\)) then the joint probability of \\((y_1, y_2, …, y_n)\\) depends on the data only though the total value \\(\\sum{y_i}\\). We say the \\(\\sum{y_i}\\) is a sufficient statistic. If you observe the data \\(n=3, y =\\) c( 1 , 2, 3 ) and I observe \\(n = 3,~ y =\\) c( 4, 1, 1 ) our inference about \\(\\theta\\) will be exactly the same. It is also a property of Poisson random variables that sums of independent Poisson random variables are distributed as Poisson. Posterior of \\(\\theta\\) satisfies \\[ p(\\theta | y_1, ..., y_n) = c_0 \\times p(\\theta) \\times p(y|\\theta)=c \\times p(\\theta) \\times \\theta^{\\sum{y_i}}e^{-n \\theta } \\] We’ll work with a conjugate prior because it makes things simple. Looking at \\(p(\\theta)\\times\\theta^{\\sum{y_i}} e^{-n\\theta }\\), what could we plug in for \\(p(\\theta)\\) such that \\(p(\\theta | y)\\) will have the same parametric form? well \\(p(\\theta | y)\\) is gonna have a \\(\\theta^{\\text{something}}\\) and a something\\(^{\\theta}\\) so the prior can have those things too! What probability distribution for \\(\\theta &gt; 0\\) allows \\(\\theta^{\\text{something}}\\) and something\\(^{\\theta}\\)? That would be the gamma distribution \\(= \\tilde c\\times\\theta^{a-1} e^{-b \\theta}\\). In this parameterization \\(b\\) is the rate parameter, \\(1/b\\) is the scale parameter. 5.1 Posterior inference for the Poisson model For our prior on \\(\\theta\\) we’ll say \\(\\theta \\sim\\) gamma\\((a, b)\\). How do you choose a and b? In general, when considering the question of how to choose a prior look at what the prior leads to in terms of the posterior. Not by peeking at the data though, just by studying the model. i.e., Let’s look at what posterior the gamma\\((a, b)\\) prior will lead to and then come to the question of what \\(a\\) and \\(b\\) should be. Key Result: If \\(\\theta \\sim \\text{gamma}(a,b)\\) and \\(Y_1,\\ldots,Y_n | \\theta \\sim\\) iid Poisson\\((\\theta)\\), then \\((\\theta | Y_1 = y_1, \\ldots, Y_n = y_n) \\sim \\text{gamma}(a+\\sum y_i, b+n)\\). What do \\(a\\) and \\(b\\) contribute to the posterior? \\(a\\) contributes analogously to \\(\\sum{y_i}\\) which is the observed total count \\(b\\) contributes analogously to \\(n\\) which is the number of observed counts. Another way to say this: The data consist of \\(n\\) observations with average value \\(\\sum{y_i} / n,\\) the prior contributes (effectively) \\(b\\) observations with average value \\(a/b\\). So in the absence of genuine prior information about \\(\\theta\\) just make sure \\(b\\) is small relative to \\(n\\) and it won’t matter very much. Make \\(b\\) small relative to \\(n\\) and make \\(a/b\\) or \\((a-1)/b\\) your prior “best guess.” Student question: What’s the difference, definition wise, between \\(n\\) observations and \\(b\\) observations? Ans: The \\(n\\) observations are real data. The \\(b\\) observations are not data they’re “pseudo-data” or “prior observations” The value of \\(\\sum{y_i}\\) is determined by the data. The values of \\(a\\) and \\(b\\) are chosen by us. 5.2 Posterior predictive distribution Now suppose we have \\(n+1\\) observations and our job is to predict the \\((n+1)\\)st. Let \\(\\tilde Y\\) represent this \\((n+1)\\)st observation. The conditional distribution of \\(\\tilde Y|Y_1 = y_1, …, Y_n = y_n\\) is called the posterior predictive distribution. Posterior because it depends on the observed data. The distinction between “estimation” and “prediction”; If it’s an observable (but as yet unobserved) quantity it’s a prediction. Like \\(\\tilde Y;\\) the \\((n+1)\\)st data point that just hasn’t been observed yet. Versus an unknown parameter like \\(\\theta\\) is an unobservable quantity. Inference about \\(\\theta\\) would be called estimation. \\[ \\begin{aligned} p(\\tilde y | y_1,...,y_n) &amp;= \\int p(\\tilde y,\\theta | y_1,...,y_n)d\\theta\\\\ &amp;=\\int \\texttt{dpois}(\\tilde y, \\theta) \\texttt{dgamma}(\\theta, a+\\sum y_i, b+n)d\\theta \\end{aligned} \\] A general result: “A gamma mixture of Poissons is negative binomial.” One way the negative binomial distribution arises is; \\(y =\\) number of failures before the \\(n\\)th success(in Bernoulli trials). This is the same negative binomial distribution as that but not the same motivation and note the “\\(n\\)” parameter need not be an integer. \\(E (\\tilde Y) = E [ E(\\tilde Y | \\theta) ] = E(\\theta).\\) Simalarly, \\(\\text{Var}[ \\tilde Y ] = \\text{Var}[ E(\\tilde Y | \\theta) ] + E[\\text{Var}(\\tilde Y | \\theta) ] = \\text{Var}( \\theta ) + E (\\theta )\\) but the posteriors! 5.3 Example: Birth rates We are comparing two populations. There’s strong evidence that there is a difference between the two populations (see slide 22). a &lt;- 2 ; b &lt;- 1 # prior parameters n1 &lt;- 111; sy1&lt;-217 # data in group 1 # posterior mean ( a+sy1 )/( b+n1 ) ## [1] 1.955357 # posterior mode ( a+sy1-1)/(b+n1 ) ## [1] 1.946429 # posterior 95% CI qgamma( c (0.025 , 0.975) , a+sy1 , b+n1 ) ## [1] 1.704943 2.222679 n2 &lt;- 44 ; sy2&lt;-66 # data in group 2 # posterior mean ( a+sy2 )/( b+n2 ) ## [1] 1.511111 # posterior mode ( a+sy2-1)/(b+n2 ) ## [1] 1.488889 # posterior 95% CI qgamma( c (0.025 , 0.975) , a+sy2 , b+n2 ) ## [1] 1.173437 1.890836 5.4 Explaining the parameters of the gamma distribution Poisson \\[ f(x|\\theta)=e^{-\\theta}\\,\\frac{\\theta^x}{x!} \\quad \\text{ for } x\\in\\{0,1,...,\\},\\quad \\theta&gt;0\\\\ \\] Gamma \\[f(x) = \\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx}, \\quad x &gt; 1\\\\[0.6cm]\\] In the density function we have \\(x^a(\\exp(-bx).\\) If this was an exponential distribution for the waiting time to the next event where the event is occurring according to a Poisson process with rate \\(b\\) it would just be \\(e^{-bx}\\) and would have a mean waiting time of \\(1/b,\\) where \\(b\\) is the rate. Why is \\(a\\) called the shape parameter? If we change the variable from waiting time till next event to waiting time till \\(a\\)th event we get a gamma\\((a, b)\\) distribution. In what sense does \\(a\\) govern the shape of a gamma dist? The answer is: When \\(a = 1\\) we have an exponential dist and in the exponential dist \\(a &lt; 1\\) means asymptote at 0, \\(a=1\\) means mode at 0 that’s a severely right-skewed dist. When \\(a &gt; 1\\), the mode is at \\((a-1) / b\\). And the bigger \\(a\\) is the farther away from zero this mode is AND the more bell-shaped is the density curve. Property of the gamma\\((a, b)\\) dist is; If \\(a\\) is big, it’s well-approximated by a normal distribution. To the original question in what sense is \\(b\\) a “rate” and in what sense does \\(a\\) determine “shape?” \\(b\\) is a rate in the “Poisson-process-connection to exponential distribution” sense. The closer \\(a\\) is to zero the more right-skewed is the gamma distribution, the bigger \\(a\\) is the more bell-shaped (normal) is the gamma distribution. In the Poisson-gamma Bayesian problem the bigger is \\(\\sum{y_i}\\) the more data we’ve observed the more events we’ve observed. A general result in Bayesian inference is: The more data you have the more normal will be the posterior dist. In the case of the Poisson-gamma model having “more data” doesn’t just mean \\(n\\) increasing. It actually depends on observing lots of events so “lots of data” in the Poisson gamma model is not just big “exposure” it’s lots of events also which is determined by \\(\\sum{y_i}\\). The more events we’ve observed the more data we have the more normal is our posterior dist hence \\(\\sum{y_i}\\) (along with \\(a\\) in the prior dist) make the posterior more and more bell-shaped or normal so they drive the shape of the posterior toward the normal dist. The rate is there to normalize things. Observing 50 events in 10 days does not mean the same thing ( about \\(\\theta\\) ) as observing 50 events in 2 days. So the \\(n\\)(the rate) is defined with respect to a unit of time which determines how many units of data we’ve observed so in that sense \\(n\\) (along with \\(b\\)) define the rate parameter in our gamma posterior. "],["monte-carlo.html", "Lecture 6 Monte Carlo 6.1 Example: birth rate 6.2 The Monte Carlo method 6.3 Example: Numerical evaluation 6.4 Posterior inference for arbitrary functions 6.5 Example: Log-odds 6.6 Example: Functions of two parameters 6.7 How many Monte Carlo samples are needed?", " Lecture 6 Monte Carlo The following notes, mostly transcribed from Neath(0510,2021) lecture, summarize sections(4.1 and 4.2) of Hoff(2009). 6.1 Example: birth rate Let’s continue with the example from last time in which \\(\\theta_1 =\\) true birthrate (children per woman) among children with no college degree (less than bachelors degree), and \\(\\theta_2\\) is the true birthrate among women with a bachelor’s degree or higher. A birthrate of 2 would be a society with a steady population. In our data, among those women with less than bachelors degrees there were \\(n_1 = 111\\) such women with a total of 217 children\\((\\sum y_{i,1} = 217)\\), so that’s \\(1.95\\) children per woman. Among those women with bachelors or higher \\(n_2 = 44,~ \\sum{y_{i,2}} = 66\\), so that’s \\(1.50\\) children per woman. The prior we used was independent gamma\\((2,1),\\) with a prior mean of \\(2\\)(representing steady growth). \\(b =\\) prior sample size of \\(1.\\) The data sample sizes were \\(44\\) and \\(111\\) so the prior won’t have a lot of influence on our inference (which is usually how we want it). We are interested in the posterior probability that \\(\\theta_1 &gt; \\theta_2,\\) \\(\\theta_1\\) and \\(\\theta_2\\) are independent gammas in this posterior distribution. \\(Pr(\\theta_1 &gt; \\theta_2 | y) = Pr( \\theta_2 &lt; \\theta_1 | y) = \\int{Pr(\\theta_2 &lt; \\theta_1 | \\theta_1, y) p(\\theta_1 | y) d\\theta_1}\\); An application of “a law of total probability.” Note on using R: dgamma returns a gamma density value pgamma returns a gamma cdf value qgamma returns a gamma quantile rgamma simulates a random draw from a gamma dist integrand &lt;- function(x) {pgamma(x, 68, 45) * dgamma(x, 219, 112)} integrate(f=integrand, lower=0, upper=Inf) ## 0.9725601 with absolute error &lt; 6.5e-06 Our posterior belief that the birthrate is lower among women with bachelors degrees is 0.973. This was cool! We got the right answer. Exact answers are cool, But this really only worked because it was such a “nice” problem. The gamma distribution is nice. Having a dimension of only 2 made it nice. Numerical integration is great in low-dimensional problems but does not work so well in high dimensional ones. Today (the subject of Chapter 4) we take up a general approach that does continue to work in high dimensional problems. This method, known as Monte Carlo approximation, is based on random sampling, and its implementation does not require a deep knowledge of calculus or numerical analysis. 6.2 The Monte Carlo method Why is the method of Monte Carlo called Monte Carlo? It’s a resort on the French Riviera with casinos (people go gambling there). Monte Carlo was the code name for a project by allied scientists in World War II. \\(p(\\theta | y_1, …, y_n)\\) or just \\(p(\\theta | y)\\) for short is the posterior density function which uniquely identifies the posterior distribution so we say \\(p(\\theta | y)\\) defines the posterior. In the Monte Carlo method we let \\(\\theta^{(s)}\\) (we’re using a superscript rather than subscript because we use subscripts to indicate a position in a vector parameter) be a random draw from the posterior distribution for \\(s = 1, …, S\\). For this to work \\(S\\) must be reasonably large. In statistics we have an unknown population distribution but if we observe data that are a random sample from that distribution we can use the data to make inference about the population distribution. When we do Monte Carlo approximations to a posterior probability distribution we’re doing “an inference problem within the inference problem.” If \\(S\\) is a big enough number a histogram of the \\(\\theta^{(s)}\\) will be a decent representation of the probability density \\(p(\\theta | y).\\) \\(\\theta_2 \\sim \\text{gamma}(68,45)\\) (the posterior for the birth rate among college grads in our earlier example) # Reproduce Figure 4.1 in Hoff par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) a &lt;- 2; b &lt;- 1; n &lt;- 44; sum.y &lt;- 66; #simulating draws theta.sim1 &lt;- rgamma(10, a+sum.y, b+n) theta.sim2 &lt;- rgamma(100, a+sum.y, b+n) theta.sim3 &lt;- rgamma(1000, a+sum.y, b+n) theta &lt;- seq(.75, 2.25, .01) p.theta &lt;- dgamma(theta, a+sum.y, b+n) xlim=c(.75, 2.25); ylim=c(0, 2.5) op &lt;- par(mfrow=c(2,3)) hist(theta.sim1, freq=F, right=F, xlim=xlim, main=&quot;&quot;, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;, breaks=20) lines(theta, p.theta, lwd=2, col=&quot;gray&quot;) hist(theta.sim2, freq=F, right=F, xlim=xlim, main=&quot;&quot;, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;, breaks=20) lines(theta, p.theta, lwd=2, col=&quot;gray&quot;) hist(theta.sim3, freq=F, right=F, xlim=xlim, main=&quot;&quot;, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;, breaks=20) lines(theta, p.theta, lwd=2, col=&quot;gray&quot;) plot(theta, p.theta, type=&quot;l&quot;, lty=1, lwd=2, col=&quot;gray&quot;, xlim=xlim, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;) lines(density(theta.sim1), lty=2, lwd=2) plot(theta, p.theta, type=&quot;l&quot;, lty=1, lwd=2, col=&quot;gray&quot;, xlim=xlim, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;) lines(density(theta.sim2), lty=2, lwd=2) plot(theta, p.theta, type=&quot;l&quot;, lty=1, lwd=2, col=&quot;gray&quot;, xlim=xlim, ylim=ylim, xlab=expression(theta), ylab=&quot;density&quot;) lines(density(theta.sim3), lty=2, lwd=2) Figure 6.1: Successive Monte Carlo approximations to the density of the gamma(68,45) distribution, along with the true density function for comparison. Row 1: histogram with true density for comparison. The gray curve is the true gamma(68, 45) density Row 2: “kernel density estimates” with true density for comparison As Monte Carlo sample size increases the agreement between the empirical distribution of the random samples and the target distribution gets better and better. “Empirical distribution” is a fancy name for the histogram. As if the population consisted of \\(S\\) different values each with equal probability and those values are the simulated draws. That’s what we mean by “empirical distribution.” This is a toy problem (we know the answer). A toy problem is the name we give to exercises where we use a method like Monte Carlo despite Monte Carlo not being necessary (the exact answer is readily available). We study the performance of a method like Monte Carlo in problems where the answer is known(i.e., toy problems) so that we have confidence in the method when applied to problems where the exact answer isn’t known (non-toy problems). The “empirical distribution approaches the target distribution” argument can be made a bit more formal by the Law of Large Numbers. LLN: As your flip a coin more and more times the proportion of observed heads will converge to the true heads probability with probability 1. Or more generally as the sample size increases the sample mean converges to the population mean with probability 1. As Monte Carlo sample size (number of simulations) increases i.e., as \\(S \\rightarrow \\infty\\) sample average converges to the posterior mean sample variance converges to the posterior variance sample proportions converge to posterior probabilities. The empirical distribution of \\(\\{\\theta^{(1)},...,\\theta^{(S)}\\} \\rightarrow p(\\theta|y_1,...,y_n)\\) A point where the proportion of \\(\\theta^{(s)} &lt;\\) this point is \\(\\alpha\\) converges to the \\(\\alpha\\)-quantile of the posterior distribution Unlike other numerical approximations, with Monte Carlo we know “If we run the computer long enough we’ll eventually get the right answer”(i.e., desired level of precision). 6.3 Example: Numerical evaluation Let’s keep studying this toy problem. This will demonstrate what happens when we go from 10 samples to 100 samples to 1000 samples – we get a better and better approximation to the true (a+sum.y) / (b+n) # true posterior mean ## [1] 1.511111 # Monte Carlo means c(mean(theta.sim1),mean(theta.sim2),mean(theta.sim3)) ## [1] 1.653986 1.522461 1.508636 pgamma(1.75, a+sum.y, b+n) # Pr(theta &lt; 1.75 | y) ## [1] 0.8998286 # Monte Carlo approximations c(mean(theta.sim1 &lt; 1.75), mean(theta.sim2 &lt; 1.75), mean(theta.sim3 &lt; 1.75)) ## [1] 0.700 0.890 0.907 # true quantile qgamma(c(.025, .975), a+sum.y, b+n) ## [1] 1.173437 1.890836 # Monte Carlo quantiles c(quantile(theta.sim1, c(.025, .975)), quantile(theta.sim2, c(.025, .975)), quantile(theta.sim3, c(.025, .975))) ## 2.5% 97.5% 2.5% 97.5% 2.5% 97.5% ## 1.276212 2.144389 1.229435 1.923309 1.190959 1.895615 We can also look at trace plot of successively better approximations to the quantity of interest # Reproduce Figure 4.2 theta.mc &lt;- theta.sim3 S &lt;- length(theta.mc) theta.bar &lt;- cumsum(theta.mc) / (1:S) #running average Prob.hat &lt;- cumsum(theta.mc &lt; 1.75) / (1:S) quant.hat &lt;- rep(NA, S) for(s in 1:S) { quant.hat[s] &lt;- quantile(theta.mc[1:s], .975) } par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,3)) plot(1:1000, theta.bar, type=&quot;l&quot;, xlab=&quot;No. samples&quot;, ylab=&quot;cumulative mean&quot;, main=&quot;&quot;) abline(h=(a+sum.y)/(b+n), col=&quot;gray&quot;, lwd=2) plot(1:1000, Prob.hat, type=&quot;l&quot;, xlab=&quot;No. samples&quot;, ylab=&quot;cumulative ecdf at 1.75&quot;, main=&quot;&quot;) abline(h=pgamma(1.75, a+sum.y, b+n), col=&quot;gray&quot;, lwd=2) plot(1:1000, quant.hat, type=&quot;l&quot;, xlab=&quot;No. samples&quot;, ylab=&quot;cumulative .975-quantile&quot;, main=&quot;&quot;) abline(h=qgamma(.975, a+sum.y, b+n), col=&quot;gray&quot;, lwd=2) Figure 6.2: Estimates of the posterior mean, Pr(theta &lt; 1.75|y) and the 97.5% posterior quantile as a function of the number of Monte Carlo samples. Horizontal gray lines are the true values. The behaviour of the sample quantile is weird. A couple samples push us way off and then we get back to the right thing. 6.4 Posterior inference for arbitrary functions A cool thing about the Monte Carlo method is this; suppose I know the distribution of \\(\\theta,\\) the posterior \\(p(\\theta | y)\\). The quantity I’m interested in is \\(\\gamma = g(\\theta)\\) for some possibly complicated function \\(g.\\) Ex: If \\(\\theta\\) is a probability \\(\\theta /(1 -\\theta)\\) is an odds \\(\\log( \\theta / (1-\\theta) ) = \\text{logit}(\\theta)\\) is a log-odds. We may be interested in posterior inference about the log odds! Monte Carlo makes this super easy. If \\(\\theta^{(s)}\\) is a random draw from \\(p(\\theta | y)\\) and \\(\\gamma^{(s)} = g( \\theta^{(s)} )\\) then \\(\\gamma^{(s)}\\) is a random draw from \\(p(\\gamma | y)\\) the posterior of \\(\\gamma\\) regardless of how complicated a function \\(g(\\theta)\\) may be. This is particularly important when \\(\\theta\\) is high dimensional. If I can simulate random draws from a probability distribution, I can learn anything I want about that probability distribution (as long as I’m patient enough to let the simulation run). The algorithm: sample \\(\\theta^{(1)} \\sim p(\\theta|y_1,...,y_n)\\), compute \\(\\gamma^{(1)} = g(\\theta^{(1)})\\); sample \\(\\theta^{(2)} \\sim p(\\theta|y_1,...,y_n)\\), compute \\(\\gamma^{(2)} = g(\\theta^{(2)})\\); etc sample \\(\\theta^{(S)} \\sim p(\\theta|y_1,...,y_n)\\), compute \\(\\gamma^{(S)} = g(\\theta^{(S)})\\); with each draw sampled independently 6.5 Example: Log-odds This is from a General Social Survey in 1998. Respondents in a General Social Survey were asked if they agreed with a Supreme Court ruling that prohibited state or local governments from requiring the reading of religious texts in public schools. Let \\(\\theta =\\) true proportion of population that would answer YES. Of the 860 respondents in the sample(860 trials) there were \\(y=441\\) successes. Given a Uniform\\((0, 1)\\) prior the posterior distribution of \\(\\theta\\) is Beta\\((1 + 441, 1 + 860 - 441) =\\) Beta\\(( 442, 420 )\\). This a pretty tight posterior distribution centered around \\(a/a+b= 442/862 =0.513\\). Using the Monte Carlo algorithm described above, we can obtain samples of the log-odds, \\(\\gamma= \\log[\\theta/(1−\\theta)]\\), from both the prior distribution and the posterior distribution of \\(\\gamma\\). # Example: Inference about log-odds rm(list=ls()); set.seed(20210506); a &lt;- 1; b &lt;- 1; theta.sim.prior &lt;- rbeta(1000, a, b) gamma.sim.prior &lt;- log( theta.sim.prior / (1-theta.sim.prior) ) n &lt;- 860; y &lt;- 441; theta.sim.post &lt;- rbeta(1000, a+y, b+n-y) gamma.sim.post &lt;- log( theta.sim.post / (1-theta.sim.post) ) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,2)) plot(density(gamma.sim.prior), xlab=expression(gamma), ylab=expression(p(gamma)),xlim=range(gamma.sim.prior),main=&quot;&quot;) plot(density(gamma.sim.post), xlab=expression(gamma), ylab=ylab, xlim=range(gamma.sim.prior),main=&quot;&quot;) lines(density(gamma.sim.prior), col=&quot;gray&quot;) Figure 6.3: Monte Carlo approximations to the prior and posterior distributions of the log-odds. theta.sim.prior is just runif(1000). A uniform prior is flat on [0, 1]. What sort of distribution does this imply for \\(\\log( \\theta / (1-\\theta) )\\)? Well it’s centered at 0 (probability = 0.5 means odds=1 means log-odds = 0). It’s symmetric. It’s not a normal distribution but it is a roughly bell shaped curve. What about the posterior? Well the posterior distribution of \\(\\theta \\sim\\) Beta\\(( 442, 420 )\\) is very tightly clustered around the value \\(\\theta = 0.513\\) or so. So the posterior distribution of the log odds is very tightly clustered around \\(0\\) or just \\(&gt; 0\\) or so. Why are we doing this? We are doing this now as a toy problem to prepare ourselves for later in the course when we encounter analytically intractable and high-dimensional posterior distributions. What we’ll see in these problems is posterior moments? Posterior probabilities? Posterior quantiles? Good luck with anything other than Monte Carlo. 6.6 Example: Functions of two parameters Returning to the birthrates example, \\(\\theta_1 =\\) birthrate for women with less than bachelor’s \\(\\theta_2 =\\) birthrate for women with at least a bachelor’s degree. \\(\\theta_1|y_{i,j}\\sim \\text{gamma}(219,112)\\) and \\(\\theta_2|y_{i,j}\\sim \\text{gamma}(68,45)\\) There are a variety of ways to describe our knowledge about the difference between \\(\\theta_1\\) and \\(\\theta_2\\). For example, we may be interested in the numerical value of \\(Pr(\\theta_1 &gt; \\theta_2|y_{i,j})\\), or in the posterior distribution of \\(\\gamma = \\theta_1/ \\theta_2.\\) Both of these quantities can be obtained with Monte Carlo sampling: This expression: \\(\\frac{1}{S}\\sum_{s=1}^S \\mathbf{1}(\\theta_1^{(s)}&gt;\\theta_2^{(s)})\\) is kinda sorta similar to: #{ \\(\\theta^{(s)} \\le c\\) }\\(/S\\). We can say “average of indicator functions” or # { successes } divided by \\(S\\) mean the same thing. # Return to the birthrates example a &lt;- 2; b &lt;- 1; n1 &lt;- 111; sum.y1 &lt;- 217; n2 &lt;- 44; sum.y2 &lt;- 66; theta1.sim &lt;- rgamma(10000, a+sum.y1, b+n1) theta2.sim &lt;- rgamma(10000, a+sum.y2, b+n2) mean(theta1.sim &gt; theta2.sim) ## [1] 0.9726 Our Monte Carlo approximation to \\(Pr( \\theta_1 &gt; \\theta_2 | y )\\) is 0.9726. The “exact value” is also 0.9726. This is not typical. Now suppose we want to do inference about the ratio \\(\\gamma = \\theta_1/ \\theta_2.\\) In your 4203/5203 (probability) class you may have studied bivariate transformations of random variables where you calculate the back-transformation then solve the “Jacobian” matrix (of partial derivatives). We could solve this problem that way OR we can get an arbitrarily precise numerical approximation with no difficult math required whatsoever. Take a random sample of : \\(\\theta_1^{(s)}, ~\\theta_2^{(s)}\\) and let \\(\\gamma^{(s)} = \\theta_1^{(s)} / \\theta_2^{(s)}\\) for \\(s = 1, … S.\\) gamma.sim &lt;- theta1.sim / theta2.sim hist(gamma.sim, freq=F, right=F, breaks=30, xlab=expression(gamma), ylab=ylab, main = &quot;&quot;) lines(density(gamma.sim)) Here is the posterior distribution of \\(\\gamma = \\theta_1 / \\theta_2.\\) It’s mostly \\(&gt; 1\\). I show a histogram and kernel density estimate on the same axes. This is not a theoretical density curve it’s the empirical one with \\(S=10000\\) and it’s pretty smooth. The posterior distribution of \\(\\gamma = \\theta_1 / \\theta_2.\\) quantile( gamma.sim, c(.025, .975) ) ## 2.5% 97.5% ## 0.9960863 1.7111331 The .025 and .975 quantiles are .996 and 1.715. I have 95% confidence that the birth rate for non-college grads is between .996 and 1.715 times that of women with college degree. 6.7 How many Monte Carlo samples are needed? We use Monte Carlo as an approximation tool. The Monte Carlo sample size does not determine posterior uncertainty. The posterior distribution \\(p(\\theta | y)\\), the posterior mean \\(E(\\theta | y)\\) and posterior variance \\(\\text{Var}(\\theta | y)\\) are what they are based on the prior distribution \\(p(\\theta)\\), and the data we observed. If the prior distribution is “vague” and the data set is not huge, the posterior variance \\(\\text{Var}(\\theta | y)\\) will be big. If the posterior distribution is approximately normal a 95% confidence will be approximately \\(E(\\theta|y) \\pm 2 \\sqrt{ \\text{Var}(\\theta|y) }\\), because with the normal distribution, mean \\(\\pm\\) 2 SD’s contains 95% of the distribution. None of that has anything to do with Monte Carlo. Monte Carlo comes in when the posterior distribution is not known analytically and we will approximate it by simulating draws from the posterior distribution and using that empirical distribution to approximate the posterior. Using Monte Carlo to approximate posteriors in Bayesian inference is “an inference problem within the inference problem.” We have uncertainty about \\(\\theta\\) that’s measured by Var\\((\\theta | y)\\), the posterior variance. How we use Monte Carlo is; \\(E(\\theta | y)\\) is only our best guess for \\(\\theta\\), it’s not the truth (it’s an estimator). What if we can’t even calculate \\(E(\\theta | y)\\) exactly, then we will approximate it using Monte Carlo. The bigger is the Monte Carlo sample size the better will be our approximation to \\(E(\\theta | y)\\). But the uncertainty about \\(\\theta\\) can’t be eliminated. In the birth rate example, our data consist of 111 women without college degree 44 women with college degree 155 women total. However precisely, we can estimate \\(\\theta_1\\) and \\(\\theta_2\\) based on these sample sizes is what it is. We can’t improve that by increasing Monte Carlo sample size. As to the question of: How big a Monte Carlo sample to take? One formal approach to this is; Take a big enough Monte Carlo sample size to estimate \\(E(\\theta | y)\\) to a desired level of precision that the mean of the empirical density (the dashed curve in figure 6.1) is within epsilon of the mean of the population curve (gray curve) which is unknown. This solution is an Intro Stat problem! Let \\(S\\) equal Monte Carlo sample size. In terms of Monte Carlo error, \\(\\bar \\theta\\)(the sample mean of the simulated draws) has approximately a Normal distribution because \\(S\\) is big. Its mean is \\(E(\\theta|y)\\). Its variance is Var\\((\\theta|y)/S\\). A 95% Monte Carlo CI for \\(E(\\theta|y)\\) is \\(\\bar \\theta \\pm 2\\sqrt{ \\text{Var}(\\theta|y) / S }\\). Now suppose we want the Monte Carlo sample size big enough so that the Monte Carlo error \\(|\\bar \\theta - E(\\theta|y )|\\) is &lt; .01 with 95% confidence. Then we need \\(2 \\sqrt{ \\text{Var}(\\theta|y) / S } &lt; .01\\) (could use \\(1.96 = \\texttt{qnorm(0.975)}\\) in place of 2). Solve \\(2 \\sqrt{\\text{Var}(\\theta|y) / S} &lt; .01\\) for \\(S\\). Of course the posterior variance is unknown so use an estimate in its place an estimate such as the sample variance in its place. Here the sample is !Monte Carlo sample! not the actual data!. In general we don’t really do this. We use \\(S = 1000\\) or \\(S = 10000\\). But this is still a thing we should know about. May be the basis for a homework 2 exercise. "],["predictive.html", "Lecture 7 Predictive 7.1 Sampling for predictive distribution 7.2 Example: Let \\(D = \\tilde Y_1 - \\tilde Y_2\\) 7.3 Posterior predictive model checking 7.4 Posterior predictive model checking", " Lecture 7 Predictive The following notes, mostly transcribed from Neath(0511,2021) lecture, summarize sections(4.3 and 4.4) of Hoff(2009). Last class, we talked about using simulated draws \\(\\theta^{(s)} \\sim p(\\theta | y)\\) to form a Monte Carlo approximation to the posterior distribution \\(p(\\theta | y)\\). Today we do Monte Carlo simulation for the posterior predictive distribution. A predictive distribution is characterized by two features: (1) known quantities are conditioned on (2) unknown quantities are integrated out. For example, if we integrate \\(\\theta\\) out of \\(\\int{ p(\\tilde y | \\theta) p(\\theta ) d\\theta } = p(\\tilde y)\\), we call this a prior predictive distribution; A predictive distribution that integrates over unknown parameters but is not conditional on observed data. A result of probability theory: If the sampling model is Poisson\\((\\theta)\\) and the prior is \\(\\theta \\sim \\text{gamma}(a, b)\\) then the prior predictive is \\(\\text{NegBinom}(a, b)\\); an overdispersed count distribution. The Poisson distribution has the property that \\(\\text{Var}(Y) = E(Y)\\). For a negative binomial distribution, \\(\\text{Var}(Y) &gt; E(Y)\\). A predictive distribution that accounts for the information available in the sample data \\((y_1, …, y_n) = y\\) is called a posterior predictive distribution and in the Poisson-gamma model, because the gamma is a conjugate for the Poisson and the posterior on \\(\\theta\\) is gamma\\((a+ \\sum{y_i},b + n)\\) the posterior predictive distribution (PPD) for an \\((n+1)\\)st observation is \\(\\text{NegBinom}(a + \\sum{y_i}, b + n)\\). R has dnbinom, pnbinom, rnbinom functions. So we don’t need to do Monte Carlo for the PPD any more than we need to for approximating the posterior of \\(\\theta\\) but we’re gonna anyway because it’s educational. Note: When I say “Poisson-gamma model” or I may say “gamma-Poisson model” that’s a shorthand for “The Bayesian statistical model where our data consists of exchangeable observations of conditionally independent Poissons with rate \\(\\theta\\) where the prior distribution on \\(\\theta\\) is gamma\\((a, b)\\)” 7.1 Sampling for predictive distribution Often, not only does the PPD not have a nice closed form but it’s also not even so straightforward to sample from. But we can still do Monte Carlo for predictive distributions! Proposition: If \\(\\theta^{(s)} \\sim p(\\theta | y)\\) and \\(y^{(s)} \\sim p(y | \\theta^{(s)})\\) then I should say \\(\\tilde y^{(s)} \\sim p(\\tilde y | \\theta^{(s)})\\) then jointly \\((\\theta^{(s)}, \\tilde y^{(s)}) \\sim p(\\theta | y) \\times p(\\tilde y | \\theta) = p(\\theta, \\tilde y | y)\\) the joint distribution of \\(\\theta \\text{ and } \\tilde Y\\). In general if \\(X_1, X_2 \\sim f(x_1, x_2)\\) then \\(X_2\\) is a draw from the marginal of \\(X_2.\\) 7.1.1 Example: birth rate Let’s return to the birthrates example. Our posterior for \\(\\theta_1\\) and \\(\\theta_2\\) were independent. \\(\\theta_1 | y \\sim\\) gamma(219, 112) and \\(\\theta_2 | y \\sim\\) gamma(68, 45). Suppose we want the posterior predictive probability \\(Pr(\\tilde Y_1 &gt; \\tilde Y_2 |\\) data). \\(\\tilde Y_1 =\\) number of children for a randomly selected 40-year-old woman with less than bachelor’s degree. \\(\\tilde Y_2 =\\) number of children for a randomly selected woman with bachelor’s or higher. \\[ \\begin{aligned} \\operatorname{Pr}\\left(\\tilde{Y}_{1}\\right.&amp;\\left.&gt;\\tilde{Y}_{2} \\mid\\left\\{y_{i,j}\\right\\}\\right)=\\sum_{\\tilde y_{1}=0}^{\\infty} \\sum_{\\tilde y_{2}=0}^{\\tilde y_{1}-1} \\operatorname{Pr}\\left(\\tilde{Y}_{1}=y_{1}, \\tilde{Y}_{2}=y_{2} \\mid\\left\\{y_{i,j}\\right\\}\\right) \\\\ &amp;=\\sum_{\\tilde y_{1}=0}^{\\infty} \\texttt {pnbinom}\\left(\\tilde y_{1}-1 \\mid 68,45\\right) \\times \\texttt {dnbinom}\\left(\\tilde y_{1} \\mid 219,112\\right) \\end{aligned} \\] Note: if I did pnbinom(y1.tilde) in the second line that’d be right for \\(\\tilde Y_2 \\le \\tilde Y_1.\\) I want \\(\\tilde Y_2 &lt; \\tilde Y_1.\\) y1.tilde &lt;- 0:100 sum( pnbinom(y1.tilde-1, size=68, mu=68/45)* dnbinom(y1.tilde, size=219, mu=219/112) ) ## [1] 0.4820895 Wait a second. Weren’t we thinking number of children for no college should be less than number of children for college grads so shouldn’t this probability be greater than 0.5? Not really. \\(Pr(\\tilde Y_1 &lt; \\tilde Y_2)\\) will also be less than 0.5 and in fact will be less than .48 (it will be 0.3 to be exact). Remember we’re talking about a discrete distribution. if you randomly select a women with less than bachelor’s degree and a woman with bachelor’s or higher there’s a decent chance they have the SAME number of kids. # Pr(Y1 tilde &gt;= Y2 tilde ) sum( pnbinom(y1.tilde, size=68, mu=68/45)* dnbinom(y1.tilde, size=219, mu=219/112) ) ## [1] 0.6997647 Notice how \\(Pr(\\tilde Y_2 \\le \\tilde Y_1|y_{i,j})\\) is over 0.50. Here we could do Monte Carlo simulations using the rnbinom function. Or we could do rgamma and rpois. They are both correct but the longer way generalizes to more complicated problems where the PPD may not be tractable. rgamma(3, 1, 1); I get 3 draws from a gamma(1,1) distribution. rpois(10, 5); I get 10 draws from a Poisson(5) rpois(10, 6:15); I get one Poisson(6) draw one Poisson(7) draw one Poisson(8) … one Poisson(15) draw These \\(\\tilde y\\)’s are Poisson variables but each with a different mean and since the Poisson means are gamma-distributed the \\(\\tilde y\\) will be negative binomially distributed. # Birth rates example; Posterior predictive simulations a &lt;- 2 ; b &lt;- 1; n1 &lt;- 111; sum.y1 &lt;- 217; n2 &lt;- 44 ; sum.y2 &lt;- 66; theta1.sim &lt;- rgamma(10000, a+sum.y1, b+n1) theta2.sim &lt;- rgamma(10000, a+sum.y2, b+n2) y1.tilde &lt;- rpois(10000, theta1.sim) y2.tilde &lt;- rpois(10000, theta2.sim) mean(y1.tilde == y2.tilde) ## [1] 0.2254 mean(y1.tilde &gt;= y2.tilde) ## [1] 0.7052 So indeed these conclusions are consistent with our calculations from above. i.e., Women without bachelors having more kids on average than women with bachelors. Here’s an important thing: Given a Monte Carlo sample from a probability distribution, I can compute (approximations to / estimates of) any feature of that distribution that interests me. E.g., moments (means, variance, etc) probabilities, quantiles. That holds for posterior predictive distributions as well as posterior distributions. 7.2 Example: Let \\(D = \\tilde Y_1 - \\tilde Y_2\\) What does \\(D\\) mean exactly? It means this: Randomly select a 40-year-old woman with less than bachelor’s and a 40-year-old woman with bachelors or higher \\(D =\\) number of kids first woman has minus number of kids second woman has. It’s a difference between two negative binomial distributions (independent NegBinom’s). I don’t know anything about the properties of such a distribution (it doesn’t matter, I don’t need to know). We can use our samples of \\(\\tilde y_1 \\text{ and } \\tilde y_2\\) to construct the posterior predictive distribution of this difference \\(D\\)! D.tilde &lt;- y1.tilde - y2.tilde D.vals &lt;- range(D.tilde)[1]:range(D.tilde)[2] ppd.D &lt;- (table(c(D.tilde, D.vals))-1) / length(D.tilde) plot(D.vals, as.vector(ppd.D), type=&quot;h&quot;, lwd=2, xlab=&quot;D=y1.tilde-y2.tilde&quot;, ylab=&quot;p(D|y1,y2)&quot;) points(D.vals, as.vector(ppd.D), pch=19) Figure 7.1: The posterior predictive distribution of D = Y1.tilde - Y2.tilde, the diﬀerence in the number of children of two randomly sampled women, one from each of the two educational population The most frequently occurring value is 0 the next most frequent is +1. The non-college-grad has 1 more kid than the college grad. This plot would have been hard to do any way other than by Monte Carlo. The weird business in the code with the (table(c(D.tilde, D.vals))-1) is there because the table function doesn’t count missing values. # Case in point x &lt;- c(1,1,1, 2,2,2, 3,3,3, 5, 6, 8);table(x) ## x ## 1 2 3 5 6 8 ## 3 3 3 1 1 1 If we are making a histogram we would want it to show that we have zero fours. Hence the weird business. 7.3 Posterior predictive model checking What we’ve done so far is assumed number of kids | \\(\\theta\\) are independent Poisson. Here’s a question: Number of kids is not a Poisson variable. The Poisson distribution counts events in a Poisson process which occur randomly and independently of each other. That is not how people have kids at all. Why did we use it? The answer is: because number of kids is a count variable and the Poisson distribution is a simple model for count variables. You know the most famous quote in statistics? “All models are wrong, some models are useful.” The real question is: Is it useful even though it’s wrong? Posterior predictive simulations (next section) are a good tool for addressing this question. Student question: I thought that Poisson was a good distribution for rare events? Ans: It is. But not only rare events. Suppose \\(Y \\sim \\text{Poisson}(\\theta).\\) No rule against \\(\\theta\\) being a big number. \\(\\theta =\\) expected count. “Rare events” just means probability is low but if exposure is high then expected number of counts can be high as well. 7.4 Posterior predictive model checking Let’s take the first group for illustration(less than bachelor’s degree). No.kids = { 0,1,2,3,4,5,6 } Frequency = { 20,19,38,20,10,2,2 } This is actually the first time we’ve seen these data because all we needed before was sufficient statistic \\((n = 111,~ \\sum{y_i} = 217)\\). Take each of these counts divided by 111 that defines the empirical distribution. The idea of “posterior predictive model checking” is; if the model is “correct” then the observed data should not appear unusual when compared to the posterior predictive distribution. \\(p(\\tilde y | y)\\) is the conditional distribution of number of chidren conditional on the observed data. If the data really were drawn from the model that we used the empirical distribution and the PPD should show close agreement. # Posterior predictive model checking for group 1 y &lt;- 0:10 e.dist &lt;- c(20,19,38,20,10,2,2,rep(0,length(y)-7)) / 111 plot(y-.05, e.dist, type=&quot;h&quot;, lwd=5, xlab=&quot;y&quot;, ylab=&quot;Pr(Y=y)&quot;) a &lt;- 2; b &lt;- 1; n &lt;- 111; sum.y &lt;- 217; p.dist &lt;- dnbinom(y, size=a+sum.y, mu=(a+sum.y)/(b+n)) points(y+.05, p.dist, type=&quot;h&quot;, lwd=5, col=&quot;gray&quot;) legend(&quot;topright&quot;, inset=.1, lwd=5, col=c(&quot;black&quot;,&quot;gray&quot;), legend=c(&quot;Empirical dist&quot;, &quot;Predictive dist&quot;)) Figure 7.2: Evaluation of model it. This shows the empirical and posterior predictive distributions of the number of children of women without a bachelor’s degree. the number of women with exactly two children is 38, which is twice the number of women in the sample with one child. In contrast, this group’s posterior predictive distribution, shown in gray, suggests that the probability of sampling a woman with two children is slightly less probable than sampling a woman with one. Should we be concerned about this? First, is it really a disagreement? There are two possible answers: It is a result of sampling variability. It is indeed a feature of the population, hence our model is wrong. The answer is based on frequentist hypothesis testing. We will simulate \\(S\\) replicated data sets (Remember \\(S\\) is a big number). For each \\(\\theta^{(s)} \\sim p(\\theta | y),\\) and for each replicated data set compute the test statistic. What test statistic to use? Whatever feature of the data you supsect the model is not capturing correctly. In this case we use \\(t(y_{obs}) = 38 / 19 = 2.0\\). What does the posterior predictive distribution of this test statistic look like? and how unusual a value is 2.0? # Define the test statistic t.sim &lt;- rep(NA, 10000) for(s in 1:10000) { theta.sim &lt;- rgamma(1, a+sum.y, b+n) y.tilde &lt;- rpois(n, theta.sim) t.sim[s] &lt;- sum(y.tilde==2) / sum(y.tilde==1) } hist(t.sim, freq=F, right=F, breaks=30, &quot;&quot;) abline(v=2, lty=2) Figure 7.3: The posterior predictive distribution of the empirical odds of having two children versus one child in a dataset of size n = 111 How often in these replicated(simulated) data sets does it happen that there are double the number of cases with \\(y=2\\) as with \\(y=1?\\) The answer is not very often. The observed test statistic \\(t(y_{obs}) = 2.0\\) is kind of out in the tail of this distribution. mean(t.sim &gt;= 2) ## [1] 0.0059 This quantity 0.0051 that’s the tail probability. This value is called a “Bayesian p-value”. This “Bayesian p-value” has nothing to do with Bayesian inference it’s strictly a tool in Bayesian model checking. We did the posterior predictive checks we found Bayesian p-value = .005 (close to zero). Based on this which bullet from above is the right one? It’s the second one! If this was to be expected due to sampling variability then it would have occurred with some frequency among the replicated data sets. But it didn’t! So this feature of the data “\\(y=2\\) with much greater frequency than \\(y=1\\)” is not explained by the model. So the conclusion is; This is indeed a deficiency of the Poisson model. To the extent that there our features of our data set that are not shared by the replicated data sets this MAY suggest a problem. We’re still back to the original question. How concerned should we be? Is predicting frequency of one kid and two kids an important goal of our inference? If yes, then we’ve got a problem because the Poisson model is not going to give accurate predictions here. It will underpredict the frequency of \\(y=2\\) and overpredict the frequency of \\(y=1\\). However if what we really care about is estimation of mean and variance of “number of kids” then this is not such a problem. So in terms of what’s the next step in this data analysis? The answer is: it depends. Thus endeth our discussion of Chapter 4. On to the next thing Chapter 5. "],["normal-mean.html", "Lecture 8 Normal Mean 8.1 Example: women’s height 8.2 Inference for the mean, conditional on the variance 8.3 Prediction 8.4 Example: Midge wing length", " Lecture 8 Normal Mean The following notes, mostly transcribed from Neath(0512,2021) lecture, summarize sections(5.1 and 5.2) of Hoff(2009). Where we been? where we going? Binomial model? check! Poisson model? check! Normal model? Next! Unlike Binomial and Poisson which are for discrete data, the normal distribution is a continuous distribution. It is completely characterized by the mean and standard deviation. Our notation will be mean \\(= \\mu = \\theta\\), standard deviation \\(= \\sigma\\), variance \\(= \\sigma^2\\). Normal distribution calculations are easy to do in R use for density values for cdf-values; \\(Pr(Y \\le y | \\theta, \\sigma^2) = \\texttt{pnorm}(y, \\texttt{mean}=\\theta, \\texttt{sd} =\\sqrt{\\sigma^2} )\\) # Reproduce Figure 5.1 of Hoff (2009) y &lt;- seq(0, 10, .05) plot(y, dnorm(y, mean=2, sd=.5), type=&quot;l&quot;, lty=1, lwd=2, col=&quot;black&quot;, xlab=&quot;y&quot;, ylab=&quot;p(y|theta, sigma2)&quot;);abline(h=0) lines(y, dnorm(y, mean=5, sd=2), lty=2, lwd=2, col=&quot;red&quot;) lines(y, dnorm(y, mean=7, sd=1), lty=3, lwd=2, col=&quot;blue&quot;) legend(&quot;topright&quot;, inset=.05, col=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lwd=2, lty=c(1,2,3), legend=legend) Figure 8.1: Some normal densities. A justification for the frequency with which the normal distribution is encountered in real-world applications is that any variable that is itself a sum or an average of a whole bunch of other variables (not necessarily observable quantities) will be well approximated by a normal distribution that’s because of the Central Limit Theorem. In practice, this means that the normal sampling model will be appropriate for data that result from the additive effects of a large number of factors. 8.1 Example: women’s height 1893 through 1989 (end of 19th century) women’s heights in inches in England. library(alr4) y &lt;- Heights$dheight c(mean(y), sd(y)) ## [1] 63.751055 2.600053 You don’t need to have an argument like 05a slide 7 at the ready to justify using a normal distribution. If you have enough observations plot the data and see! If the histogram looks like a bell curve or even better if the “normal probability plot” or the so-called QQ-plot which stands for quantile-quantile resembles a straight line then a normal model is appropriate. yvals &lt;- seq(min(y)*0.95, max(y)*1.05, length=100) hist(y, freq=F, right=F, breaks=20, xlab=&quot;Height in inches&quot;, ylab=&quot;&quot;, main=&quot;&quot;, col=&quot;gray&quot;) lines(density(y), lty = 2, lwd=2, col = &quot;red&quot;) lines(yvals, dnorm(yvals, mean(y), sd(y)), lwd=2) legend(&quot;topright&quot;, legend=c(&quot;population&quot;,&quot;sample&quot;), col=c(1,&quot;red&quot;), lwd=2, lty=c(1,2), bty=&quot;n&quot;) Figure 8.2: Height data and a normal density qqnorm(y) 8.2 Inference for the mean, conditional on the variance Our interest in this course is; How do you do Bayesian inference about the mean \\(\\theta\\) and the variance \\(\\sigma^2\\) in a normal model. \\[ p(y_1,...,y_n|\\theta, \\sigma^2)=\\left(2 \\pi \\sigma^{2}\\right)^{-n / 2} \\exp \\left\\{-\\frac{1}{2} \\sum\\left(\\frac{{y_{i}-\\theta}}{\\sigma}\\right)^{2}\\right\\} \\] From this we can see a two-dimensional sufficient statistic. Because the density only depends on the data set through \\(\\sum y_i\\) and \\(\\sum y_i^2\\). That means \\(\\sum y_i\\) and \\(\\sum y_i^2\\) are a sufficient statistic and since there exists a one to one mapping between these two statistics and \\((\\bar y, s^2)\\) that means \\((\\bar y, s^2)\\) is a sufficient statistic. \\[\\bar y=\\frac{1}{n}\\sum y_i ~; \\quad s^2=\\frac{1}{n-1}\\sum(y_i-\\bar y)^2\\] This is a feature of the normal distribution. We will take the problem of inference about \\((\\bar y, s^2)\\) and break it into two pieces. How will we do that? We will write \\(p(\\theta , \\sigma^2 | y_1, …, y_n)= p(\\theta | \\sigma^2, y_1, …, y_n) \\times p( \\sigma^2 | y_1, …, y_n).\\) Today we will only work on the first one; that is “inference about the mean of a normal distribution assuming the variance is known” or equivalently “inference about the normal mean conditional on the variance.” \\[ p(y_1,...,y_n|\\theta,\\sigma^2) = c\\times e^{-\\frac{1}{2\\sigma^2}\\sum (y_i - \\theta)^2} \\propto e^{c_1(\\theta - c_2)^2} \\] The “likelihood” as a function of \\(\\theta\\) is \\(e^{\\text{quadratic in }\\theta}.\\) So the conjugate prior \\(p(\\theta|\\sigma^2)\\) will be a distribution whose density consists of \\(e^{\\text{quadratic in }\\theta}\\). What probability distribution has a density that is \\(e^{\\text{quadratic thing }}?\\) The normal distribution! We have just proven the conjugate prior for the mean in a normal sampling model is the normal distribution. Suppose \\(\\theta \\sim \\text{Normal}( \\mu_0, \\tau_0^2 ),\\) where \\(\\tau_0^2\\) is the prior variance, \\(\\{Y_1, …, Y_n | \\theta\\}\\sim\\text{iid Normal}( \\theta, \\sigma^2)\\) (where \\(\\sigma^2\\) is known) then \\(\\{\\theta | y_1, …., y_n\\} \\sim \\text{Normal}\\) because it’s a conjugate prior. i.e., \\[ p(\\theta | y_1, …, y_n, \\sigma^2) \\propto \\exp\\bigg\\{ -\\frac{1}{2} \\bigg( \\frac{\\theta-b/a}{1/\\sqrt{a}}\\bigg)^2 \\bigg\\} \\] where \\(a = (1/\\tau_0^2)+(n/\\sigma^2) \\text{ and } b = (\\mu_0/\\tau_0^2)+(\\sum y_i/\\sigma^2).\\) Thus \\(p(\\theta | y, \\sigma^2)\\) has the same shape as a normal density with mean of \\(b/a\\) and a standard deviation of \\(1/\\sqrt a\\) therefore \\(\\{\\theta | y, \\sigma^2\\}\\) is normally distributed with mean \\(= b/a\\) and sd \\(= 1/\\sqrt a.\\) So we have \\(\\{\\theta | y_1, …, y_n , \\sigma^2\\} \\sim \\text{ Normal}(\\mu_n , \\tau_n^2)\\). So there’s a very sensible notational convention being employed here \\(\\mu_0\\) is prior mean (after observing 0 data) \\(\\mu_n\\) is posterior mean after observing \\(n\\) data points. \\(\\tau_0^2\\) is the prior variance prior to observing data (after observing 0 cases), \\(\\tau_n^2\\) is the variance after observing the data \\(n\\) observations. \\(\\tau_n^2 &lt; \\tau_0^2, ~~\\mu_n\\) should be an average between \\(\\mu_0\\) and \\(\\bar y\\). Let’s interpret the parameters in the posterior distribution. First for the posterior variance we have; \\(1 / \\tau_n^2 = 1 / \\tau_0^2 + n / \\sigma^2\\). It’s not the variances that add together, it’s the inverses added together, which makes sense because the uncertainty is decreasing. Definition: precision = 1 / variance. Think of precision as quantifying information and we have; posterior information = prior information + data information = \\((1/\\tau_0^2)+(n/\\sigma^2)=a\\). Data information = information in \\(n\\) observations times the information in a single observation. Data information is also the information in a single observation of \\(\\bar y,~ \\bar y \\sim \\text{Normal}(\\theta, \\sigma^2 / n)\\) so the variance of \\(\\bar y\\) is \\(\\sigma^2 / n\\) so the precision for \\(\\bar y\\) is \\(n / \\sigma^2\\) so the information contained in \\(\\bar y\\) is \\(n / \\sigma^2=\\) data information. What about the posterior mean? \\[ \\mu_{n}=\\frac{1/{\\tau}_{0}^{2}}{1/{\\tau}_{0}^{2}+n/{\\sigma}^{2}} \\mu_{0}+\\frac{n/{\\sigma}^{2}}{1/{\\tau}_{0}^{2}+n /{\\sigma}^{2}} \\bar{y} \\] \\(\\mu_n =\\) weighted average of \\(\\mu_0\\) and \\(\\bar y\\). Weight given to \\(\\mu_0\\) is proportional to \\(1/\\tau_0^2\\)(the prior precision), weight given to \\(\\bar y\\) is proportional to \\(n / \\sigma^2\\) the “data information.” 8.3 Prediction Let \\(\\tilde Y\\) be an (\\(n+1\\))st observation that has not as yet been observed but which we wish to predict based on observed values of \\(Y_1, …, Y_n\\). To find the posterior predictive distribution, we use the fact that \\[ \\tilde Y|\\theta,\\sigma^2 \\sim N(\\theta, \\tilde e) \\iff \\tilde Y=\\theta + \\tilde e \\text{ where } \\tilde e \\sim N(\\theta, \\sigma^2) \\] The posterior predictive mean and variance of \\(\\tilde Y\\) are \\[ \\begin{aligned} \\mathrm{E}\\left[\\tilde{Y} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] &amp;=\\mathrm{E}\\left[\\theta+\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\mathrm{E}\\left[\\theta \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right]+\\mathrm{E}\\left[\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\mu_{n}+0=\\mu_{n} \\end{aligned} \\] and \\[ \\begin{aligned} \\operatorname{Var}\\left[\\tilde{Y} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] &amp;=\\operatorname{Var}\\left[\\theta+\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\operatorname{Var}\\left[\\theta \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right]+\\operatorname{Var}\\left[\\tilde{\\epsilon} \\mid y_{1}, \\ldots, y_{n}, \\sigma^{2}\\right] \\\\ &amp;=\\tau_{n}^{2}+\\sigma^{2} \\end{aligned} \\] So \\(\\{\\tilde Y | y_1, …., y_n, \\sigma^2\\} \\sim \\text{Normal}(\\mu_n, \\tau_n^2+\\sigma^2).\\) Note that there are two sources of uncertainty (variance) in our predictions (1) we don’t know what \\(\\theta\\) is! \\(\\theta|y \\sim \\text{Normal}(\\mu_n , \\tau_n^2 )\\) (2) even if we knew \\(\\theta\\) exactly \\(\\tilde Y \\sim \\text{Normal}( \\theta , \\sigma^2 )\\) won’t equal \\(\\theta\\) exactly. 8.4 Example: Midge wing length Goal is: Estimate the mean wing length for a species of midge (a fly), i.e., we wish to make inference about the population mean \\(\\theta\\). Current data: \\(n = 9\\) observations, \\(\\bar y = 1.804\\) Prior information : For other similar species the mean wing length is about \\(1.9\\)mm so this will be our prior mean \\(\\mu_0\\). In this case, \\(\\theta =\\) mean wing length, which means \\(Pr(\\theta &gt; 0) = 1.\\) This is not a property of the normal model since the normal model spans both negative and positive values. So we need to find a way to get our prior to have mass only on \\(\\theta &gt; 0\\). Our prior variance is gotten by; back into \\(\\tau_0\\) so that prior probability of \\(\\theta &lt; 0\\) is small. So we want \\[ \\mu_0-2\\tau_0&gt;0\\implies1.9&gt;2\\tau_0\\implies 0.95 &gt; \\tau_0 \\] So we will use \\(\\tau_0 = .95\\) as our prior variance. We have a prior distribution \\(\\theta \\sim \\text{Normal}( \\mu_0 = 1.9,\\tau_0^2 = .95^2)\\), we have data \\(\\{Y_1, …, Y_9 | \\theta\\} \\sim \\text{ iid Normal}(\\theta, \\sigma^2)\\). The posterior is \\(\\{\\theta | y_1, …, y_9, \\sigma^2\\} \\sim \\text{Normal}( \\mu_n, \\tau_n^2 )\\) where \\(1/\\tau_n^2 = (1 / \\tau_0^2) + (n/\\sigma^2).\\) We need \\(\\sigma^2\\) to finish this problem. Set \\(\\sigma^2 = \\text{sample variance} = s^2\\). options(digits = 4) y &lt;- c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08) # Prior mu.0 &lt;- 1.9; tau.0 &lt;- 0.95; tau2.0 &lt;- tau.0^2; # Calculations ybar &lt;- mean(y) s2 &lt;- var(y) # = sum((y-mean(y))^2)/(length(y)-1) n &lt;- length(y) sigma &lt;- sqrt(s2) mu.n &lt;- (mu.0/tau2.0 + n*ybar/s2) / (1/tau2.0 + n/s2) tau2.n &lt;- 1 / (1/tau2.0 + n/s2) c(ybar, s2) ## [1] 1.80444 0.01688 c(mu.n, tau2.n) ## [1] 1.804643 0.001871 So \\(\\{\\theta|y_1,...,y_9,\\sigma^2=0.017\\} \\sim \\text{Normal}(1.805,0.002)\\) Notice \\(\\bar y = 1.804,\\) \\(\\mu_0 = 1.9,\\) and \\(\\mu_0 = 1.805.\\) So we’re giving a lot more weight to the data than to the prior. This makes sense because our 9 observations are from the species we’re interested in. However, our prior was a similar species but not the one we’re interested in. # 95% posterior interval; qnorm(c(.025, .975), mu.n, sqrt(tau2.n)) (CI &lt;- mu.n+c(-1,1)*1.96*sqrt(tau2.n)) ## [1] 1.720 1.889 theta &lt;- seq(0, 4, .01) plot(theta, dnorm(theta, mean=mu.n, sd=sqrt(tau2.n)), type=&quot;l&quot;, lwd=2, ylab=&quot;p(theta|y,sigma2=0.017)&quot;) lines(theta, dnorm(theta, mean=mu.0, sd=sqrt(tau2.0)), lwd=2, col=&quot;gray&quot;) abline(v=CI, lty = 3, col = &quot;gray&quot;) legend(&quot;topright&quot;, inset=.05, lwd=2, col=c(&quot;gray&quot;, &quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;)) Figure 8.3: Prior and conditional posterior distributions for the population mean wing length. We have very high posterior belief that the mean wing length is close to 1.8. In this example, we were pretending that the population variance \\(\\sigma^2\\) (variance of midge wing lengths ) was known to be \\(s^2 =0.017\\). In fact we don’t know it! As a result, it is possible that this interval is narrower than it should be because it fails to account for our uncertainty about the population variance \\(\\sigma^2.\\) We deal with this in the next lecture. "],["joint-inference-for-normal-mean-and-variance.html", "Lecture 9 Joint inference for Normal mean and variance 9.1 Marginal posterior of \\(\\sigma^2\\) 9.2 Example: Midge wing length 9.3 Monte Carlo sampling 9.4 Summary of Normal formulas", " Lecture 9 Joint inference for Normal mean and variance The following notes, mostly transcribed from Neath(0513,2021) lecture, summarize section(5.3) of Hoff(2009). What if we don’t really care about the variance? We’re only interested in doing inference about the mean anyway? Can we use the methods from the last lesson? NO we should not. Unless of course we genuinely do know the population variance. So even if our inferential goals don’t include the variance and we’re only interested in the mean we still have to account for the fact that the variance is unknown(there’s uncertainty about the variance) to do valid inference about the mean. In this case we say the variance is a nuisance parameter. So let’s talk about how to do that. Model: We have \\(n\\) exchangeable observations from a population that is normal with a mean of \\(\\theta\\) and a variance of \\(\\sigma^2\\) both of which are unknown. Given a prior distribution on \\((\\theta, \\sigma^2)\\), we use Bayes rule to compute a posterior distribution where the prior describes our belief before observing the sample data the posterior will describe our belief after observing the data. Is there a conjugate distribution for \\((\\theta, \\sigma^2)\\) together? Last class we saw that conditional on \\(\\sigma^2\\) the conjugate prior for \\(\\theta\\) was the normal distribution. Let’s use that fact going forward. Let’s write our prior distribution as the joint density \\(p(\\theta, \\sigma^2) = p(\\theta|\\sigma^2)p(\\sigma^2) = \\texttt{dnorm} (\\theta|\\mu_0,\\tau_0^2)p(\\sigma^2).\\) Since we are conditioning on \\(\\sigma^2\\) anyway let’s set \\(\\tau_0^2=\\sigma^2/\\kappa_0,\\) where \\(\\kappa_0 =\\) number of prior observations, this way we have described(parameterized) our uncertainty about \\(\\theta\\) conditionally on \\(\\sigma^2\\). If \\(\\theta | \\sigma^2 \\sim \\text{Normal}( \\mu_0 , \\sigma^2 / \\kappa_0 )\\) and data are \\(\\{y_1, …, y_n | \\theta, \\sigma^2\\} \\sim \\text{ iid Normal}( \\theta, \\sigma^2)\\). Equivalently in terms of the sufficient statistic \\(\\{\\bar y | \\theta, \\sigma^2\\} \\sim \\text{Normal}( \\theta, \\sigma^2 / n)\\) The posterior of \\(\\theta, \\{\\theta | y, \\sigma^2\\} \\sim \\text{Normal}\\) with mean \\(\\mu_n = (\\kappa_0\\mu_0+n\\bar y)/\\kappa_n\\) where \\(\\mu_n\\) is a weighted average of prior mean \\(\\mu_0\\) and data mean \\(\\bar y\\) and these two weights are proportional to \\(\\kappa_0\\) (prior sample size) and \\(n\\) (“real data” sample size). From previous lecture we found that the posterior variance \\(\\tau_n^2 = 1/a=1/(1/\\tau_0^2+n/\\sigma^2)\\). Now plug in \\(\\tau_0^2 = \\sigma_0^2 / \\kappa_0\\) and you get \\(\\tau_n^2 = 1 / ( \\kappa_0 / \\sigma^2 + n / \\sigma^2 ) = \\sigma^2 / (\\kappa_0 + n)=\\sigma^2/\\kappa_n\\). This makes perfect sense! Our data consist of \\(n\\) observations. Our prior consists of \\(\\kappa_0\\) “observations.” So finally, we have \\[\\{\\theta | y_1,...,y_n, \\sigma^2\\} \\sim \\text{Normal}( \\mu_n = (\\kappa_0\\mu_0+n\\bar y)/\\kappa_n, ~\\sigma^2/\\kappa_n)\\] We’re half way there. \\(p(\\theta , \\sigma^2) = p(\\theta | \\sigma^2)p(\\sigma^2) = \\texttt{dnorm}( \\theta | \\mu_0 , \\sqrt{\\sigma^2/\\kappa_0} ) p(\\sigma^2)\\) What might be the conjugate prior for the variance \\(\\sigma^2?\\) The gamma distribution has support (0, infinity). Might this work? It does not. It turns out. The gamma distribution is not conjugate for the variance, \\(\\sigma^2\\). However, the gamma distribution is conjugate for the precision = 1 / variance. How did we get this? Condition on the likelihood \\(p(\\sigma^2 | y) \\propto p(y | \\sigma^2) p(\\sigma^2),\\) and look at what the likelihood contributes. Recall, \\(p(y|\\theta,\\sigma^2) = (\\sqrt{2\\pi\\sigma^2})^{-1}\\text{exp}\\{-(1/2\\sigma^2)(y-\\theta)^2\\} \\propto (\\sigma^2)^{-1}\\text{exp}(-1/\\sigma^2),\\) and the gamma distribution has the form \\(p(\\theta) \\propto \\theta^{a-1}e^{-b\\theta}.\\) So what the likelihood contributes in this case for a normal variance is \\((\\sigma^2)^{\\text{-something}} \\times e^{- \\text{something} / \\sigma^2}.\\) Since it’s not \\((\\sigma^2)^{\\text{something}} \\times e^{- \\sigma^2/\\text{something} },\\) then it’s not \\(\\sigma^2\\) that has a gamma distribution but rather the precision, \\(1/\\sigma^2\\). Definition: If \\(X \\sim\\) gamma\\((a, b)\\) and \\(Y = 1/X\\) then \\(Y \\sim\\) InvGamma\\((a, b).\\) The conjugate prior for the normal variance, \\(\\sigma^2\\) is to say \\(1/\\sigma^2 \\sim \\text{gamma}(a, b)\\) and since the parameter \\(a &gt; 0, ~ b &gt; 0\\) are arbitrary we can reparameterize this distribution to gamma\\(( \\nu_0 / 2 , ~ \\nu_0\\sigma_0^2 / 2)\\). We could do this by \\(\\nu_0 = 2a, ~ \\sigma_0^2 = b / a\\). The reason we did this reparameterization is that this gives a more natural way to think about the variance. We know the mean and variance of the gamma distribution are \\(a/b\\) and \\(a/b^2\\) so we have; \\(E(1/\\sigma^2) = 1/ \\sigma_0^2\\) Var\\(( 1 / \\sigma^2) = 2 / [ \\nu_0 \\times (\\sigma_0^2)^2 ]\\) What do all these parameters in this prior distribution represent? \\(\\mu_0\\) is prior best guess at \\(\\theta\\) \\(\\kappa_0\\) measures the strength of that belief \\(\\sigma^2_0\\) is our prior best guess at \\(\\sigma^2\\) \\(\\nu_0\\) measures the strength of that belief Remember that in inference for the normal distribution, inference about the mean and inference about the variance proceed “independently” in a sense \\((\\bar Y \\perp s^2)\\). So we’re allowed to have different prior sample sizes for the mean and the variance. Our data consist of \\(n\\) observations from the population so the data will contribute \\(n\\) to both the mean and the variance. The prior contributes \\(\\kappa_0\\) to the mean and \\(\\nu_0\\) to the variance. No requirement that these be equal. The posterior density satisfies \\(p(\\theta, \\sigma^2 | y) = p(\\theta | \\sigma^2, y)p(\\sigma^2 | y)\\). The first piece is already solved! \\(p(\\theta | \\sigma^2, y) = \\texttt{dnorm}( \\theta | \\mu_n , \\sqrt{\\sigma^2 / \\kappa_n })\\). Now for the second piece. 9.1 Marginal posterior of \\(\\sigma^2\\) The result is that \\(1/\\sigma^2 | y \\sim \\text{gamma}( \\nu_n / 2 , \\nu_n \\sigma_n^2 / 2 )\\). So that’s why that reparameterization was so useful. \\(\\nu_0\\) in the prior becomes \\(\\nu_n = \\nu_0 + n\\) in the posterior the \\(\\sigma_0^2\\) in the prior becomes \\(\\sigma_n^2\\) in the posterior which is: \\[\\sigma_n^2 = \\frac{1}{\\nu_n}[\\nu_0\\sigma_0^2 + (n-1)s^2+\\frac{\\kappa_0n} {\\kappa_n}(\\bar y - \\mu_0)^2]\\] It’s almost a weighted average of the “prior” variance \\(\\sigma_0^2\\) and the “data variance” \\(s^2\\). \\(\\nu_n = \\nu_0 + n = \\nu_0 + (n-1) + 1\\). The prior variance gets weight proportional to \\(\\nu_0\\). The sample variance \\(s^2\\) gets weight proportional to \\((n-1)\\). That extra piece is weird. It only gets weight of \\(1 / \\nu_n &lt; (n-1) / n\\). 9.2 Example: Midge wing length Our prior best guesses at the mean and variance for this population are \\(\\mu_0 = 1.9\\) and \\(\\sigma_0 = 0.01\\) based on studies of other populations. Our data consist of \\(n\\) observations. Our prior belief is based on not anything we have a whole lot of confidence in, but as long as we set \\(\\kappa_0\\) and \\(\\nu_0\\) to be small relative to \\(n=9\\) they won’t get much weight in the posterior anyway. Set \\(\\nu_0 = 1, ~ \\kappa_0 = 1\\) so prior gets 10% weight and data gets 90% weight in the posterior. That seems reasonable. y &lt;- c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08) # Prior mu.0 &lt;- 1.9; sigma.0 &lt;- 0.1; sigma2.0 &lt;- sigma.0^2; nu.0 &lt;- 1; kappa.0 &lt;- 1 ; # Calculations n &lt;- length(y); ybar &lt;- mean(y); s2 &lt;- var(y); nu.n &lt;- nu.0 + n ; kappa.n &lt;- kappa.0 + n; mu.n &lt;- (kappa.0*mu.0 + n*ybar) / kappa.n sigma2.n &lt;- (1/nu.n) * (nu.0*sigma2.0 + (n-1)*s2 + kappa.0*n*(ybar-mu.0)^2 / kappa.n) c(mu.n, sigma2.n, sqrt(sigma2.n)) ## [1] 1.81400 0.01532 0.12379 Our posterior belief about \\((\\theta , \\sigma^2)\\) is described by the joint probability distribution: \\(\\{\\theta|y_1,...,y_n,\\sigma^2\\} \\sim \\text{Normal}(\\mu_n, \\sigma^2/\\kappa_n) = \\text{Normal}(1.814, \\sigma^2/10)\\) \\(\\{1/\\sigma^2|y_1,...,y_n\\} \\sim \\text{gamma}(\\nu_n/2, \\nu_n\\sigma^2_n/2)=\\text{gamma}(10/2, 10\\times 0.015/2)\\) Joint density of \\((\\theta, \\sigma^2)\\) I want to draw a picture of the joint density \\(p(\\theta, \\sigma^2 | y)\\) but this is a three-dimensional figure and I have a two-dimensional dimensional monitor on my laptop. There’s lots of ways to do this. The one I happen to like is kind of old fashioned and that is the contour plot. A contour plot is defined by; If \\(f(x, y)\\) is a joint density for random variables \\((X, Y)\\) then find the mode of this density (the peak) and then find all the points \\((x, y)\\) such that \\(f(x, y) = 0.95 \\times f(x.\\text{mode}, y.\\text{mode})\\). That is all the points where the density is 95% of the peak value. Then draw a line connecting those points. Assuming a unimodal distribution, that will be the innermost contour. Then do this again for 90% of the peak value, and 85% of the peak value down to 0.001% of the peak value. # These values arrived at by lots of trial and error gs &lt;- 800 theta &lt;- seq(1.5, 2.1, length=gs) I.sig2 &lt;- exp(seq(log(1), log(250), length=gs)) sigma2 &lt;- 1 / exp(seq(log(1000), log(11), length=gs)) # Do mean and precision first log.post &lt;- matrix(NA, gs, gs); for(i in 1:gs){ for(j in 1:gs){ log.post[i,j] &lt;- dnorm(theta[i], mu.n, 1/sqrt(I.sig2[j]*kappa.n), log=T) + dgamma(I.sig2[j], nu.n/2, nu.n*sigma2.n/2, log=T) }} maxie &lt;- max(log.post) log.post &lt;- log.post - maxie post.P &lt;- exp(log.post) rm(maxie); rm(log.post) # Now do mean and variance log.post &lt;- matrix(NA, gs, gs); for(i in 1:gs){ for(j in 1:gs){ log.post[i,j] &lt;- dnorm(theta[i], mu.n, sqrt(sigma2[j]/kappa.n), log=T) + dgamma(1/sigma2[j], nu.n/2, nu.n*sigma2.n/2, log=T) - 2*log(sigma2[j]) }} maxie &lt;- max(log.post) log.post &lt;- log.post - maxie post.V &lt;- exp(log.post) contours &lt;- c(.001, .01, seq(.05, .95, .10)) rm(maxie);rm(log.post) par(mar=c(3,3,2,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0),mfrow=c(1,2)) contour(theta, I.sig2, post.P, levels=contours, drawlabels=F, xlab=xlab, ylab=ylab2, main=&quot;Mean and precision&quot;) contour(theta, sigma2, post.V, levels=contours, drawlabels=F, xlab=xlab, ylab=ylab1, main=&quot;Mean and variance&quot;) Figure 9.1: Joint posterior distributions of (theta,precision) and (theta, sigma^2). The posterior mode of (mean, precision) \\((\\theta, 1/\\sigma^{2} )\\) is at (1.8 and 60) or so. The innermost contour is the set of all points whose joint density is .95 times that peak value. The next contour shows all the points whose joint density is .85 times the max value. There should be 12 contours in this picture. They are the .95, .85, … .15, .05, .01, .001. This is the equivalent of Fig 5.4 in the book but the author doesn’t show as much as I have here there’s no .001 contour. I always like to see the tails. What do we know about this distribution? We know that conditional on \\(\\sigma^2, ~ \\theta\\) is Normal. What that means is that every horizontal slice from this joint distribution is a bell curve. Remember that if \\(f(x,y)\\) is the joint density of \\((X,Y)\\) the conditional density of \\(Y | X=x_0\\) is found by taking the vertical slice of the joint density at the point \\(x=x_0\\). Similarly, I’ve got in this picture the joint posterior of \\((\\theta, \\sigma^{-2})\\) the conditional of \\(\\theta\\) given \\(\\sigma^2\\) is a horizontal slice. Given that the shape makes perfect sense. The bigger is \\(\\sigma^2\\) (smaller is \\(1/\\sigma^2\\)) the weaker is our belief about \\(\\theta\\), hence the wideness of the LHS plot. The smaller is \\(\\sigma^2\\) (the bigger is \\(1/\\sigma^2\\)) the stronger our belief about \\(\\theta\\), hence the peak on the LHS plot. What the belief is does not depend on \\(\\sigma^2\\) that belief in “\\(\\theta\\) is about 1.805 or so.” Notes on the code I did \\(800 \\times 800\\) calculations of the joint density. For numerical reasons it is a good practice to compute log-densities then subtract the max log-density off of every value THEN exponentiate back. post.P represents joint posterior of mean and precision post.V is the joint posterior of mean and variance The calculation is; \\(p(\\theta, \\sigma^2) = p(\\theta | \\sigma^2, y) p(\\sigma^2 | y)\\) or \\(\\log[ p(\\theta, \\sigma^2 | y) ] = \\log[ p(\\theta | \\sigma^2, y) ] + \\log[ p(\\sigma^2 | y) ]\\) Question: why do we need to subtract off \\(2 \\log(\\sigma^2)?\\) in the calculation of post.V? Let \\(V =\\) variance, \\(P =\\) precision, \\(V = 1/P\\), \\(P = 1/V\\). The random variable \\(P\\) has a gamma distribution the random variable \\(V\\) has an inverse-gamma distribution. The probability density of \\(V\\) is the gamma pdf evaluated at \\(1/v \\times 1/v^2\\) because that’s the Jacobian. Look at “nonlinear transformations of random variables” from probability theory for more insight. 9.3 Monte Carlo sampling Of course, In the above problem the calculations were not easy but possible. But going forward when we get to the really messy problems we’re gonna have no choice but to do Monte Carlo. I want a Monte carlo sample that is \\(\\theta^{(1)} , ..., \\theta^{(S)}\\) such that \\(\\theta^{(s)} \\sim p(\\theta |y)\\). The problem is; I don’t know the marginal posterior of \\(\\theta\\), \\(p(\\theta | y).\\) I know \\(p(\\theta | y, \\sigma^2)\\) and I know \\(p(\\sigma^2 | y)\\), and that’s enough. It just means each simulation is gonna require two steps. First simulate \\(\\sigma^{2(s)}\\) and then simulate \\(\\theta^{(s)} \\sim p(\\theta |y, \\sigma^{2(s)}).\\) The result is \\((\\sigma^{2(s)} , \\theta^{(s)}) \\sim p(\\theta , \\sigma^2 | y)\\) which means marginally \\(\\theta^{(s)} \\sim p(\\theta | y).\\) S &lt;- 10000 sigma2.sim &lt;- 1 / rgamma(S, nu.n/2, nu.n*sigma2.n/2) theta.sim &lt;- rnorm(S, mu.n, sqrt(sigma2.sim/kappa.n)) # Scatterplot; empirical joint distribution of MC sample contour(theta, sigma2, post.V, levels=contours, drawlabels=F, xlab=xlab, ylab=ylab1, main=&quot;Mean and variance&quot;) points(theta.sim, sigma2.sim, pch=19, cex=.25, xlim=c(1.51, 2.11), ylim=c(0, .10)) # Marginal density estimates, and 95% CI for theta (CI.theta &lt;- quantile(theta.sim, c(.025, .975))) ## 2.5% 97.5% ## 1.727 1.901 I actually know the theoretical marginal distribution of \\(\\sigma^2\\). But if I didn’t I could draw a histogram of the Monte Carlo sample or a kernel density estimate based on the monte carlo samples as below. par(mar=c(3,3,1.5,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0),mfrow=c(1,2)) hist(sigma2.sim, xlim=c(0, .08), main=&quot;&quot;, freq=F, xlab=ylab1, ylab=ylab3, ylim=c(0,60), border=&quot;lightpink1&quot;,col=&quot;pink&quot;) lines(density(sigma2.sim), lwd = 2) hist(theta.sim, freq=F, xlim=c(1.6, 2.0), main=&quot;&quot;, xlab=xlab, ylab=ylab4, ylim=c(0,10), border=&quot;lightpink1&quot;,col=&quot;pink&quot;) lines(density(theta.sim), lwd=2) abline(v=CI.theta, lty=2, lwd=1.5) Figure 9.2: Monte Carlo samples from and estimates of the joint and marginal distributions of the population mean and variance. The vertical lines in the third plot give a 95% quantile-based posterior interval for theta The 95% Bayesian confidence interval is \\([1.72, 1.90]\\). In the last lecture, when we were pretending \\(\\sigma^2\\) was known, the CI was \\([1.720, 1.889]\\). Furthermore, the frequentist interval based on the t-distribution is \\([1.70, 1.90]\\). Our Bayesian interval brings the lower bound up just a tad (because the prior mean was 1.9 vs data mean of 1.8 or so) and it shortens the CI a bit because our prior counts for one observation so our “Bayesian sample size” was 10 not just 9. 9.4 Summary of Normal formulas \\(p(\\theta,\\sigma^2 | y_1,...,y_n) \\propto p(\\theta,\\sigma^2) p(y_1,...,y_n| \\theta,\\sigma^2)\\) \\(p(\\theta,\\sigma^2)=p(\\theta|\\sigma^2)p(\\sigma^2) = \\texttt{dnorm} (\\theta,\\mu_0,\\tau_0 = \\sigma/\\sqrt{\\kappa_0})p(\\sigma^2)\\) \\(1/\\sigma^2 \\sim\\) gamma\\((\\nu_0/2, \\nu_0 \\sigma_0^2/2)\\) \\(\\{\\theta|\\sigma^2\\} \\sim\\) normal\\((\\mu_0, \\sigma^2/\\kappa_0) \\equiv \\text{normal}(\\mu_0, \\tau_0^2)\\) if \\(\\{Y_1,\\ldots,Y_n\\} \\sim\\) i.i.d. normal\\((\\theta,\\sigma^2)\\) then \\(\\{1/\\sigma^2|y_1,...,y_n\\} \\sim \\text{gamma}(\\nu_n/2, \\nu_n \\sigma^2_n/2) \\equiv\\sigma^2_{mc} \\sim 1 / \\texttt{rgamma}(S, \\nu_n/2, \\nu_n \\sigma^2_n/2)\\) where \\(\\nu_n = \\nu_0 + n\\) \\(\\{ \\theta|y_1,...,y_n, \\sigma^2\\} \\sim \\text{normal}(\\mu_n, \\sigma^2 / \\kappa_n) \\equiv \\theta_{mc}\\sim\\texttt{rnorm}(S, \\mu_n, \\sqrt{\\sigma^2_{mc}/\\kappa_n})\\) where \\(\\kappa_n = \\kappa_0 + n\\) \\[ \\begin{aligned} \\mu_{n} &amp;= \\frac{1/{\\tau}_{0}^{2}}{1/{\\tau}_{0}^{2}+n/{\\sigma}^{2}} \\mu_{0}+\\frac{n/{\\sigma}^{2}}{1/{\\tau}_{0}^{2}+n /{\\sigma}^{2}} \\bar{y}\\\\[0.3cm] \\text{ if }\\tau_0^2=\\sigma^2/\\kappa_0, ~~\\mu_n &amp;= \\frac{\\kappa_{0}}{\\kappa_{0}+n} \\mu_{0}+\\frac{n}{\\kappa_{0}+n} \\bar{y} = \\frac{\\kappa_0\\mu_0+n\\bar y}{\\kappa_n}\\\\ \\frac{1}{\\tau_n^2} &amp;= \\frac{1}{\\tau_0^2}+\\frac{n}{\\sigma^2}\\\\[0.3cm] \\end{aligned} \\] \\[\\sigma_n^2 = \\frac{1}{\\nu_n}[\\nu_0\\sigma_0^2 + (n-1)s^2+\\frac{\\kappa_0n} {\\kappa_n}(\\bar y - \\mu_0)^2]\\] \\(\\{\\tilde Y|\\sigma^2, y_1,...,y_n\\} \\sim \\text{normal}(\\mu_n,\\tau_n^2+\\sigma^2) \\equiv \\texttt{rnorm}(S, \\theta_{mc}, \\sqrt{\\sigma^2_{mc}})\\) \\(\\mu_0 \\text{ and }\\kappa_0\\) is the mean and sample size from a prior set of observations. \\(\\nu_0\\) prior sample size, from which a prior sample variance \\(\\sigma_0^2\\) has been obtained. \\(s^2 = \\sum_{i=1}^n (y_i - \\bar y)/(n-1) = \\texttt{var}(\\boldsymbol{y})\\) sample variance \\((n-1)s^2\\) is the sum of squared observations from the sample mean \\(\\nu_0 \\sigma_0^2 \\text{ and } \\nu_n\\sigma_n^2\\) as prior and posterior sum of squares, respectively. "],["gibbs-sampler.html", "Lecture 10 Gibbs sampler 10.1 Review of conjugate prior for normal model 10.2 A semiconjugate prior distribution 10.3 Gibbs sampling 10.4 Example: Midge wing length 10.5 Discrete approximation of posterior distribution 10.6 Example", " Lecture 10 Gibbs sampler The following notes, mostly transcribed from Neath(0517,2021) lecture, summarize sections(6.1-6.4) of Hoff(2009). 10.1 Review of conjugate prior for normal model We write our model; \\(n\\) observations from a normal population with mean \\(\\theta\\) and variance \\(\\sigma^2\\) and we want to do Bayesian inference about \\(\\theta\\) (and maybe \\(\\sigma^2\\)). The conjugate prior for this model is: \\(\\sigma^2 \\sim \\text{InverseGamma}(a = \\nu_0 / 2, b = \\nu_0\\sigma_0^2 / 2).\\) Where does this come from? (Review) \\(X \\sim\\) Inverse gamma means \\(X = 1 / Z\\) where \\(Z \\sim\\) gamma distribution. When we say the conjugate prior for a normal variance is the inverse gamma that’s equivalent to saying the conjugate prior for a normal precision(precision = 1 / variance) is the gamma distribution. The conjugate prior for \\((\\theta, \\sigma^2)\\) is completed by \\(\\theta | \\sigma^2 \\sim \\text{Normal}( \\mu_0, \\sigma^2 / \\kappa_0 )\\). The variance depending on \\(\\sigma^2\\) makes sense because the distribution is specified conditionally on \\(\\sigma^2.\\) Back to that gamma distribution. The reparameterization from the usual gamma(\\(a =\\) shape, \\(b =\\) rate) to \\((a = \\nu_0 /2, b = \\nu_0 \\sigma^2_0 / 2)\\) is strategic. \\(\\sigma^2_0\\) is the “prior best guess” and \\(\\nu_0\\) measures the strength of belief in that best guess. More precisely, this prior contributes exactly the same information to the posterior as would \\(\\nu_0\\) observations with a sample variance of \\(\\sigma_0^2.\\) This conjugate prior represents a “prior sample of size \\(\\nu_0\\)” “with a sample variance of \\(\\sigma_0^2\\).” Similarly (and more straightforwardly) the prior on \\(\\theta\\) contributes to the posterior exactly what would be contributed by \\(\\kappa_0\\) observations with a sample mean of \\(\\mu_0\\). The parameters in the normal conjugate prior are; \\(\\mu_0\\) (prior sample mean) \\(\\kappa_0\\) (prior sample size for the mean) \\(\\sigma_0^2\\) (prior sample variance) \\(\\nu_0\\) (prior sample size for variance). Then the updating is very intuitive; \\[\\theta | \\sigma^2, y_1, …, y_n \\sim \\text{Normal}( \\mu_n, \\sigma^2 / \\kappa_n), ~~\\kappa_n = \\kappa_0 + n,~ \\mu_n = (\\kappa_0 \\mu_0 + n \\bar y)/ (\\kappa_0 + n)\\] Question: How to decide a proper \\(\\kappa_0\\)? If you’re uncertain, take \\(\\kappa_0\\) to be small relative to \\(n\\) and it doesn’t really matter. One of the strengths of the Bayesian paradigm is that it allows the incorporation of prior information. But in practice, non informative priors are much more commonly used (\\(\\kappa_0\\) is small relative to \\(n\\)), thus the posterior is mostly determined by the results of the data, experiment, sample, etc. Though, this is not the only reason we use Bayesian Statistics. We also like the updates and the interpretations in terms of probability. Back in chapter 5 (our previous class in fact) we skipped some stuff about “improper priors.” Given this line of questioning maybe we should “un-skip” this section some time in the next few days The punchline: There is a way to do Bayesian inference and not incorporate ANY prior information (be “objective”). You still have some decisions to make regarding the prior distribution but they’re in terms of form not content. It will often be (usually be) very similar in terms of the substantive conclusions if not exactly the same to the frequentist. But there are some very complex models (maybe we get to this stuff toward the end of our course) where the Bayesian answer is actually a lot easier to get to than the frequentist. 10.2 A semiconjugate prior distribution For today’s class, we’re still doing normal model but suppose I don’t like the “conjugate prior” above. I don’t like that the conjugate prior forces me to describe my uncertatinty about \\(\\theta\\) conditionally on \\(\\sigma^2\\) what if my prior knowledge of \\(\\theta\\) and my prior knowledge of \\(\\sigma^2\\) have nothing to do with each other and I want my prior on \\(\\theta\\) to be independent of my prior on \\(\\sigma^2\\). This joint prior distribution is ‘semiconjugate’ for the normal model. \\[ \\theta \\sim \\text{Normal}(\\mu_0, \\tau_0^2); \\quad 1/\\sigma^2 \\sim \\text{gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2/2) \\] With \\(\\sigma^2\\) fixed this would be the conjugate prior for \\(\\theta\\) instead, and with \\(\\theta\\) fixed this would be the conjugate prior for \\(\\sigma^2\\). Taking the prior above, what posterior results from it? Conditionally \\(\\sigma^2,\\) \\(\\theta\\) has a normal posterior. i.e., { \\(\\theta | y_1, …, y_n , \\sigma^2 ~ \\} \\sim \\text{Normal}( \\mu_n, \\tau_n^2 )\\). Remember we write \\(1/\\tau_n^2 = 1 / \\tau_0^2 + n / \\sigma^2.\\) i.e., posterior precision = prior precision + data precision. So ‘this is a prior that is defined not dependent on \\(\\sigma^2\\) but the posterior is specified conditionally on \\(\\sigma^2\\).’ Note: there is no \\(\\kappa_0\\) in this prior because \\(\\kappa_0\\) is a parameter of the prior \\(p(\\theta | \\sigma^2)\\). In this prior \\(p(\\theta | \\sigma^2) = p(\\theta).\\) Student question: Could you explain more about what a semi conjugate prior is? Ans: The conjugate prior for the normal model satisfies this \\(p(\\theta, \\sigma) = p(\\sigma^2) p(\\theta | \\sigma^2)\\) and \\(p(\\theta, \\sigma^2 | y) = p(\\sigma^2 | y) p(\\theta | \\sigma^2 , y).\\) For the fully conjugate prior, the joint posterior \\(p(\\theta, \\sigma^2 | y)\\) has the same parametric form as the joint prior \\(p(\\theta, \\sigma).\\) In the semiconjugate prior the conditional prior \\(p(\\theta | \\sigma)\\) has the same parametric form as the conditional posterior \\(p(\\theta | \\sigma, y)\\) and same thing with conditional prior \\(p(\\sigma^2 | \\theta) = p(\\sigma^2)\\) has the same parametric form as the conditional posterior \\(p(\\sigma | \\theta , y)\\). The seminconjugate prior is not strictly conjugate because \\(\\theta\\) and \\(\\sigma^2\\) are independent in the prior but not in the posterior however each conditional prior is conjugate. Again, \\(\\theta \\sim\\) Normal and \\(\\sigma^2 \\sim\\) inverse-gamma with \\(\\theta\\) and \\(\\sigma^2\\) independent is not strictly conjugate because the joint posterior has a different form than the joint prior however both conditional posteriors have the same form as the corresponding priors. (#fig:contour plot for precision)Conjugate prior for mean and precision. This is a posterior distribution for the (the flies’ wings) but pretend it’s the conjugate prior for \\(\\theta\\) and \\(1/\\sigma^2\\). The conditional prior \\(p(\\theta | \\sigma^2 )\\) is very tight (has low variance) when precision is high and is highly diffuse (has a high variance) when precision is low. Remember in these pictures the conditional distribution \\(p(\\theta | \\sigma^2 )\\) is visualized in these pictures by taking ‘horizontal slices’ Last week (Ch 5 in Hoff) we wrote \\(p(\\theta, \\sigma^2) = p(\\sigma^2) p(\\theta | \\sigma^2)\\). For the semi conjugate prior we’re studying today, this picture would not be like this. Instead, \\(p(\\theta | \\sigma^2)\\) would be the same for all \\(\\sigma^2\\) because \\(\\theta \\perp \\sigma^2.\\). Let’s agree that this semiconjugate prior \\(\\theta \\sim \\text{Normal}( \\mu_0, \\tau_0^2 ), ~ \\sigma^2 \\sim \\text{InverseGamma}(\\nu_0 / 2, \\nu_0 \\sigma_0^2 / 2 )\\) where \\(\\sigma^2\\) independent i.e., \\(p(\\theta, \\sigma^2) = \\texttt{dnorm}(\\theta | …) \\times \\texttt{dinvgamma}(\\sigma^2 | … ),\\) is worth considering. So we ask: what posterior results? and we got half way to answering the question. We know that \\(p(\\theta | \\sigma^2, y_1, …, y_n)\\) is Normal\\((\\mu_n, \\tau_n^2 )\\) so if we can solve \\(p(\\sigma^2 | y_1, …, y_n)\\) we solve the posterior. However, this doesn’t have a nice solution as it turns out. What does have a nice solution is \\[p(\\sigma^2 | \\theta, y_1, …, y_n)\\sim\\text{InverseGamma}( \\nu_n/2 , \\nu_n \\sigma_n^2(\\theta) / 2 )\\] \\[ \\nu_{n}=\\nu_{0}+n \\quad \\text { and } \\quad \\sigma_{n}^{2}(\\theta)=\\frac{1}{\\nu_{n}}\\left[\\nu_{0} \\sigma_{0}^{2}+n s_{n}^{2}(\\theta)\\right] \\] and \\(s_{n}^{2}(\\theta)=\\sum\\left(y_{i}-\\theta\\right)^{2} / n\\), the unbiased estimator of \\(\\sigma^{2}\\) if \\(\\theta\\) were known. The \\(\\sigma_n^2(\\theta)\\) parameter is a weighted average of \\(\\sigma_0^2\\) and the sample variance around \\(\\theta\\). 10.3 Gibbs sampling Here’s our predicament; to sample from the posterior \\(p(\\theta, \\sigma^2 | y_1, …, y_n)\\) it would be sufficient to be able to sample from \\(p(\\theta | y_1, …, y_n)\\) and \\(p(\\sigma^2 | \\theta, y_1, …, y_n)\\). But the marginal of \\(\\theta\\) is not nice. Similarly, we could also sample from the posterior \\(p(\\theta , \\sigma^2 | y_1, …., y_n)\\) if we could sample from \\(p(\\theta | \\sigma^2, y_1, …, y_n)\\) and \\(p(\\sigma^2 | y_1, ….,y_n)\\), but the marginal of \\(\\sigma^2\\) is also not nice. So we have answers for both of the conditionals i.e., \\(p(\\sigma^2 | \\theta, y_1, …, y_n)\\) and \\(p(\\theta | \\sigma^2, y_1, …, y_n)\\), but not either of the marginals. Just thinking about a pair of random variables call them \\((X_1, X_2)\\). I can write their joint density as \\(f(x_1, x_2) = f(x_1)f(x_2 | x_1)\\) or \\(f(x_1, x_2) = f(x_2 )f(x_1 | x_2)\\). But if I don’t know \\(f(x_1)\\) or \\(f(x_2)\\) but I know both of \\(f(x_1|x_2)\\) and \\(f(x_2 | x_1)\\). what can I do with this? I can do a Gibbs sampler! Let’s pretend we had a draw from \\(\\sigma^{2(1)} \\sim p(\\sigma^2 | y_1,...,y_n ),\\) then we could sample \\[\\theta^{(1)} \\sim p(\\theta | \\sigma^{2(1)}, y_1,...,y_n)\\] and \\((\\theta, \\sigma^2)^{(1)}\\) would be a sample from the joint distribution \\(p(\\theta, \\sigma | y_1,...,y_n).\\) Additionally, \\(\\theta^{(1)}\\) can be considered a sample from the marginal distribution \\(p(\\theta | y_1,...,y_n).\\) From this \\(\\theta\\)-value, we can generate \\[\\sigma^{2(2)} \\sim p(\\sigma^2 | \\theta^{(1)}, y_1,...,y_n)\\] so now we got \\((\\theta^{(1)}, \\sigma^{2(2)}) \\sim p(\\theta , \\sigma^2 | y_1,...,y_n),\\) thus \\(\\sigma^{2(2)} \\sim p(\\sigma^2 | y_1,...,y_n).\\) Now sample \\(\\theta^{(2)} \\sim p(\\theta | \\sigma^{2(2)}, y_1,...,y_n)\\) etc. So the answer to the question : What can we do with both conditional distributions (but neither marginal distribution)? “with” here means “the means to simulate samples from.” Well if we could just get a starting point \\(\\sigma^{2(1)}\\) we could simulate a sequence such that each element in this sequence is marginally drawn from the joint posterior distribution \\(p(\\theta, \\sigma^2 | y).\\) One complication here is that the draws would not be independent. You see why? \\(\\theta^{(1)} \\sim p(\\theta | y), \\theta^{(2)} \\sim p(\\theta | y)\\) but they are not independent because \\(\\theta^{(2)}\\) is drawn conditionally on \\(\\sigma^{2(2)}\\) and \\(\\sigma^{2(2)}\\) is drawn conditionally on \\(\\theta^{(1)}\\) and as a result there is dependence between \\(\\theta^{(2)}\\) and \\(\\theta^{(1)}\\). We’ll worry about that tomorrow. This iterative sampling for the iteratively updated conditional distributions is called the Gibbs sampler. We’ll define it here in our 2-parameter model. The algorithm goes: sample \\(\\theta^{(s+1)} \\sim p\\left(\\theta \\mid \\sigma^{2(s)}, y_{1}, \\ldots, y_{n}\\right)\\); sample \\(\\sigma^{2(s+1)} \\sim p\\left(\\sigma^{2} \\mid \\theta^{(s+1)}, y_{1}, \\ldots, y_{n}\\right)\\); let \\(\\phi^{(s+1)}=\\left(\\theta^{(s+1)}, \\sigma^{2(s+1)}\\right)\\). The code: n &lt;- length(y) ybar &lt;- mean(y) s2 &lt;- var(y) S &lt;- 1000 phi &lt;- matrix(NA, S, 2) # starting values theta &lt;- ybar # sample mean sigma2 &lt;- (nu.0*sigma2.0 + (n-1)*s2) / (nu.0 + n) for(s in 1:S){ tau2.n &lt;- 1 / (1/tau2.0 + n/sigma2) mu.n &lt;- tau2.n * (mu.0/tau2.0 + n*ybar/sigma2) theta &lt;- rnorm(1, mean=mu.n, sd=sqrt(tau2.n)) sigma2 &lt;- 1/rgamma(1, (nu.0 + n)/2, (nu.0*sigma2.0 + (n-1)*s2 + n*(ybar-theta)^2)/2) phi[s,] &lt;- c(theta, sigma2) } This R code assumes we have a data vector \\(\\boldsymbol{y}\\) and parameter variables \\(\\mu_0=\\texttt{mu.0}\\), \\(\\tau_0^2=\\texttt{tau2.0}\\) (the parameters of the normal prior on \\(\\theta\\)) \\(\\nu_0=\\texttt{nu.0},\\) \\(\\sigma_0^2=\\texttt{sigma2.0}\\)(parameters of the inverse gamma prior on \\(\\sigma^2\\)). As we learn more of these Markov chain Monte Carlo methods we’ll see this structure more and more. Where the simulation step is not done by theta.sim &lt;- r&quot;dist&quot; ( S, … ) but rather it is done by for-loops because each draw depends on the previous draw. The object \\(\\phi=\\texttt{phi}\\) in this code is the matrix of simulations. The \\(s\\)th row of \\(\\texttt{phi}\\) is the \\(s\\)th iteration of the Gibbs sampler. The first column of \\(\\texttt{phi}\\) is the \\(\\theta\\)-components the second column is the \\(\\sigma^2\\) components. See Hoff’s book. He does mean and precision. Note: \\(s_{n}^{2}(\\theta)=\\sum\\left(y_{i}-\\theta\\right)^{2} / n\\),so {\\(\\sigma^2 | \\theta, y\\)} depends on \\(\\sum (y_i - \\theta)^2.\\) Recalculating this for every updated \\(\\theta\\) is inefficient. Instead of calculating \\(\\sum (y_i - \\theta)^2\\) for each updated \\(\\theta\\) value we use \\(s_{n}^{2}(\\theta)=(n-1)s^2+n(\\bar y - \\theta)^2\\) and only recalculate \\((\\bar y - \\theta)^2\\). 10.4 Example: Midge wing length Our prior information about these insects leads us to expect mean \\(\\texttt{mu.0 = 1.9}\\) What variance to put on that? the logic that went into \\(\\texttt{tau2.0 = .95^2?}\\) was to give low prior probability to \\(\\theta &lt; 0.\\) Expected sd around 0.10 or so, so set \\(\\sigma_0^2= (.10)^2 = .01\\) and then set \\(\\nu_0 = 1.\\) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,3)) # First the joint dist of (theta, sigma2) plot(phi, type=&quot;l&quot;, col=&quot;gray&quot;, xlab=&quot;theta&quot;, ylab=&quot;sigma2&quot;) points(phi, pch=19, cex=.75) # Now the marginal dist of theta plot(density(phi[,1]), lwd=2, xlab=&quot;theta&quot;, ylab=&quot;p(theta|y1,...,yn)&quot;, main=&quot;&quot;) abline(v= quantile(phi[,1], c(.025, .975)), lty=2) # Now the marginal of sigma2 plot(density(phi[,2]), lwd=2, xlab=&quot;sigma2&quot;, ylab=&quot;p(sigma2|y1,...,yn)&quot;, main=&quot;&quot;) Figure 10.1: The ﬁrst panel shows 1,000 iterations of the Gibbs sampler. The second and third panels give kernel density estimates to the distributions of Gibbs samples of theta and sigma2. Vertical dashed bars on the second plot indicate 2.5% and 97.5% quantiles of the Gibbs samples of theta. The left-most is a scatterplot. The “point cloud” represents the empirical joint posterior! The marginal of \\(\\theta\\) is not Normal but it appears symmetric and bell shaped so that’s nice. The marginal \\(p(\\sigma^2 | y_1, …, y_n)\\) is right-skewed the peak is around .02 or so. The sample variance was .017 so I guess this makes sense. There’s another thing in the scatterplot that we wouldn’t normally draw (and we wouldn’t draw these lines in our data analysis reports either) they’re just to illustrate the path that the draws have taken. What would it look like if we did a plot like this but with independent draws? I believe it would be uglier than this. Let’s see.. Let’s go back one class for an example where we could do independent simulations So the difference between these two picture. In the left hand side the draws are independent, i.e., \\(\\theta^{(s)}\\) is independent of \\(\\theta^{(s-1)}.\\) In the right hand plot there is serial dependence. Is that apparent? I think not really. In this case (the normal model with Gibbs sampling) the dependence between \\(\\theta^{(s)}\\) and \\(\\theta^{(s-1)}\\) is VERY weak. # Confidence interval for population mean quantile(phi[,1], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 1.716 1.806 1.903 # Confidence interval for population variance quantile(phi[,2], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 0.007533 0.017341 0.053634 # Confidence interval for population standard deviation quantile(sqrt(phi[,2]), c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 0.08679 0.13169 0.23159 10.5 Discrete approximation of posterior distribution Let’s be real general here. Suppose you have a single-parameter \\(\\theta\\). You can write the posterior \\(p(\\theta|y) = c\\times p(\\theta)p(y | \\theta) = p(\\theta) p(y | \\theta) / p(y). ~~ p(y)\\) depends on integrating \\(\\theta\\) out of the numerator of this thing. That may be hard. So here’s a thing you can do. Pick a bunch of \\(\\theta\\) values, \\(\\theta^{(1)} &lt; \\theta^{(2)} &lt; … &lt; \\theta^{(S)}\\). I’m using the notation of MC simulation but it’s not that these are fixed points. \\(Pr(\\theta &lt; \\theta^{(1)} |y) = 0\\) \\(Pr(\\theta &gt; \\theta^{(S)} | y) = 0\\) If those two conditions are met and \\(|\\theta^{(s)} - \\theta^{(s-1)}|\\) is small then the continuous distribution \\(p(\\theta | y)\\) can be well approximated by the discrete distribution \\(p(\\theta^{(s)} | y)\\) for \\(s = 1, …, S\\) and the discrete distribution can be computed exactly because I can compute \\(p(\\theta^{(s)}) p(y | \\theta^{(s)})\\) for each value of \\(\\theta^{(s)}.\\) Divide each entry by the sum of all entries and the sum of the entries becomes 1 so it’s a probability distribution! You did something like this on your first HW assignment. For the mixture distribution posterior you calculated it at a bunch of points between 0 and 1. Now suppose you had two parameters \\(p(\\theta_1, \\theta_2 | y)\\). You can do the exact same thing except it doesn’t require double the computation. What does it require? If I split the range of \\(\\theta\\) into 100 points. I had to compute the posterior 100 times. If I split the range of \\(\\theta^{(1)}\\) and the range of \\(\\theta^{(2)}\\) into 100 points each, I have to calculate the posterior \\(100^2\\) times. Still fine. What if I had 16 parameters \\((\\theta_1, \\theta_2, …., \\theta_{16} ) = \\boldsymbol\\theta\\). Then I couldn’t do discrete approximation any more but I could still do a Gibbs sampler (or some other MCMC approach). So that’s going to become our go-to method. See Hoff chapter 6.2 for more information on discrete approximation. 10.6 Example The R-code below evaluates \\(p(\\theta, 1/\\sigma^2| y_1 ,..., y_n )\\) on a \\(121\\times250\\) grid of evenly \\(1/\\sigma^2\\) spaced parameter values, with \\(\\theta \\in \\{{ 1.500, 1.505, . . . , 2.095, 2.100 \\}}\\) and \\(1/\\sigma^2 \\in \\{ 1, 2, . . . , 249, 250 \\}\\). theta &lt;- seq(1.5, 2.1, .005) I.sig2 &lt;- seq(1, 250, 1) G &lt;- length(theta); H &lt;- length(I.sig2); #121 #250 log.post &lt;- matrix(NA, G, H); for(g in 1:G) { for(h in 1:H) { log.post[g,h] &lt;- dnorm(theta[g], mu.0, 1/sqrt(tau2.0), log=T) + dgamma(I.sig2[h], nu.0/2, nu.0*sigma2.0/2, log=T) + sum(dnorm(y, theta[g], 1/sqrt(I.sig2[h]), log=T)) }} post.grid &lt;- exp(log.post); rm(log.post); post.grid &lt;- post.grid / sum(post.grid) par(mar=c(3,3,3,1),mgp=c(1.75,.75,0),oma=c(0,0,.5,0)) par(mfrow=c(1,3)) contours &lt;- c(.001, .01, seq(.05, .95, .10)) * max(post.grid) contour(theta, I.sig2, post.grid, levels=contours, drawlabels=F, xlab=&quot;theta&quot;, ylab=&quot;1/sigma2&quot;, main=&quot;Joint distribution&quot;) plot(theta, apply(post.grid, 1, sum), type=&quot;l&quot;, lwd=2, xlab=&quot;theta&quot;, ylab=&quot;Probability&quot;, main=&quot;Marginal of mean&quot;) plot(I.sig2, apply(post.grid, 2, sum), type=&quot;l&quot;, lwd=2, xlab=&quot;1/sigma2&quot;, ylab=&quot;Probability&quot;, main=&quot;Marginal of precision&quot;) The first panel gives the discrete approximation to the joint distribution of \\((\\theta, 1/\\sigma^2).\\) "],["mcmc-diagnostics.html", "Lecture 11 MCMC diagnostics 11.1 The Gibbs sampler 11.2 Distinguishing estimation from approximation 11.3 Introduction to MCMC diagnostics 11.4 Discussion", " Lecture 11 MCMC diagnostics The following notes, mostly transcribed from Neath(0518,2021) lecture, summarize sections(6.5 and 6.6) of Hoff(2009). 11.1 The Gibbs sampler We had a posterior distribution \\(p(\\theta, \\sigma^2 | y)\\) where the full conditional distributions \\(p(\\theta | \\sigma^2, y)\\) and \\(p(\\sigma^2 | \\theta, y)\\) took a convenient form, but neither marginal distribution did. In a Gibbs sampler you alternately draw from both “full conditional” distributions and the result is; as \\(s\\) increases the sampling distribution of \\((\\theta, \\sigma^2)^{(s)}\\) approaches the target distribution (which is the posterior). The Gibbs sampler we saw last time was very simple, just two parameters, but the idea extends to \\(p\\) parameters. Let \\(\\boldsymbol{\\phi} = (\\phi_1,\\phi_2,...,\\phi_p)\\) be a vector of parameters. Let \\(p(\\boldsymbol{\\phi}) = p(\\phi_1,\\phi_2,...,\\phi_p)\\) be the target distribution and the goal is to approximate probabilities and moments and quantiles etc with respect to this target distribution. In Bayesian statistics the target distribution is generally the posterior distribution \\(p(\\boldsymbol{\\phi} | y)\\). Often in today’s class you’ll notice we’re just writing \\(p(\\boldsymbol{\\phi})\\) to denote a generic target distribution. But it’s probably (in practical application) the posterior not the prior. We are using subscripts to denote the position in a vector \\(\\boldsymbol{\\phi} = (\\phi_1, …, \\phi_p)\\) and superscripts to denote the iteration number of the simulation. Superscripts are in parentheses so we don’t think it means “raised to that power.” The way the Gibbs sampler works is; each iteration of the Gibbs sampler itself requires \\(p\\) steps (\\(p=2\\) in yesterday’s example). Exercise: Write down the form of the \\(j\\)th update of the gibbs sampler \\(\\phi_j^{(s)} \\sim p(\\phi_j | \\phi_1^{(s)}, …, \\phi_{j-1}^{(s)}, \\phi_{j+1}^{(s-1)}, …., \\phi_p^{(s-1)} )\\) We are conditioning on parameters that have been updated as well as those that are yet to be updated. The set of conditional distributions \\(p(\\phi_j | \\phi_1, …, \\phi_{j-1}, \\phi_{j+1}, …., \\phi_p )\\) for all of \\(j = 1, 2, … p,\\) are collectively called the full conditional distributions for the target distribution \\(p(\\phi_1, …., \\phi_p)\\). You see why it’s a dependent sample? Take the semiconjugate normal model to illustrate. \\(\\theta^{(s)}\\) depends on \\(\\sigma^{2(s-1)}\\) but \\(\\sigma^{2(s-1)}\\) depends on \\(\\theta^{(s-1)}\\) therefore \\(\\theta^{(s)}\\) depends on \\(\\theta^{(s-1)}\\). The Gibbs sampler output is generally less good than would be ordinary Monte Carlo simulations (that is independent draws). However, the Gibbs sampler method works in more complicated models where direct simulation may not be feasible. How does (in very broad terms) Markov chain Monte Carlo relate to ordinary Monte Carlo? It’s not as good because (1) the draws are dependent and (2) they don’t have the right sampling distribution exactly. Note this property; \\(\\boldsymbol \\phi^{(s)}\\) depends on \\(\\boldsymbol \\phi^{(s-1)}\\) and \\(\\boldsymbol \\phi^{(s-1)}\\) is dependent on \\(\\boldsymbol \\phi^{(s-2)}\\) therefore \\(\\boldsymbol \\phi^{(s)}\\) is dependent on \\(\\boldsymbol \\phi^{(s-2)}\\). By the principle of induction \\(\\boldsymbol \\phi^{(s)}\\) is dependent on \\(\\boldsymbol \\phi^{(0)}\\). However, \\(\\boldsymbol \\phi^{(s)}\\) is conditionally independent of \\(\\boldsymbol \\phi^{(s-2)}\\) and \\(\\boldsymbol \\phi^{(s-3)}\\) etc given \\(\\boldsymbol \\phi^{(s-1)}\\). If you’ve taken stochastic processes (stat 4207 / 5207) you’ve seen this idea before you know this is called the Markov property(evolution of the Markov process in the future depends only on the present state and does not depend on past history). So the Gibbs sampler produces a realization of a Markov chain and such is an example of a more general method called Markov chain Monte Carlo. If \\(\\boldsymbol{\\phi}^{(0)} \\sim p(\\boldsymbol{\\phi})\\)(the right target distribution) then \\(\\boldsymbol{\\phi}^{(1)} \\sim p(\\boldsymbol{\\phi})\\) and \\(\\boldsymbol{\\phi}^{(2)} \\sim p(\\boldsymbol{\\phi})\\) etc. In general this will not be the case. The starting point \\(\\boldsymbol{\\phi}^{(0)}\\) is determined somehow, but it’s not a draw from the target distribution. Therefore, the marginal distribution of \\(\\boldsymbol{\\phi}^{(s)}\\) is NOT \\(p(\\boldsymbol{\\phi})\\), i.e., is not the target distribution. However, as \\(s\\) increases the marginal distribution of \\(\\boldsymbol{\\phi}^{(s)}\\) approaches the target distribution. Below is a technical statement of this property; \\[ Pr(\\boldsymbol\\phi^{(s)} \\in A) \\rightarrow \\int_A p(\\boldsymbol\\phi)d \\boldsymbol\\phi \\quad \\text{ as } s \\rightarrow \\infty \\] You know about the Law of Large Numbers (LLN) for independent draws. This is the LLN for Markov chains; \\[\\frac{1}{S} \\sum_{s=1}^{S} g\\left(\\boldsymbol\\phi^{(s)}\\right) \\rightarrow \\mathrm{E}[g(\\boldsymbol\\phi)]=\\int g(\\boldsymbol{\\phi}) p(\\boldsymbol{\\phi}) d \\boldsymbol{\\phi} \\quad \\text { as } S \\rightarrow \\infty\\] In the midge data example we did 1000 iterations of the Gibbs sampler for the semiconjugate normal model and from that we approximate posterior mean by \\(1.808\\) and posterior 95% interval by \\([1.72, 1.90]\\). 11.2 Distinguishing estimation from approximation What role does MCMC (or Monte Carlo in general) play in a Bayesian analysis? It is not an inferential method. The inferential method is Bayesian inference. We have a sampling model for our observable data \\(p(\\boldsymbol{y}|\\phi)\\), we have a probability distribution that describes our prior belief about \\(\\phi, ~p(\\phi)\\). Once these items are specified according to Bayes rule our belief about \\(\\phi\\) is “updated” to reflect the observed data; \\(p(\\phi | \\boldsymbol{y}) = p(\\phi) p(\\boldsymbol{y} | \\phi) / p(\\boldsymbol{y})\\). So what is the Gibbs sampler used for? The problem is \\(p(\\phi | \\boldsymbol{y})\\) may be a very complicated object particularly if \\(\\boldsymbol{\\phi} = (\\phi_1, \\phi_2, …., \\phi_p)\\) and \\(p\\) is a big number. That’s where Monte Carlo comes in. Monte Carlo is a computational tool for describing features of the posterior distribution. Confusion comes about because Monte Carlo is an approximation method and is based on the principles of statistical inference. If \\(\\phi\\) is a scalar the average value of the simulated \\(\\phi^{(s)}\\) is an approximation to the posterior mean \\(E(\\phi|\\boldsymbol{y}).\\) Sample quantiles of \\(\\phi^{(1)} ,..., \\phi^{(S)}\\) approximate \\(L\\) and \\(U\\) such that \\(Pr(L &lt; \\phi &lt; U | y) = 1-\\alpha\\). It is better we not use the term estimation for this purpose and refer to such approximations as Monte Carlo approximations. The inferential problem is; what do we know about \\(\\phi\\) after observing the data \\(\\boldsymbol{y}?\\) and that problem is completely answered by the posterior probability distribution \\(p(\\phi | \\boldsymbol{y}).\\) Where Monte Carlo methods come in is as a tool for helping us understand this complex object that is \\(p(\\phi | \\boldsymbol{y}).\\) Again, distinction between estimation and approximation; The estimation problem(the inference problem) is in principle solved the minute we write down \\(p(\\phi|\\boldsymbol{y}) = c \\times p(\\phi) p(\\boldsymbol{y}|\\phi).\\) But in practice in order to make useful statements about this posterior distribution we use various computational tools including Monte Carlo simulation and Markov chain Monte Carlo (like the Gibbs sampler). So the distinction is estimation (inference) is solved by Bayes rule and approximation is where Monte Carlo comes in. Student question - Suppose we take the .025 and .975 quantiles of a 1000 iterations of a Gibbs sampler and call that our 95% confidence interval for \\(\\theta\\). Is that estimation or approximation? The answer is: Both are going on here. The true value of \\(\\theta\\) is unknown. The true quantiles \\([\\theta_{.025}, \\theta_{.975}]\\) that satisfy \\(Pr( L &lt; \\theta &lt; U | y ) = 0.95\\) is a solution to the estimation problem. When \\(\\theta_{.025}\\) and \\(\\theta_{.975}\\) are not solvable exactly because the posterior distribution is too complicated and we use the sample quantile from a simulation based on Gibbs sampler that’s an approximation. 11.3 Introduction to MCMC diagnostics We never talked about ordinary Monte Carlo diagnostics because there’s no such thing. What MCMC diagnostics is concerned with is those two features of MCMC that make it less good than ordinary Monte Carlo: \\(\\theta^{(s)}\\) is not exactly marginally \\(\\sim p(\\theta | y)\\) only approximately (with this approximation improving as \\(s\\) increases). The draws are not independent, they are positively correlated. Recall that if \\(X_1\\) and \\(X_2\\) have the same mean and variance then \\(\\text{Var}( [ X_1+X_2]/2 ) = \\text{Var}(X) / 4\\) if they are independent. But is greater than that if they are positively correlated \\(\\text{Var}( [X_1+X_2]/2 ) = \\text{Var}(X_1) /4 + \\text{Var}(X_2) /4 + 2\\text{Cov}(X_1, X_2) / 4.\\) So if that covariance is positive, the average of two draws is still probably better(lower variance) than a single draw but not by as much as if \\(X_1\\) and \\(X_2\\) were independent. Let \\(\\phi\\) be a “parameter,” \\(p(\\phi)\\) is the target distribution which is probably a posterior. The gold standard is ordinary Monte Carlo in which \\(\\phi^{(s)} \\stackrel{\\text{ iid }} \\sim ~ p(\\phi)\\). That means each has sampling distribution that is exactly \\(p(\\phi)\\) and the draws are independent. The two ways Gibbs sampling (and MCMC more generally) is less good than ordinary (iid) Monte Carlo are; The sampling distribution of \\(\\phi^{(s)}\\) is exactly \\(p(\\phi)\\) in ordinary Monte Carlo but only approximately so under MCMC. Draws are independent under ordinary Monte Carlo but positively correlated in a Gibbs sampler. This will be illustrated by the example below. 11.3.1 Example: mixture of normal densities Consider the following target distribution \\(\\boldsymbol{\\phi} = (\\delta, \\theta), ~ \\delta \\in \\{1,2,3\\}\\) with probability { 0.45, 0.10, 0.45 } \\(\\{\\theta | \\delta = 1 \\}\\sim \\text{Normal}(-3 , 1/3 )\\) \\(\\{\\theta | \\delta=2\\} \\sim \\text{Normal}( 0, 1/3 )\\) \\(\\{\\theta | \\delta=3\\} \\sim \\text{Normal}( 3 , 1/3 )\\). This is a mixture distribution (mixture of three normals) and simulating draws from a mixture distribution is straightforward: If \\(\\delta \\sim\\) 1 or 2 or 3 with probability .45 or .10 or .45, then \\(\\theta | \\delta \\sim \\text{Normal}( \\mu_\\delta, \\sigma^2)\\). In this case the “mixture of 3 normals” distribution has three modes. Though this is not always the case. In your homework problem the prior mixture of two betas had two modes but the posterior mixture of two betas does not. Student question: Is there a way to know in general whether a mixture distribution will have multiple modes or not? In the case of a mixture of normals it depends on how far apart the means are relative to the variances. Here’s a rule for sampling from a mixture distribution: Simulate \\(\\delta^{(s)} \\sim p(\\delta)\\) and \\(\\theta^{(s)} \\sim p(\\theta | \\delta^{(s)} )\\) result; \\((\\delta^{(s)}, \\theta^{(s)}) \\sim p(\\delta,\\theta)\\). Also marginally \\(\\theta^{(s)} \\sim p(\\theta)\\). Below is what a mixture distribution looks like. Its density is \\(\\{ [p_1\\times \\texttt{dnorm}(\\theta, \\mu_1, \\sigma)] + [p_2 \\times\\texttt{dnorm}(\\theta | \\mu_2, \\sigma)] + [p_3 \\times \\texttt{dnorm} (\\theta | \\mu_3 , \\sigma)] \\}\\) S &lt;- 1000 mu &lt;- c(-3, 0, 3); sigma &lt;- rep(1/sqrt(3),3); p &lt;- c(.45, .10, .45) delta.MC &lt;- sample(3, S, replace=T, prob=p) theta.MC &lt;- rnorm(S, mu[delta.MC], sigma[delta.MC]) par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) hist(theta.MC, freq=F, right=F, col=&quot;pink&quot;, xlim=c(-6,6), ylim=c(0, .32), breaks=30, xlab=expression(theta), ylab=expression(p(theta)), main=&quot;&quot;) theta.vals &lt;- seq(-6, 6, .01) p.theta &lt;- p[1] * dnorm(theta.vals, mu[1], sigma[1]) + p[2] * dnorm(theta.vals, mu[2], sigma[2]) + p[3] * dnorm(theta.vals, mu[3], sigma[3]) lines(theta.vals, p.theta, lwd=2) Figure 11.1: A mixture of normal densities and a Monte Carlo approximation In this picture, the curve represents the target distribution i.e., the exact marginal density of \\(p(\\theta) = \\sum_\\delta p(\\theta|\\delta)p(\\delta)\\). The histogram (empirical distribution) is 1000 independent samples from the target distribution. Agreement is pretty good. Let’s do a Gibbs sampler for this problem. In ordinary Monte Carlo we can get independent samples by going; \\(\\delta^{(s)} \\sim p(\\delta), ~ \\theta^{(s)} \\sim p(\\theta | \\delta^{(s)}).\\) A Gibbs sampler would go; (Though this is a practically silly thing to do in this problem because in fact ordinary Monte Carlo is more straightforward than the Gibbs sampler.) \\(\\delta^{(s)} \\sim p(\\delta | \\theta^{(s-1)}),\\) then \\(\\theta^{(s)} \\sim p(\\theta | \\delta^{(s)}).\\) However, \\(\\delta^{(s)}\\) depends on \\(\\theta^{(s-1)}\\) and therefore \\(\\theta^{(s)}\\) is dependent on \\(\\theta^{(s-1)}\\). Full conditionals The full conditional distribution of \\(\\theta\\) is \\(p(\\theta \\mid \\delta)=\\texttt{dnorm}\\left(\\theta \\mid \\mu_{\\delta}, \\sigma\\right)\\) where \\(\\left(\\mu_{1}, \\mu_{2}, \\mu_{3}\\right)=(-3,0,+3)\\) with probabilities \\((.45, .10, .45)\\), and \\(\\sigma^{2}=1 / 3\\) Using Bayes’ rule we can show that the full conditional distribution of \\(\\delta\\) is given by \\[ \\begin{aligned} p(\\delta=k | \\theta) &amp;= p(\\delta)p(\\theta|\\delta) / p(\\theta)= \\frac{p(\\delta)p(\\theta|\\delta)}{\\sum_\\delta p(\\theta|\\delta)p(\\delta)}\\\\ &amp;= \\frac{Pr(\\delta=k)\\texttt{dnorm}(\\theta|\\mu_k,\\sigma)}{\\sum_{d=1}^3 Pr(\\delta=d)\\texttt{dnorm}(\\theta|\\mu_d,\\sigma)}, \\quad k=1,2,3 \\end{aligned} \\] Since \\(\\delta \\in \\{1,2,3\\},\\) let’s start it at \\(3.\\) # Gibbs sampler delta.Gibbs &lt;- rep(NA, S) theta.Gibbs &lt;- rep(NA, S) delta &lt;- 3 # starting value for(s in 1:S) { theta &lt;- rnorm(1, mu[delta], sigma[delta]) pdgt &lt;- p * dnorm(theta, mu, sigma) delta &lt;- sample(3, 1, prob=pdgt) delta.Gibbs[s] &lt;- delta theta.Gibbs[s] &lt;- theta } hist(theta.Gibbs, freq=F, right=F, col=&quot;pink&quot;, xlim=c(-6,6), breaks=30, # ylim=c(0, .32), breaks=30, xlab=&quot;theta&quot;, ylab=&quot;p(theta)&quot;, main=&quot;&quot;) theta.vals &lt;- seq(-6, 6, .01) p.theta &lt;- p[1] * dnorm(theta.vals, mu[1], sigma) + p[2] * dnorm(theta.vals, mu[2], sigma) + p[3] * dnorm(theta.vals, mu[3], sigma) lines(theta.vals, p.theta, lwd=2) Figure 11.2: A mixture of normal densities and 1000 Gibbs samples Here, 1000 draws from the Gibbs sampler do not agree very closely with the target distribution. Namely, values close to -3 are way overrepresented, values close to zero are overrepresented, values close to +3 are underrepresented. It’s a mixture of the right 3 things but it’s not the right mixture. What went wrong? par(mfrow=c(1,2)) plot(1:S, theta.MC, type=&quot;l&quot;, xlab=&quot;Iteration s&quot;, ylab=&quot;theta[s]&quot; ,main = &quot;Monte Carlo trace plot&quot;) plot(1:S, theta.Gibbs,type=&quot;l&quot;,xlab=&quot;Iteration s&quot;,ylab=&quot;theta[s]&quot; ,main = &quot;Gibbs sampler trace plot&quot;) Figure 11.3: Trace plots of theta-sequence What went wrong is that Gibbs sampler draws are highly correlated on RHS so there is a very strong tendency for \\(\\theta^{(s)}\\) to be close to \\(\\theta^{(s-1)}\\). So when I get \\(\\theta^{(s)}\\) close to 3 there is very high probability that \\(\\theta^{(s+1)}\\) is also going to be close to 3. Compare this to the independent Markov chain Monte Carlo draws on LHS which are jumping all over the place. Result is even a very small value of \\(S\\) is likely to be representative of the target distribution under ordinary MC. With the Gibbs sampler it takes a LOT more simulation to get a representative sample. The target distribution; 45% of it is concentrated around 3, 10% concentrated around 0, 45% concentrated around -3. With independent samples that just happens. With the Gibbs sampler that happens in the long run but not so well in the short run. BIG IDEA: The information about \\(p(\\theta)\\) contained in \\(S\\) draws of a Gibbs sampler is less than the information about \\(p(\\theta)\\) contained in \\(S\\) independent draws. Recall that we have: \\(\\{\\theta | \\delta = 1 \\}\\sim \\text{Normal}(-3 , 1/3 )\\) \\(\\{\\theta | \\delta=2\\} \\sim \\text{Normal}( 0, 1/3 )\\) \\(\\{\\theta | \\delta=3\\} \\sim \\text{Normal}( 3 , 1/3 )\\). If \\(\\theta^{(s)}\\) is close to zero it is highly probable that we will get \\(\\delta^{(s)} = 2,\\) if we get \\(\\delta^{(s)} = 2\\) we expect \\(\\theta^{(s+1)}\\) should be close to zero and so on. We can basically tell from the trace plot for \\(\\theta\\) what the trace plot for \\(\\delta\\) would look like. Isn’t the Gibbs sampler guaranteed to eventually provide a good approximation? Yes it is. If we ran a lot more than 1000 draws, “eventually” we’d get about 45% of the time hovering around +3 about 10% hovering around 0 and about 45% hovering around -3. In the long run yes we would see exactly this, but \\(S = 1000\\) is apparently “short” for our example. 11.4 Discussion Concrete example: A particle moving around the parameter space. In this example the parameter is \\(\\phi\\) the parameter space is the real line but generally the probability in the target distribution is contained between -5 and 5. Think of \\(\\phi\\) as a particle moving around the line and where it is at iteration \\(s\\) is recorded in this plot (like the trace plot above). In the long run let \\(A_1, A_2\\) and \\(A_3\\) be three subsets of the parameter space so that \\(\\int_{A2}p(\\phi) d\\phi\\) is pretty small and \\(A_1\\) and \\(A_3\\) are separated by \\(A_2\\) (just like the example above!). In the long run our particle should spend little time in \\(A_2\\) but a lot more time in \\(A_1\\) and \\(A_3\\). Suppose we start in \\(A_2\\), two things we would want to see happen (1) the chain should move out of \\(A_2\\) pretty quickly (stationarity) (2) the chain to move between \\(A_1\\) and \\(A_3\\) fairly readily, that is, not get stuck for long stretches in either one (low autocorrelation). Definition: If \\(\\phi^{(0)} \\sim p(\\phi)\\) then the chain is stationary. And if the chain is stationary then the sampling distribution of \\(\\phi^{(s)}\\) is \\(p(\\phi)\\) (the target distribution) for every \\(s\\). And in that case: Issue number 1 (the “non-stationarity” issue) would be a non-issue. Regarding the stationarity issue, the practical issue presented is where to start? In the problems we’ve done so far this really hasn’t been an issue. A mode of the target distribution is a good starting point. The second issue, the autocorrelation issue, is also called mixing. Low autocorrelation and fast mixing go together, high autocorrelation and slow mixing go together. 11.4.1 How does autocorrelation(slow mixing) affect our MCMC approximation? For this discussion assume stationarity. With good starting values this is a reasonable assmption. We have \\(\\phi^{(0)} \\sim p(\\phi)\\) or close enough therefore \\(\\phi^{(s)} \\sim p(\\phi)\\) for each \\(s,\\) the only issue is the draws are not independent. How does that muck up our estimation? Suppose autocorrelation was zero i.e., independent samples. \\(\\bar \\phi = (1/S)[ \\phi^{(1)} + … + \\phi^{(S)} ]\\) is our Monte Carlo approximation to \\(E(\\phi)\\). In this notation \\(\\phi_0 = E(\\phi)\\). \\(\\phi\\) is the parameter (an uncertain quantity), \\(p(\\phi)\\) is the probability distribution, \\(\\phi_0\\) is the mean of that distribution. \\(\\phi^{(s)}\\) for \\(s = 1, …, S\\) are a set of \\(S\\) generated values being used to approximate \\(\\phi_0\\). If the \\(\\phi^{(s)}\\) are uncorrelated, then we can define Monte Carlo standard error (mcse) as \\(\\sqrt{\\text{Var}_{MC}}\\), where \\(\\text{Var}_{MC} = E[(\\bar \\phi - \\phi_0)^2] = \\text{Var}(\\phi)/S=\\) Expected squared approximation error associated with using \\(\\bar \\phi\\) to estimate \\(\\phi_0\\). \\(\\text{Var()}\\) with no subscript means the variance of the target distribution. Want to have high confidence (say 95%) that your approximation error will be less than \\(\\epsilon?\\) solve for \\(S\\) so that \\(2\\text{mcse} = 2\\sqrt{\\text{Var}_{MC}} &lt; \\epsilon\\). If we are using a Gibbs sampler so the \\(\\phi^{(s)}\\) are not independent (they’re correlated) the expected squared approximation error \\(\\text{Var}_{MCMC}\\) is not Var\\((\\phi) / S\\). It’s bigger. It’s bigger by a term that depends on how strongly correlated are \\(\\phi^{(s)}\\) and \\(\\phi^{(t)}\\) for \\(t \\neq s\\) “MCMC standard error” equals square root of ( expected squared approximation error under MCMC sampling ) i.e., \\(\\text{mcmcse} = \\sqrt{\\text{Var}_{MCMC}}\\), where \\(\\text{Var}_{MCMC} &gt; \\text{Var}_{MC}\\) by an amount that depends on how highly correlated are successive draws in the Gibbs sampler. The bigger is our expected squared approximation error \\((\\text{Var}_{MCMC})\\) the more we expect our approximation to not be good. 11.4.2 Autocorrelation par(mfrow=c(1,2)) acf(theta.MC) acf(theta.Gibbs) Figure 11.4: acf for independent MC draws and Gibbs samples Corr\\((\\phi^{(s)} , \\phi^{(s+1)})\\) is called the lag-1 autocorrelation Corr\\(( \\phi^{(s)} , \\phi^{(s+2)})\\) is called the lag 2 autocorrelation In general the lag-\\(t\\) autocorrelation is defined as Corr\\((\\phi^{(s)}, \\phi^{(s+t)})\\) and if we consider this as a function of \\(t,~ t = 1, 2, 3, …,\\) for a typical Gibbs sampler it is always positive and (generally) decreasing toward 0 as \\(t\\) increases. The faster it zeros out the better. The “art” of MCMC is finding samplers for which the autocorrelation zeros out quickly. Student Question: If positive autocorrelation is bad in the sense that it makes the Monte Carlo less efficient might there be a way to simulate negatively correlated samples to get more efficient Monte Carlo!? Methods like that exist but to be able to implement them requires that you know quite a lot about the target distribution and the whole point of MCMC is that you can do it in situations where you know very little about the target distribution. There is sampling technique called antithetic variables or something like this with this goal but it’s pretty limited. In high-dimensional complicated Bayesian models you’re stuck with this situation where the draws will be positively correlated but MCMC is very much an art and the art is to make that correlation as little as possible. The software package Stan uses something called Hamiltonian Monte Carlo which is an MCMC method but has the goal of producing chains that are not so highly autocorrelated. 11.4.3 Sample autocorrelation function \\[ \\operatorname{acf}_{t}(\\phi)=\\frac{\\frac{1}{S-t} \\sum_{s=1}^{S-t}\\left(\\phi_{s}-\\bar{\\phi}\\right)\\left(\\phi_{s+t}-\\bar{\\phi}\\right)}{\\frac{1}{S-1} \\sum_{s=1}^{S}\\left(\\phi_{s}-\\bar{\\phi}\\right)^{2}} \\] an expression for lag-t sample autocorrelation. Denominator is variance (recall that correlation = cov / sd\\(\\times\\)sd ) numerator is a sample correlation between.. say for lag \\(t=5\\), it will be between \\(\\phi^{(1)}\\) with \\(\\phi^{(6)}\\), \\(\\phi^{(2)}\\) with \\(\\phi^{(7)}\\), \\(\\phi^{(3)}\\) with \\(\\phi^{(8)}\\), etc. 11.4.4 Effective sample size \\(\\text{Var}_{MC}(\\bar \\phi) = \\text{Var}(\\phi) / S\\) The effective sample size for MCMC is defined by \\(\\text{Var}_{MC}(\\bar \\phi) = \\text{Var}(\\phi) / S_{\\text{eff}}.\\) So what this means is that when we run the Markov chain for \\(S\\) iterations we get the same precision for our MCMC approximation as we would get from \\(S_{\\text{eff}}\\) independent samples from \\(p(\\phi)\\) S &lt;- 10000 delta.Gibbs &lt;- rep(NA, S) theta.Gibbs &lt;- rep(NA, S) delta &lt;- 3; theta &lt;- 3; # starting values for(s in 1:S) { theta &lt;- rnorm(1, mu[delta], sigma[delta]) pdgt &lt;- p * dnorm(theta, mu, sigma) delta &lt;- sample(3, 1, prob=pdgt) delta.Gibbs[s] &lt;- delta; theta.Gibbs[s] &lt;- theta; } hist(theta.Gibbs, freq=F, right=F, col=&quot;pink&quot;, xlim=c(-6,6), breaks=30, ylim=c(0, .32), xlab=&quot;theta&quot;, ylab=&quot;p(theta)&quot;, main=&quot;&quot;) theta.vals &lt;- seq(-6, 6, .01) p.theta &lt;- p[1] * dnorm(theta.vals, mu[1], sigma[1]) + p[2] * dnorm(theta.vals, mu[2], sigma[2]) + p[3] * dnorm(theta.vals, mu[3], sigma[3]) lines(theta.vals, p.theta, lwd=2) Figure 11.5: 10,000 iterations of the Gibbs sampler instead of 1,000 Even with 10,000 iterations the realized chain is not perfectly representative of the target distribution. It’s too frequently around zero and not frequently enough around -3 and +3. There are numerous formulas for computing effective sample size they give different answers and in general the estimation of \\(S_{\\text{eff}}\\) is highly unstable. But it’s still a useful quantity. So let’s find it for this example. I like the effective sample size calculator that’s in the R package mcmcse library(mcmcse) ess(theta.Gibbs) ## [1] 13.54 The autocorrelation in this Markov chain is so severe that 10,000 iterations of the Gibbs sampler yields the same precision for estimating \\(E(\\theta)\\) as would only 13 or 14 independent draws from \\(p(\\theta)\\) (using iid Monte Carlo). CRAZY! par(mfrow=c(1,2)) acf(theta.Gibbs) acf(theta.Gibbs, lag.max =2000) Figure 11.6: autocorrelation function for theta-chain from the Gibbs sampler. We have \\(S-1\\) pairs as a basis for estimating lag 1 autocorrelation. It’s the correlation between \\((\\phi^{(1)} , …, \\phi^{(S-1)})\\) and \\((\\phi^{(2)} , …., \\phi^{(S)})\\). In general we have \\(S-t\\) data points to estimate the lag-\\(t\\) autocorrelation. At some point we run out of data points. When you get something like this in a problem you care about you have two choices (1) bigger \\(S\\). Sometimes that’s not feasible (2) figure out a better way to do Monte Carlo sampling. That’s the “art” of MCMC in practice. This was a toy example chosen precisely to make this point. One of the most MCMC-confounding situations we might encounter is multimodality. "],["multivariate-normal.html", "Lecture 12 Multivariate Normal 12.1 Example: Reading comprehension 12.2 The multivariate normal density 12.3 A semiconjugate prior distribution for the mean 12.4 The inverse-Wishart distribution 12.5 Full conditional distribution of the covariance matrix", " Lecture 12 Multivariate Normal The following notes, mostly transcribed from Neath(0520,2021) lecture, summarize sections(7.1-7.4) of Hoff(2009). 12.1 Example: Reading comprehension Our notation : \\(n =\\) sample size (number of subjects on which we have collected data) \\(p =\\) dimension of observed response vector (number of measurements taken on each subject) In this example, \\(n = 22, ~p = 2\\), \\(y_1 =\\) score on pretest, \\(y_2 =\\) score on posttest. For \\(p = 2\\) there are 5 parameters of interest; \\(\\theta_1 =\\) mean pretest scores, \\(\\theta_2 =\\) mean posttest scores, \\(\\sigma_1^2 =\\) variance of pretest scores, \\(\\sigma_2^2 =\\) variance of posttest scores, \\(\\rho = \\sigma_{1,2} / (\\sigma_1 \\times \\sigma_2)\\) the correlation between pretest scores and posttest scores. And we will assume that the pairs of test scores for a given student follow a bivariate normal distribution, even if they don’t follow perfectly. 12.2 The multivariate normal density With univariate data, there’s just one measurement taken on each subject. What are the concrete examples we have done so far? \\[ \\begin{aligned} y &amp;= \\text{number of tumors a mouse got}\\\\ y &amp;= \\begin{cases}{1 \\text{ if released is reincarcerated}\\\\ 0 \\text{ otherwise}} \\end{cases} \\end{aligned} \\] With multivariate data there are \\(p\\) measurements taken on each subject. So associated with each subject is a vector value \\(\\{ y_1, …, y_p \\}\\). If there are \\(p\\) components to the random variable there are \\(p\\) mean values, there are \\(p\\) variances, there \\(p(p-1)/2\\) covariances (the number of off-diagonal entries). \\(\\text{Cov}(Y_1, Y_2),\\text{Cov}(Y_1, Y_3),...,\\text{Cov}(Y_1, Y_p)\\) that’s \\(p-1\\) then we have \\(\\text{Cov}(Y_2, Y_3), \\text{Cov}(Y_2, Y_4),...,\\text{Cov}(Y_2, Y_p)\\) that’s \\(p-2\\) eventually we get to \\(\\text{Cov}(Y_{p-1} , Y_p ).\\) So the total number of covariances is \\(1 + 2 + … + p-1 = p(p-1) / 2\\). We can use the multivariate normal model if we want to make inference about mean values, variances(standard deviations) and correlations. This is what the density function for the multivariate normal distribution looks like; \\[ p(\\boldsymbol y | \\boldsymbol{\\theta,\\Sigma} ) = (2\\pi)^{-p/2}|\\boldsymbol \\Sigma|^{1/2}\\text{exp}\\{-(\\boldsymbol y - \\boldsymbol \\theta)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol y - \\boldsymbol \\theta)/2 \\} \\] When we have a multivariate distribution the mean becomes a mean vector(there is a mean for each variable). We’ll write \\(\\boldsymbol{\\theta}\\) in a bold font. \\(\\boldsymbol{\\theta}\\) is a \\(p \\times 1\\) vector, \\(\\theta_j = E( Y_j )\\). In univariate data in addition to the mean we have the variance. For multivariate data we have a variance-covariance matrix or just covariance matrix \\(= \\boldsymbol \\Sigma = \\text{Cov}[\\mathbf Y].\\) For a covariance matrix the \\(j\\)th diagonal entry is \\(\\sigma^2_j,\\) the variance for the \\(j\\)th variable. The \\((j, k)\\) entry of the covariance matrix is the covariance between \\(Y_j\\) and \\(Y_k\\). \\(\\sigma_{j,k} = \\text{Cov}( Y_j, Y_k )\\), \\(\\sigma_{j, j} = \\text{Var}( Y_j )\\). A subscript \\(i\\) means the \\(i\\)th of the \\(n\\) observations, so \\(\\boldsymbol y_i\\) is a \\(p \\times 1\\) vector. When I write \\(Y_j\\), I’m thinking of the distribution for the \\(j\\)th of the \\(p\\) variables. The exponential term of the univariate normal density is \\(-(1/2)(y - \\theta)^2 / \\sigma^2.\\) Write this as \\((y - \\theta) \\sigma^{-2} (y - \\theta).\\) This way generalizes to \\(p\\)-variate normal density \\((\\boldsymbol y-\\boldsymbol\\theta)^T\\boldsymbol \\Sigma^{-1}(\\boldsymbol y -\\boldsymbol\\theta)\\). The bigger are the entries of a matrix the bigger is the determinant. A matrix whose determinant is zero does not have an inverse matrix. Just as with univariate normal distribution where we require \\(\\sigma &gt; 0,\\) with MVN distribution determinant of the covariance matrix will always be positive, \\(|\\boldsymbol \\Sigma| &gt; 0\\). Just as \\(x \\times 1/x = 1, ~ \\mathbf{A A}^{-1} = \\mathbf{I},\\) which is the identity matrix. It has 1’s along the main diagonal and 0’s in the off-diagonal positions. \\(\\mathbf{A I} = \\mathbf{A} = \\mathbf{IA}\\) Bold-face capital letters (greek and latin both) will indicate matrices, bold-face lower-case letters will indicate vectors. A vector will always be a column vector, hence \\(\\mathbf{b}\\) is \\(p \\times 1\\), \\(\\mathbf{b}^T\\) is \\(1 \\times p\\). If \\(\\mathbf{b}\\) is a \\(p \\times 1\\) vector and \\(\\mathbf{A}\\) is a \\(p \\times p\\) matrix, then \\(\\mathbf{b}^T \\mathbf{A} \\mathbf{b}\\) is a scalar! \\((1 \\times p) (p \\times p ) (p \\times 1)\\). Similarly, \\(\\boldsymbol y\\) and \\(\\boldsymbol \\theta\\) are both \\(p\\)-vectors, \\(\\boldsymbol{\\Sigma}\\) is \\(p \\times p\\) matrix, \\((\\boldsymbol y - \\boldsymbol \\theta)\\) that’s a \\(p\\)-vector, \\((\\boldsymbol y - \\boldsymbol \\theta)^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol y - \\boldsymbol \\theta)\\) that’s a scalar! With \\(p=2\\) (bivariate normal distribution) we can “draw” the bivariate normal density using contour plots. A contour is a collection of points \\((y_1, y_2)\\) such that \\(p(y_1, y_2) =\\) same for all such \\((y_1, y_2)\\) on the same contour. The concentric circles corresponds to a set of values that are 95% of the peak, 85% of the peak, down to 0.001% of the peak. y1 &lt;- seq(0, 100, 1) y2 &lt;- seq(0, 100, 1) G &lt;- length(y1); H &lt;- length(y2); theta &lt;- c(50,50) Sigma2 &lt;- matrix(c(64, 0, 0, 144), 2, 2) Sigma1 &lt;- Sigma2; Sigma1[1,2] &lt;- -48; Sigma1[2,1] &lt;- -48; Sigma3 &lt;- Sigma2; Sigma3[1,2] &lt;- +48; Sigma3[2,1] &lt;- +48; p1 &lt;- matrix(NA, G, H) p2 &lt;- matrix(NA, G, H) p3 &lt;- matrix(NA, G, H) for(g in 1:G){ for(h in 1:H){ p1[g,h] &lt;- dmvnorm(c(y1[g],y2[h]), mean=theta, sigma=Sigma1, log=T) p2[g,h] &lt;- dmvnorm(c(y1[g],y2[h]), mean=theta, sigma=Sigma2, log=T) p3[g,h] &lt;- dmvnorm(c(y1[g],y2[h]), mean=theta, sigma=Sigma3, log=T) }} maxie &lt;- max(p1); p1 &lt;- p1 - maxie; rm(maxie); p1 &lt;- exp(p1); maxie &lt;- max(p2); p2 &lt;- p2 - maxie; rm(maxie); p2 &lt;- exp(p2); maxie &lt;- max(p3); p3 &lt;- p3 - maxie; rm(maxie); p3 &lt;- exp(p3); contours &lt;- c(.001, .01, seq(.05, .95, .10)) op &lt;- par(mfrow=c(1,3)) contour(y1, y2, p1, levels=contours, drawlabels=F, xlab=&quot;y1&quot;, ylab=&quot;y2&quot;, main=&quot;rho=-0.5&quot;) points(rmvnorm(n=1000, mean=theta, sigma=Sigma1), pch=19, cex=.5) contour(y1, y2, p2, levels=contours, drawlabels=F, xlab=&quot;y1&quot;, ylab=&quot;y2&quot;, main=&quot;rho=0&quot;) points(rmvnorm(n=1000, mean=theta, sigma=Sigma2), pch=19, cex=.5) contour(y1, y2, p3, levels=contours, drawlabels=F, xlab=&quot;y1&quot;, ylab=&quot;y2&quot;, main=&quot;rho=+0.5&quot;) points(rmvnorm(n=1000, mean=theta, sigma=Sigma3), pch=19, cex=.5) Figure 12.1: Contour plots for bivariate normal densities Take the middle density for example. The peak is in the middle. The innermost counter is the collection of all points \\((y_1, y_2)\\) such that \\(p(y_1, y_2) = 0.95 \\times\\)max-density. The outermost contour is the set of all points \\((y_1, y_2)\\) such that \\(p(y_1, y_2) = 0.001\\times\\) max-density. These three distributions have the same \\(\\theta_1 ~(\\)mean of \\(Y_1)\\) \\(\\theta_2 ~(\\)mean of \\(Y_2)\\) \\(\\sigma_1 ~(\\)sd of \\(Y_1) ~\\sigma_2 (\\)sd of \\(Y_2)\\) they only differ with respect to their correlation. If you want to describe a picture like this: Tell the person where it’s centered, how spread out the values are (refer to \\(\\theta\\) and \\(\\sigma\\) values for help with this) and how the variables are correlated. The middle plot shows an instance where the variables are uncorrelated (independent). LHS plot shows a negative correlation. RHS plot shows a positive correlation. To describe the spread you’ll have to read the labels on the axes otherwise every picture of a normal distribution looks exactly the same. Case in point, if I drew two BVN densities both have \\(\\boldsymbol \\theta = (0, 0)\\) one has \\(\\boldsymbol \\Sigma = \\begin{pmatrix} 1, 0 \\\\ 0, 1 \\end{pmatrix}\\) and one has \\(\\boldsymbol \\Sigma = \\begin{pmatrix} 100, 0\\\\ 0, 100\\end{pmatrix},\\) the two pictures will appear identical. You have to read the labels on the axes to describe the spread in a normal distribution. If \\((Y_1, Y_2)\\) are bivariate normal then marginally, \\(Y_1 \\sim \\text{Normal}(\\theta_1, \\sigma_1^2),\\) \\(Y_2 \\sim \\text{Normal}(\\theta_2, \\sigma_2^2)\\), \\(\\{Y_2 | Y_1=y_1\\} \\sim\\) Normal. We’ll save that discussion for another day. 12.3 A semiconjugate prior distribution for the mean Let’s talk about Bayesian inference about the \\(p\\)-vector \\(\\boldsymbol \\theta\\) and the \\(p\\times p\\) covariance matrix \\(\\boldsymbol \\Sigma\\) in that order. Just like we did for univariate normal let’s condition on \\(\\boldsymbol\\Sigma\\) and find the conjugate prior for \\(\\boldsymbol \\theta.\\) Just as the conjugate prior for the univariate normal sampling model was the univariate normal distribution we will see that the conjugate prior for the mean of a multivariate normal sampling model is multivariate normal. Let \\(\\boldsymbol \\theta\\) have a \\(p\\)-variate normal distribution with mean vector \\(\\boldsymbol\\mu_0\\) and covariance matrix \\(\\boldsymbol \\Lambda_0\\) and let \\(\\{Y_1, …, Y_n | \\theta\\}\\) be iid \\(p\\)-variate Normal \\((\\boldsymbol\\theta, \\boldsymbol\\Sigma)\\) Let’s look at the prior first. I’m only interested in terms that include a \\(\\theta\\) \\[ \\begin{aligned} p(\\boldsymbol\\theta) &amp;\\propto \\text{exp}\\left\\{-\\frac{1}{2} \\boldsymbol\\theta^{T} \\mathbf{A}_{0} \\boldsymbol\\theta+\\boldsymbol\\theta^{T} b_{0}\\right\\} \\end{aligned} \\] where, \\(\\mathbf{A}_0 = \\boldsymbol\\Lambda_0^{-1}\\), \\(\\mathbf b_0 = \\boldsymbol\\Lambda_0^{-1} \\boldsymbol\\mu_0\\) then \\(\\boldsymbol\\mu_0 = \\boldsymbol\\Lambda_0 \\mathbf b_0 = \\mathbf{A}_0^{-1}\\mathbf b\\) Consequence: The general form of a MVN density is \\(p(\\boldsymbol\\theta) = c \\times \\text{exp}\\{ -0.5 \\boldsymbol\\theta^T \\mathbf{A} \\theta + \\boldsymbol\\theta^T \\mathbf{b} \\}\\) for some matrix \\(\\mathbf{A}\\) and vector \\(\\mathbf b\\). Next, the sampling distribution. \\[ p(y_1, …, y_n | \\theta) \\propto \\exp \\left\\{-\\frac{1}{2} \\boldsymbol{\\theta}^{T} \\mathbf{A}_{1} \\boldsymbol{\\theta}+\\boldsymbol{\\theta}^{T} \\boldsymbol{b}_{1}\\right\\} \\] where \\(\\mathbf{A}_{1}=n \\boldsymbol\\Sigma^{-1},\\) \\(\\mathbf{b}_{1}=n \\boldsymbol\\Sigma^{-1} {\\boldsymbol{\\bar y}},\\) and \\({\\boldsymbol{\\bar y}}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} y_{i, 1}, \\ldots, \\frac{1}{n} \\sum_{i=1}^{n} y_{i, p}\\right)^T\\). Combining the above we get that \\[\\{\\boldsymbol \\theta \\mid \\boldsymbol y_{1}, \\ldots, \\boldsymbol y_{n}, \\boldsymbol \\Sigma\\} \\sim \\operatorname{Normal}_{p}\\left(\\boldsymbol \\mu_{n}, \\boldsymbol \\Lambda_{n}\\right)\\] where \\(\\boldsymbol \\Lambda_{n}=\\left(\\boldsymbol \\Lambda_{0}^{-1}+n \\boldsymbol \\Sigma^{-1}\\right)^{-1}\\) and \\(\\boldsymbol \\mu_{n}=\\left(\\boldsymbol \\Lambda_{0}^{-1}+n \\boldsymbol \\Sigma^{-1}\\right)^{-1}\\left(\\boldsymbol \\Lambda_{0}^{-1} \\boldsymbol \\mu_{0}+n \\boldsymbol \\Sigma^{-1} \\bar{\\boldsymbol y}\\right)\\) Just as in the univariate case the posterior precision, or inverse variance, is the sum of the prior precision and the data precision, and the posterior expectation is a weighted average of the prior expectation and the sample mean, weighted by their respective precisions. 12.4 The inverse-Wishart distribution For univariate data, the conjugate prior for variance was the inverse-gamma distribution. For multivariate data we have a covariance matrix. A covariance matrix has \\(p^2\\) entries however it must be symmetric, which means \\(\\sigma_{j,k} = \\sigma_{k,j}\\). So it’s not really \\(p^2\\) entries it’s something less than that because the top half = bottom half. Also it must be positive definite, which means \\(\\boldsymbol x^T \\boldsymbol x &gt; 0\\). We need a probability distribution for covariance matrices, defined on the set of all \\(p \\times p\\) symmetric positive definite matrices. This is a very complicated space, you can’t picture it in your head. We can construct such distributions from more basic things. Let’s say \\(\\boldsymbol z_1, \\boldsymbol z_2, …, \\boldsymbol z_n\\) are all \\(p\\)-vectors, then \\(\\boldsymbol {z_i z_i}^T\\) ( \\((p \\times 1)(1 \\times p)\\) that’s a \\(p \\times p\\) matrix ). \\[ \\sum_{i=1}^n \\boldsymbol z_i\\boldsymbol z_i^T = \\mathbf{Z}^T\\mathbf{Z} \\] is the sum of \\(n~~ p \\times p\\) matrices where the \\(i\\)th row of \\(\\mathbf{Z}_{n\\times p}\\) is \\(\\boldsymbol z_i^T\\). A way to construct a “random” covariance matrix is; sample \\(\\boldsymbol z_1,...,\\boldsymbol z_{\\nu_0} \\stackrel{\\text{iid}}\\sim\\) Normal\\(_p(\\boldsymbol 0, \\boldsymbol\\Phi_0)\\) calculate \\(\\mathbf{Z}^T\\mathbf{Z} =\\sum_{i=1}^{\\nu_0} \\boldsymbol z_i\\boldsymbol z_i^T\\). As long as \\(\\nu_0 &gt; p\\), the result will be a \\(p\\times p\\) covariance matrix called the Wishart\\((\\nu_0, \\Phi_0)\\) distribution \\(\\nu_0\\) is called the degrees of freedom and \\(\\Phi_0\\) is the scale matrix. The expected value, \\(E(\\mathbf{Z}^T\\mathbf{Z}),\\) is \\(\\nu_0 \\boldsymbol\\Phi_0\\) It will be symmetric and positive definite The Wishart distribution generalizes the gamma distribution (equivalently the chi-square distribution) to higher dimensions. \\(\\nu_0 &gt; p\\) guarantees that the \\(\\boldsymbol z_i\\) will be linearly independent. In this construction \\(\\nu_0\\) must be a positive integer. The bigger \\(\\nu_0,\\) is the more \\(\\boldsymbol z_i\\)’s are being added together the more there will be an “averaging out” of variation between the \\(\\boldsymbol z_i\\)’s. So if I have (sum of \\(n\\) things) the bigger \\(n\\) is the more variable this sum is. However the bigger \\(n\\) is the less variable (sum of \\(n\\) things)/\\(n\\) will be. It turns out the Wishart distribution is conjugate for the precision matrix in a multivariate normal model which means the inverse-Wishart distribution is conjugate for a covariance matrix in a multivariate normal model. We can define the inverse-Wishart distribution as follows; Let \\(\\boldsymbol z_i \\stackrel{\\text{iid}}\\sim\\) Normal\\(_p(\\boldsymbol 0, \\mathbf S_0^{-1})\\) then take the “sum of squares” of the \\(\\boldsymbol z_i\\) and invert it, so we have \\(\\boldsymbol \\Sigma = (\\mathbf{Z}^T\\mathbf{Z})^{-1}.\\) Under this simulation scheme, the precision matrix \\(\\mathbf \\Sigma^{-1}\\) has a Wishart\\((\\nu_0, \\boldsymbol S_0^{-1})\\) distribution, and the covariance matrix \\(\\boldsymbol \\Sigma\\) has an inverse-Wishart\\((\\nu_0, \\mathbf S_0^{-1})\\) where \\(\\nu_0 =\\) degrees of freedom (df) and \\(\\mathbf S_0^{-1} =\\) scale matrix. \\[ \\mathrm{E}(\\boldsymbol{\\Sigma}^{-1})=\\nu_{0} \\mathbf{S}_{0}^{-1} \\quad \\text { and } \\quad \\mathrm{E}(\\boldsymbol{\\Sigma})=\\frac{1}{\\nu_{0}-p-1} \\mathbf{S}_{0} \\] The expectation for a Wishart-distributed random matrix is df \\(\\times\\) scale matrix. The expectation for an inverse-Wishart random matrix is inverse of scale matrix divided by (df - \\(p\\) - 1) . This should help us figure out how to set a prior distribution for the covariance matrix. A sensible conjugate prior requires two things: (1) a prior best guess for the parameter value and (2) a fair assessment of your degree of confidence in that prior best guess which determines \\(\\nu_0.\\) You can’t have have \\(\\nu_0 &lt; p\\), only \\(\\nu_0 &gt; p\\). And to have a prior expectation, you need \\(\\nu_0 &gt; p+1.\\) So with very low confidence in your prior belief set \\(\\nu_0 = p+2,\\) and \\(\\mathbf S_0 = \\boldsymbol\\Sigma_0.\\) Howeber, if we are confident that the true covariance matrix is near some covariance matrix \\(\\boldsymbol\\Sigma_0\\), then set the scale matrix \\(\\mathbf S_0^{-1}\\) to be the inverse of \\(\\mathbf S_0\\) where \\(\\mathbf S_0 = (\\nu_0 -p -1) \\mathbf\\Sigma_0\\). 12.5 Full conditional distribution of the covariance matrix If \\(p(\\boldsymbol\\Sigma)\\) is an inverse-Wishart density and \\(p(\\boldsymbol y | \\boldsymbol \\theta, \\boldsymbol \\Sigma)\\) is a Normal\\(_p(\\boldsymbol \\theta , \\boldsymbol \\Sigma)\\) likelihood then \\[ p(\\boldsymbol \\Sigma | \\boldsymbol y, \\boldsymbol \\theta) \\sim \\text{inverse-Wishart}(\\nu_0+n, [\\mathbf S_0+\\mathbf S_{\\theta}]^{-1}) \\] \\(\\mathbf S_{\\theta} = \\sum_{i=1}^n (\\boldsymbol y_i - \\boldsymbol\\theta)(\\boldsymbol y_i-\\boldsymbol\\theta)^T\\\\\\) The posterior expectation of \\(\\boldsymbol\\Sigma\\) is a weighted average of the prior expectation and the sample covariance matrix (\\(1/n \\times \\mathbf S_{\\theta}\\)) conditional on mean \\(\\boldsymbol \\theta\\) being known. \\[ \\begin{array}{l} \\mathrm{E}\\left(\\boldsymbol{\\Sigma} \\mid \\boldsymbol{y}_{1}, \\ldots, \\boldsymbol{y}_{n}, \\boldsymbol{\\theta}\\right)=\\frac{1}{\\nu_{0}+n-p-1}\\left(\\mathbf{S}_{0}+\\mathbf{S}_{\\theta}\\right) \\\\ \\quad=\\frac{\\nu_{0}-p-1}{\\nu_{0}+n-p-1} \\cdot \\frac{1}{\\nu_{0}-p-1} \\mathbf{S}_{0}+\\frac{n}{\\nu_{0}+n-p-1} \\cdot \\frac{1}{n} \\mathbf{S}_{\\theta} \\end{array} \\] Since we have full conditional distributions for \\(\\boldsymbol \\theta\\) and \\(\\boldsymbol \\Sigma\\), we can do a Gibbs sampler! We just come up with a reasonable starting value and alternate between the two full conditionals. Starting values? easy enough. Start \\(\\boldsymbol \\theta\\) at \\(\\bar{\\boldsymbol y}\\) (sample mean vector) and start \\(\\boldsymbol \\Sigma\\) at sample covariance matrix. Choosing priors We want \\(\\nu_0\\) to be small relative to \\(n\\) but we need it to be at least \\(&gt; p+1\\) so set it to \\(p+2 = 4\\) and it will not get a lot of weight relative to \\(n=22\\). The relative weights in this posterior expectation are \\(n=22\\) for the sample data and \\(\\nu_0 - p - 1 = 4 - 2 - 1 = 1\\) for the prior. So the posterior expectation ( conditional on \\(\\boldsymbol \\theta\\) ) will be a \\(22/23\\) versus \\(1/23\\) weighted toward the data. # Reading comprehension example from Chapter 7 of Hoff (2009) # y1 is pretest score, y2 is posttest score y1 &lt;- c(59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 70, 34, 50, 41, 52, 60, 34, 28, 35) y2 &lt;- c(77, 39, 46, 26, 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 60, 75, 47, 48, 33) y &lt;- data.frame(cbind(y1, y2)); rm(y1, y2); # Hyperparamaters mu.0 &lt;- c(50, 50) Lambda0 &lt;- matrix(c(625, 312.5, 312.5, 625), 2, 2) S.0 &lt;- Lambda0; nu.0 &lt;- 4; # Data summaries n &lt;- dim(y)[1] p &lt;- dim(y)[2] ybar &lt;- apply(y, 2, mean) Cov.y &lt;- cov(y) ybar ## y1 y2 ## 47.18 53.86 Cov.y ## y1 y2 ## y1 182.2 148.4 ## y2 148.4 243.6 # Gibbs sampler approximation to posterior distribution S &lt;- 5000 theta &lt;- ybar # initial values Sigma &lt;- Cov.y # initial values # Calculations that will be used repeatedly Lambda0.inv &lt;- solve(Lambda0) Lam.inv.mu.0 &lt;- Lambda0.inv %*% mu.0 nu.n &lt;- nu.0 + n # Now generate the Markov chains: theta and Sigma theta.chain &lt;- matrix(NA, S, p) Sigma.chain &lt;- matrix(NA, S, p^2) for(s in 1:S) { n.Sigma.inv &lt;- n * solve(Sigma) Lambda.n &lt;- solve( Lambda0.inv + n.Sigma.inv) mu.n &lt;- Lambda.n %*% (Lam.inv.mu.0 + n.Sigma.inv %*% ybar) theta &lt;- rmvnorm(1, mu.n, Lambda.n)[1,] S.n &lt;- S.0 + (n-1)*Cov.y + n * (ybar-theta) %*% t(ybar-theta) Sigma &lt;- solve( rWishart(1, nu.n, solve(S.n))[,,1] ) theta.chain[s,] &lt;- theta Sigma.chain[s,] &lt;- Sigma } # posterior covariance matrix matrix(apply(Sigma.chain, 2, mean),2) ## [,1] [,2] ## [1,] 202.6 156.4 ## [2,] 156.4 260.9 Posterior summaries # 95% interval for theta2 - theta1 quantile(theta.chain[,2] - theta.chain[,1], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 1.484 6.600 11.795 We estimate that \\(\\theta_2 - \\theta_1\\) is between 1.5 and 11.8 # Pr(theta2 &gt; theta1 | y) mean(theta.chain[,2] &gt; theta.chain[,1]) ## [1] 0.9936 \\(Pr(\\theta_2 &gt; \\theta_1 | \\boldsymbol y) &gt; 0.99\\). So very strong evidence that the instruction program is effective. We can also draw a scatterplot of the simulated pairs, \\((\\theta_1^{(s)}, \\theta_2^{(s)}),\\) to approximate the marginal posterior \\(p(\\theta_1, \\theta_2 | \\boldsymbol y)\\) plot(theta.chain, cex=.5, xlab=expression(theta[1]), ylab=expression(theta[2])) abline(0,1, lwd=2, col=&quot;pink&quot;) Figure 12.2: Marginal posterior distribution of (theta1, theta2) 99.4% of the line is above the 45 degree line which is why we have the high believe that the treatment program is effective on average. Now let’s ask a slightly different question. What is the probability that a randomly selected child will score higher on the second exam than on the first? To answer this, we sample from the posterior predictive distribution \\(p(\\tilde y|\\boldsymbol y)\\). # Posterior predictive simulations Y.tilde &lt;- matrix(NA, S, p) for (s in 1:S){ Y.tilde[s,] &lt;- rmvnorm(1, mean=theta.chain[s,], sigma=matrix(Sigma.chain[s,],2,2))[1,] } mean(Y.tilde[,2] &gt; Y.tilde[,1]) ## [1] 0.7178 plot(Y.tilde, cex=.5, xlab=expression(tilde(y[1])), ylab=expression(tilde(y[2]))) abline(0, 1, lwd=2, col=&quot;pink&quot;) Figure 12.3: Posterior predictive distribution There is A LOT more variability in the posterior predictive distribution than in the posterior distribution of \\(\\boldsymbol \\theta\\). The posterior distribution of \\((\\theta_1, \\theta_2)\\) sits 99% above the 45-degree line, the posterior predictive distribution \\((\\tilde y_1, \\tilde y_2)\\) sits 70% above the 45-degree line. "],["group-comparisons.html", "Lecture 13 Group comparisons 13.1 Comparing two groups 13.2 A Bayesian model", " Lecture 13 Group comparisons The following notes, mostly transcribed from Neath(0524,2021) lecture, summarize section 8.1 of Hoff(2009). What’s going on with our masterplan? We have jumped from 07a to 08a there was to be a 07b(Missing data and imputation) but we’re skipping that at least for now. We were two days behind schedule, by cancelling the 07b lesson we’re now one day behind. 13.1 Comparing two groups We have a concrete problem to motivate us y1 &lt;- c(52.11, 57.65, 66.44, 44.68, 40.57, 35.04, 50.71, 66.17, 39.43, 46.17, 58.76, 47.97, 39.18, 64.63, 69.38, 32.38, 29.98, 59.32, 43.04, 57.83, 46.07, 47.74, 48.66, 40.80, 66.32, 53.70, 52.42, 71.38, 59.66, 47.52, 39.51) y2 &lt;- c(52.87, 50.03, 41.51, 37.42, 64.42, 45.44, 46.06, 46.37, 46.66, 29.01, 35.69, 49.16, 55.90, 45.84, 35.44, 43.21, 48.36, 74.14, 46.76, 36.97, 43.84, 43.24, 56.90, 47.64, 38.84, 42.96, 41.58, 45.96) boxplot(list(y1, y2), ylab=&quot;score&quot;, names=c(&quot;school 1&quot;, &quot;school 2&quot;), col=0) Figure 13.1: Boxplots of samples of 10th grade math scores from two schools The appearance is: mean score at school 1 is higher than that at school 2. But these 31 and 28 students are not the entire the entire school, they are just a sample from a larger population of 10th grade students. So might this difference in mean score be attributable to sampling error? i.e, might a different set of 31 and 28 students give different means? How do we test this? Let \\(\\theta_1 =\\) mean score at school 1 (that’s the population mean!) \\(\\theta_2 =\\) mean score at school 2 (population mean!) We want to test: \\(H_0: \\theta_1 = \\theta_2\\) against \\(H_a: \\theta_1 \\neq \\theta_2\\) If we assume that both populations are normal with the same variance just possibly different mean then we can conduct this test by two-sample t-test . The \\(t\\)-statistic: \\[ t(\\boldsymbol y_1, \\boldsymbol y_2) = \\frac{\\bar y_1 - \\bar y_2}{s_p\\sqrt{1/n_1 + 1/n_2}}, \\text{ where } s_p^2 = \\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1 + n_2 -2} \\] This is the classic test statistic for classical hypothesis testing (not bayesian). Our best estimate of \\(\\sigma\\) (assuming equal variance) is the pooled sample standard deviation. A test statistic measures the difference between the data you got and the data you’d have expected if \\(H_0\\)(null hypothesis) were true. The data we got had a difference in means of about 4.66 points. What we’d expect if \\(H_0\\) were true is no difference in sample means. n1 &lt;- length(y1); y1bar &lt;- mean(y1); s1 &lt;- sd(y1); n2 &lt;- length(y2); y2bar &lt;- mean(y2); s2 &lt;- sd(y2); y1bar-y2bar ## [1] 4.663 # pooled sample standard deviation sp &lt;- sqrt( ( (n1-1)*s1^2 + (n2-1)*s2^2 ) / (n1+n2-2)) # test-statistic (t.stat &lt;- (y1bar- y2bar) / (sp * sqrt(1/n1 + 1/n2)) ) ## [1] 1.742 We get t-stat \\(= 1.74\\). The difference in sample means that we observed is \\(1.74\\) standard errors greater than the difference we’d expect to observe if \\(H_0\\) were true. If \\(H_0\\) is true the \\(t\\)-statistic has a null sampling distribution that is the t-distribution with degrees of freedom \\(= n_1 + n_2 - 2=57\\). We expect the t-statistic to not be far out in the t-distribution. tvals &lt;- seq(-4, 4, 0.01) plot(tvals, dt(tvals, df=n1+n2-2), type=&quot;l&quot;, lwd=2, ylab=&quot;&quot;,xlab=&quot;&quot;) segments(1.74,0,1.74, dt(1.74, df=n1+n2-2), col=&quot;red&quot;, lwd=2, lty=3) segments(-1.74,0,-1.74, dt(1.74,df=n1+n2-2), col=&quot;red&quot;, lwd=2, lty=3) abline(h=0) Figure 13.2: The null distribution for testing equality of the population means. The red dashed line indicates the observed value of the t-statistic. P-value is the measure of the strength against the null hypothesis. The smaller the p-value the more unlikely would be the data you got if the null hypothesis were true. # two-sided p-value 2 * (1 - pt(abs(t.stat), df=n1+n2-2)) ## [1] 0.08693 If the two populations indeed follow the same normal population, then the pre-experimental probability of sampling a dataset that would generate a value of \\(t(\\mathbf Y_1, \\mathbf Y_2)\\) greater in absolute value than 1.74 is 8.7% For a two-sided alternative hypothesis the p-value counts both tail probabilities. P-value \\(= 0.087\\) is generally considered (by convention) to be not very strong evidence against \\(H_0.\\) The data we observed would have been not entirely unexpected with \\(H_0\\) being true so it’s only weak evidence that \\(H_0\\) is false. # Or just go t.test(y1, y2, var.equal=T) ## ## Two Sample t-test ## ## data: y1 and y2 ## t = 1.7, df = 57, p-value = 0.09 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.6977 10.0234 ## sample estimates: ## mean of x mean of y ## 50.81 46.15 If our job is to estimate \\(\\theta_1\\) and \\(\\theta_2,\\) the mean scores at the two schools, we have a decision to make. Should we combine the information at both schools and report pooled estimates for both? \\(\\hat \\theta_1 = \\hat \\theta_2 = (n_1 \\bar y_1+ n_2\\bar y_2) / (n_1 + n_2)\\) Or report separate estimates? \\(\\hat \\theta_1 = \\bar y_1\\) \\(\\hat \\theta_2 = \\bar y_2\\) Model selection (deciding between single mean or separate-means model) based on p-values: One proposed recipe for making this decision is to do it based on the result of the significance test. Conduct a test of \\(H_0: \\theta_1 = \\theta_2\\). Reject if p-value \\(&lt; \\alpha\\), take \\(\\alpha = 0.05\\) If you reject this \\(H_0\\) then you’ll want to report separate estimates. If you fail to reject interpret this to mean we “accept” the null and report the the pooled estimate. This is not entirely satisfactory. In this example (following this recipe) our conclusion would be; Do not reject the null. So take the average score of all \\(31 + 28 = 59\\) students as the estimated average score for both schools. On the other hand the data suggest \\(\\theta_1 &gt; \\theta_2.\\) It wasn’t conclusive statistical proof but it’s what the data indicated. So maybe it would be better to use separate estimates. On the other hand, suppose we get a p-value of 0.051 or 0.049? In general, if you are following a methodology that will do one thing if you get a p-value = 0.049 and something substantially different if you get a p-value of 0.051 you need to rethink your life choices. Another way to think about this: We are saying that we will estimate \\(\\theta_1\\) (mean score in school 1) by a weighted average of the two school averages \\(w\\bar y_1 + (1-w) \\bar y_2\\). It makes sense that \\(w\\) is closer to 1 than 0 but maybe the observed scores of students at school 2 contain valuable info about the average score at school 1. That’s the big insight of hierarchical statistical models, the idea that that data from different (but similar) sources can contribute to our estimation at the current source. This is obviously a very Bayesian idea. What that naïve testing-based recipe says is: either take \\(w = 1\\) or take \\(w \\approx 1/2\\). What if there’s a better answer somewhere in between? Wouldn’t it be cool to have a method that would return the estimate \\(\\hat \\theta_1 = w\\bar y_1 + (1-w) \\bar y_2\\) and would tell us what \\(w\\) should be based on say sample size and variance?! The method that does this is the Bayesian hierarchical model. 13.2 A Bayesian model Let’s make a hierarchical Bayesian model for estimating two related means. Consider the sampling model: \\[ \\begin{aligned} Y_{i, 1} &amp;=\\mu+\\delta+\\epsilon_{i, 1} \\\\ Y_{i, 2} &amp;=\\mu-\\delta+\\epsilon_{i, 2} \\\\ \\epsilon_{i, j} &amp; \\sim \\text { iid } \\operatorname{Normal}\\left(0, \\sigma^{2}\\right) \\end{aligned} \\] This model says: The \\(i\\)th score at school \\(j\\) is \\(\\mu + \\delta\\) for \\(j=1\\) or \\(\\mu -\\delta\\) for \\(j=2\\) plus \\(\\epsilon_{i,j}\\) (student random effect). \\(\\mu\\) is the overall mean at both schools \\(i\\) is the unit, \\(j\\) is the group(school effect) \\(\\mu\\) and \\(\\delta\\) are model parameters. The third is \\(\\sigma^2,\\) the variance of the random error term. A reparameterization of the two-means model is; \\(\\theta_1 = \\mu + \\delta,\\) \\(\\theta_2 = \\mu - \\delta\\), \\(\\mu = (\\theta_1+\\theta_2) / 2\\) \\(\\delta = (\\theta_1 - \\theta_2) / 2\\). Hence \\(2\\times\\delta\\) is the difference in means \\(\\sigma^2 =\\) variance among scores by students at the same school (within group variance) The conjugate prior for this model: \\(\\mu \\sim \\text{Normal}(\\mu_0, \\gamma_0^2)\\) \\(\\delta \\sim \\text{Normal}(\\delta_0, \\tau_0^2)\\) \\(\\sigma^2 \\sim\\) Inverse-gamma\\((\\nu_0/2,~\\nu_0\\sigma_0^2/2)\\) The posterior full conditionals: \\(\\{\\mu|\\boldsymbol y_1, \\boldsymbol y_2,\\delta,\\sigma^2\\} \\sim \\text{Normal}(\\mu_n,\\gamma_n^2)\\) where \\[ \\begin{array}{l} \\gamma_{n}^{2}=\\left[\\frac{1}{\\gamma_{0}^{2}}+\\frac{n_{1}+n_{2}}{\\sigma^{2}}\\right]^{-1}\\\\[0.1cm] \\mu_{n}=\\gamma_{n}^{2} \\times\\left[\\frac{\\mu_{0}}{\\gamma_{0}^{2}}+\\frac{\\sum_{i=1}^{n_{1}}\\left(y_{i, 1}-\\delta\\right)+\\sum_{i=1}^{n_{2}}\\left(y_{i, 2}+\\delta\\right)}{\\sigma^{2}}\\right] \\end{array} \\] What is going on above? Observe \\(E(\\bar y_1 | \\mu, \\delta) = \\mu + \\delta\\) \\(E(\\bar y_2 | \\mu, \\delta) = \\mu - \\delta\\) \\(E(\\mu | y_1, \\delta) = \\bar y_1 - \\delta\\) \\(E(\\mu | y_2, \\delta) = \\bar y_2 + \\delta\\) \\(1/\\gamma_n^2 = 1/\\gamma_0^2 + n_1/\\sigma^2 + n_2/\\sigma^2\\) Posterior expectation \\(\\mu_n\\) is a weighted average of \\(\\mu_0\\) (weight is \\(1 / \\gamma_0^2\\) ), \\(\\bar y_1\\) \\(- \\delta\\) (weight \\(n_1/\\sigma^2\\)) and \\(\\bar y_2 + \\delta ~ (\\)weight of \\(n_2/\\sigma^2)\\) Next we have; \\(\\{\\delta|\\boldsymbol y_1, \\boldsymbol y_2,\\mu,\\sigma^2\\} \\sim \\text{Normal}(\\delta_n,\\tau_n^2)\\) where \\[ \\begin{array}{l} \\tau_{n}^{2}=\\left[\\frac{1}{\\tau_{0}^{2}}+\\frac{n_{1}+n_{2}}{\\sigma^{2}}\\right]^{-1} \\\\[0.1cm] \\delta_{n}=\\tau_{n}^{2}\\times\\left[\\frac{\\delta_{0}}{\\tau_{0}^{2}}+\\frac{\\sum_{i=1}^{n_{1}}\\left(y_{i, 1}-\\mu\\right)-\\sum_{i=1}^{n_{2}}\\left(y_{i, 2}-\\mu\\right)}{\\sigma^{2}}\\right] \\end{array} \\] \\(E(\\bar y_1 | \\mu, \\delta) = \\mu + \\delta\\) \\(E(\\bar y_2 | \\mu, \\delta) = \\mu - \\delta\\), so \\(E(\\delta | \\mu, \\bar y_1) = \\bar y_1 - \\mu\\) \\(E(\\delta | \\mu, \\bar y_2) = -(\\bar y_2 - \\mu)\\) \\(1/\\tau_n^2 = 1 /\\tau_0^2+n_1 / \\sigma^2+n_2 / \\sigma^2\\) The posterior expectation \\(\\delta_n\\) is a weighted average of \\(\\delta_0\\) (weight \\(1 / \\tau_0^2\\) ), \\(\\bar y_1 - \\mu\\) ( weight of \\(n_1/\\sigma^2\\) ) and \\(\\bar y_2 - \\mu\\) (weight of \\(n_2 / \\sigma^2\\) ) Finally, we have; \\(\\{\\sigma^2|\\boldsymbol y_1, \\boldsymbol y_2,\\mu,\\delta\\} \\sim \\text{Inverse-gamma}(\\nu_n/2,\\nu_n\\sigma_n^2/2)\\) where \\[ \\begin{array}{l} \\nu_{n}=\\nu_{0}+n_{1}+n_{2}\\\\[0.05cm] \\sigma_{n}^{2}=\\frac{\\sum_{i=1}^{n_{1}}\\left(y_{i, 1}-[\\mu+\\delta]\\right)^{2}+\\sum_{i=1}^{n_{2}}\\left(y_{i, 2}-[\\mu-\\delta]\\right)^{2}}{n_1+n_2} \\end{array} \\] Student question: what is \\(\\tau_0?\\) We’ve got more variances than we are used to so we’re using more greek letters than ever before. Let’s get em straight. A greek letter with no subscript is a random variable and a greek letter with a subscript is a “constant” in this notation system) \\(\\sigma^2\\) represents the variance of the observed responses as usual. Its prior is Inverse-gamma\\(( \\nu_0/2 ,~ \\nu_0\\sigma_0^2/ 2)\\). \\(\\gamma_0^2\\) and \\(\\tau_0^2\\) are the variances for the priors of \\(\\mu\\) and \\(\\delta\\) respectively. There are two unknown means \\(\\theta_1\\) and \\(\\theta_2\\). And we’ve reparameterized \\(\\theta_1 = \\mu + \\delta\\) and \\(\\theta_2 = \\mu - \\delta\\). Our uncertainty about \\(\\mu\\) is measured by \\(\\gamma_0^2\\). Our uncertainty about \\(\\delta\\) is measured by \\(\\tau_0^2\\) 13.2.1 Analysis of the math scores data We need: \\(\\mu_0\\) (best guess at combined average) \\(\\delta_0\\) (best guess at difference between the schools) \\(\\gamma_0^2\\) and \\(\\tau_0^2\\) (the corresponding variances for those two things) \\(\\sigma_0^2\\) our best guess at the within-school / between-students variance in scores \\(\\nu_0\\) Let \\(\\mu_0 = 50\\) and \\(\\sigma_0 = 10\\). That’s the designed average score and SD for this test. Let \\(\\delta_0= 0\\) because we no prior info that one school is expected to have higher average than other. Scores range from 0 to 100 so let’s make \\(\\gamma_0 = 25\\) (mean \\(\\pm 2\\)SD = \\(50\\pm2\\gamma_0\\le100,\\) covers range of possible values). Let \\(\\tau_0^2=\\gamma_0^2= 625\\). To make that prior “diffuse” for \\(\\sigma^2?\\) take \\(\\nu_0 = 1\\) How are we gonna do posterior simulation? We know the full conditionals: \\(p(\\mu | \\boldsymbol y_1,\\boldsymbol y_2, \\delta, \\sigma^2)\\) \\(p(\\delta | \\boldsymbol y_1,\\boldsymbol y_2, \\mu, \\sigma^2)\\) \\(p(\\sigma^2 | \\boldsymbol y_1,\\boldsymbol y_2, \\mu, \\delta)\\) This looks like a job for the Gibbs sampler If we want to sample \\((X, Y, Z)\\) we can do this if we know \\(p(x)\\) and \\(p(y|x)\\) and \\(p(z | x, y)\\). But, if all we know is \\(p(x | y,z)\\), \\(p(y | x,z)\\), and \\(p(z| x, y)\\), we can’t do direct simulation. That’s where the Gibbs sampler comes in, but it’s not as good as method 1 because the simulated draws may be correlated. With a Gibbs sampler: we need starting values! \\(\\phi^{(s)}\\) is generated from \\(\\phi^{(s-1)}\\), so we need a \\(\\phi^{(0)}\\) to be able to get a \\(\\phi^{(1)}\\). But remember: You only need starting values for \\(p-1\\) of the \\(p\\) parameters. We’re gonna start by sampling \\(\\sigma^{2(1)}\\sim p(\\sigma^2 | y_1, y_2, \\mu^{(0)}, \\delta^{(0)})\\). Sensible starting values are obvious! Let \\(\\mu =\\) overall average, \\(\\delta =\\) half the difference in means # Hyperparmeters mu.0 &lt;- 50 ; gamma2.0 &lt;- 625; delta.0 &lt;- 0 ; tau2.0 &lt;- 625; sigma2.0 &lt;- 100; nu.0 &lt;- 1 ; # Starting values mu &lt;- (y1bar + y2bar) / 2 delta &lt;- (y1bar - y2bar) / 2 # Now let&#39;s generate the Markov chain! S &lt;- 5000 mu.chain &lt;- rep(NA, S); delta.chain &lt;- rep(NA, S); sigma2.chain &lt;-rep(NA, S) ; for(s in 1:S) { # First update sigma2, then mu then delta sigma2 &lt;- 1/rgamma(1, (nu.0 + n1 + n2) / 2, (nu.0*sigma2.0 + sum( (y1 - mu - delta)^2 ) + sum( (2 - mu + delta)^2 ) ) / 2 ) gamma2.n &lt;- 1 / (1/gamma2.0 + (n1+n2)/sigma2) mu.n &lt;- gamma2.n * (mu.0/gamma2.0 + ( sum(y1-delta) + sum(y2+delta) ) / sigma2 ) mu &lt;- rnorm(1, mean=mu.n, sd=sqrt(gamma2.n)) tau2.n &lt;- 1 / (1/tau2.0 + (n1+n2)/sigma2) delta.n &lt;- tau2.n * (delta.0/tau2.0 + ( sum(y1-mu) - sum(y2-mu)) / sigma2 ) delta &lt;- rnorm(1, mean=delta.n, sd=sqrt(tau2.n)) mu.chain[s] &lt;- mu delta.chain[s] &lt;- delta sigma2.chain[s] &lt;- sigma2 } par(mfrow=c(1,2)) plot(mu.chain+delta.chain, mu.chain-delta.chain, xlab=xlab, ylab=ylab,pch=19,cex=0.3) plot(mu.chain, delta.chain,pch=19,cex=0.3,xlab=xlab2,ylab=ylab2) Figure 13.3: Joint distribution for (theta1,theta2) and (mu,delta). This is the scatterplot of the joint posterior distribution of \\((\\theta_1, \\theta_2).\\) They seem to be uncorrelated. \\((\\mu, \\delta)\\) are likewise uncorrelated in their posterior distribution. This makes sense since we have an orthogonal transformation of the parameters. Student question: What is orthogonal transformation of the parameters? Ans: If \\(\\theta_1\\) and \\(\\theta_2\\) are independent then \\((\\theta_1 + \\theta_2)/2\\) and \\((\\theta_1 - \\theta_2) / 2\\) are also independent because (+1 +1) and (+1 -1) are orthogonal. \\(\\begin{pmatrix} 0.5,0.5\\\\0.5,-.5 \\end{pmatrix}^T=\\begin{pmatrix} 0.5,0.5\\\\0.5,-.5 \\end{pmatrix}^{-1}\\) (I think). par(mfrow=c(1,2)) plot(density(mu.chain), xlim=c(25, 75), lwd=2, xlab=expression(mu), ylab=&quot;density&quot;, main=&quot;&quot;) mu.vals &lt;- seq(25, 75, .10) lines(mu.vals,dnorm(mu.vals, mu.0, sqrt(gamma2.0)), lwd=2, col=&quot;pink&quot;) legend(&quot;topright&quot;, inset=.05, lwd=2, col=c(&quot;pink&quot;, &quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), cex=0.6) plot(density(delta.chain), xlim=c(-25, 25), lwd=2, xlab=expression(delta), ylab=&quot;density&quot;, main=&quot;&quot;) delta.vals &lt;- seq(-25, 25, .10) lines(delta.vals, dnorm(delta.vals, delta.0, sqrt(tau2.0)), lwd=2, col=&quot;pink&quot;) legend(&quot;topleft&quot;, inset=.05, lwd=2, col=c(&quot;pink&quot;, &quot;black&quot;), legend=c(&quot;Prior&quot;, &quot;Posterior&quot;), cex=0.6) Figure 13.4: Marginal posteriors of mu and delta. These plots show posteriors and priors for \\(\\mu\\) and \\(\\delta\\). Our prior on \\(\\mu\\) was Normal(mean=50, sd=25). The posterior mean is shifted left a bit (because the combined average is a little less than 50). Our prior on \\(\\delta\\) is is Normal(0, sd=25). The posterior is shifted right a bit because we have some belief now that \\(\\theta_1 &gt; \\theta_2\\) Notice how these are very close. (y1bar+y2bar)/2; mean(mu.chain) ## [1] 48.48 ## [1] 48.47 mean(delta.chain)*2; y1bar-y2bar ## [1] 4.627 ## [1] 4.663 # Posterior belief about other quantities of interest # 2*delta = theta1 - theta2 quantile(2*delta.chain, c(.025, .975)) ## 2.5% 97.5% ## -0.4788 9.7720 We are 95% confident that the mean score at school 1 is between a half-point less and 9.75 points greater than the mean score at school 2 so this is consistent with our t-test inference were we got \\([-0.698, 10.023]\\). The data were not conclusive (no strong evidence that \\(\\theta_1 &gt; \\theta_2\\) ) but the data suggested that. The Bayesian interval is a bit shorter than the t-test interval because we used some prior information. # Posterior belief that theta1 &gt; theta2 mean(delta.chain &gt; 0) ## [1] 0.9616 \\(\\delta = (\\theta_1 - \\theta_2) / 2\\), \\(\\delta &gt; 0\\) means average score is higher at school 1. Our posterior belief that the average score is higher at school 1 is about 96%. What’s the predictive probability a school 1 student outscores a school 2 student? It should be \\(&gt; 0.5\\) but maybe not by much. Let’s see… # Posterior predictive simulation y1.tilde &lt;- rnorm(S, mean=mu.chain+delta.chain, sd=sqrt(sigma2.chain)) y2.tilde &lt;- rnorm(S, mean=mu.chain-delta.chain, sd=sqrt(sigma2.chain)) mean(y1.tilde &gt; y2.tilde) ## [1] 0.6246 About a 62% probability that the school 1 student scores higher. Looking back (right before 13.2) we said the ‘best estimate’ of \\(\\theta_1\\) should be somewhere between just \\(\\bar y_1\\) and a straight average of \\(\\bar y_1 \\&amp; \\bar y_2\\), but we didn’t actually solve that did we? No. But is there an ‘implicit’ \\(w\\)? there must be, right? Let’s see! \\(\\bar y_1 = 50.8\\) that would be the “no pooling” estimate. y1bar ## [1] 50.81 The average of the two schools \\((\\bar y_1 + \\bar y_2) / 2 = 48.5\\). That would be the “fully pooled”estimate (in Gelman’s terminology). (y1bar+y2bar)/2 ## [1] 48.48 The “hierarchical model-based estimated” (which is what we did) SHOULD be between these two; \\(\\hat \\theta_1 = \\hat\\mu+\\hat\\delta=50.78.\\) So it’s a lot closer to no pooling. It’s lowered just a bit by school 2 which makes sense. mean(mu.chain+delta.chain) ## [1] 50.78 So even though the classical model testing approach would lead to pooling the optimal answer is a lot closer to no pooling. The author (Hoff) lost me a bit on some pages from section 8.3. If you find yourself similarly lost don’t feel bad about that. Tomorrow: We will consider \\(m\\) different groups not just two. Our model will say; \\(\\{y_{i,j} | \\theta, \\sigma^2\\} \\sim\\) Normal\\((\\theta_j , \\sigma^2)\\) where \\(y_{i,j}\\) is the \\(i\\)th response from the \\(j\\)th group. \\(i\\) goes from 1 to \\(n_j\\), \\(j\\) goes up to \\(m\\). \\(\\theta_j\\) is the true mean for the \\(j\\)th group. \\(\\{\\theta_1, …., \\theta_m | \\mu, \\tau\\} \\sim\\) iid Normal\\(( \\mu, \\tau^2 )\\). Think about the data coming about this way: There are a whole bunch of schools out there. Our data include a sample of \\(m\\) of them. At each of these schools there are a whole bunch of students. We sample \\(n_j\\) students at school \\(j,\\) \\(\\theta_j\\) represents mean score at \\(j\\)th school. The hierarchical model says: \\(y_{i,j} \\sim\\) Normal\\(( \\theta_j , \\sigma^2 )\\) \\(\\theta_1, …, \\theta_m\\) are iid Normal\\(( \\mu, \\tau^2 )\\). So the \\(\\theta_j\\) are sampled from a population with mean \\(\\mu\\) and variance \\(\\tau^2\\). So the model parameters are: \\(\\sigma^2\\) (within-school variance), \\(\\tau^2\\) (between-school variance ) and \\(\\mu\\) the overall mean. Note that the \\(\\theta_j\\)’s are not model parameters. "],["the-hierarchical-normal-model.html", "Lecture 14 The hierarchical normal model", " Lecture 14 The hierarchical normal model "]]
