<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 8 Predictive | Bayesian Statistics notes from summer 2021</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 8 Predictive | Bayesian Statistics notes from summer 2021" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 8 Predictive | Bayesian Statistics notes from summer 2021" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Chisom Onyishi" />


<meta name="date" content="2021-05-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="monte-carlo.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Belief and Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i><b>1.1</b> Example:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#binomial"><i class="fa fa-check"></i><b>1.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.4</b> Poisson distribution</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#example-normalmu-10.75-sigma0.8"><i class="fa fa-check"></i><b>1.5.1</b> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exchangeability.html"><a href="exchangeability.html"><i class="fa fa-check"></i><b>2</b> Exchangeability</a><ul>
<li class="chapter" data-level="2.1" data-path="exchangeability.html"><a href="exchangeability.html#discrete-joint-distributions"><i class="fa fa-check"></i><b>2.1</b> Discrete joint distributions</a></li>
<li class="chapter" data-level="2.2" data-path="exchangeability.html"><a href="exchangeability.html#bayesrule"><i class="fa fa-check"></i><b>2.2</b> Bayes’ rule and parameter estimation</a></li>
<li class="chapter" data-level="2.3" data-path="exchangeability.html"><a href="exchangeability.html#independent-random-variables"><i class="fa fa-check"></i><b>2.3</b> Independent random variables</a></li>
<li class="chapter" data-level="2.4" data-path="exchangeability.html"><a href="exchangeability.html#exchangeability-1"><i class="fa fa-check"></i><b>2.4</b> Exchangeability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binomial-1.html"><a href="binomial-1.html"><i class="fa fa-check"></i><b>3</b> Binomial</a><ul>
<li class="chapter" data-level="3.1" data-path="binomial-1.html"><a href="binomial-1.html#example---exchangeable-binary-data"><i class="fa fa-check"></i><b>3.1</b> Example - Exchangeable binary data</a></li>
<li class="chapter" data-level="3.2" data-path="binomial-1.html"><a href="binomial-1.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.2</b> The beta distribution</a><ul>
<li class="chapter" data-level="3.2.1" data-path="binomial-1.html"><a href="binomial-1.html#properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Properties of the beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="binomial-1.html"><a href="binomial-1.html#binomial-distribution"><i class="fa fa-check"></i><b>3.3</b> Binomial distribution</a><ul>
<li class="chapter" data-level="3.3.1" data-path="binomial-1.html"><a href="binomial-1.html#posterior-inference-for-a-binomial-sampling-model"><i class="fa fa-check"></i><b>3.3.1</b> Posterior inference for a binomial sampling model</a></li>
<li class="chapter" data-level="3.3.2" data-path="binomial-1.html"><a href="binomial-1.html#conjugacy"><i class="fa fa-check"></i><b>3.3.2</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="binomial-1.html"><a href="binomial-1.html#combining-information"><i class="fa fa-check"></i><b>3.4</b> Combining information</a><ul>
<li class="chapter" data-level="3.4.1" data-path="binomial-1.html"><a href="binomial-1.html#example-1"><i class="fa fa-check"></i><b>3.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="binomial-1.html"><a href="binomial-1.html#prediction"><i class="fa fa-check"></i><b>3.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>4</b> Applications</a><ul>
<li class="chapter" data-level="4.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>4.1</b> Example one</a></li>
<li class="chapter" data-level="4.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>4.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="5.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>5.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.2" data-path="CI.html"><a href="CI.html#how-do-we-compute-intervals"><i class="fa fa-check"></i><b>5.2</b> How do we compute intervals?</a><ul>
<li class="chapter" data-level="5.2.1" data-path="CI.html"><a href="CI.html#example-2"><i class="fa fa-check"></i><b>5.2.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>6</b> Poisson model</a><ul>
<li class="chapter" data-level="6.1" data-path="poisson-model.html"><a href="poisson-model.html#posterior-inference-for-the-poisson-model"><i class="fa fa-check"></i><b>6.1</b> Posterior inference for the Poisson model</a></li>
<li class="chapter" data-level="6.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>6.2</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="6.3" data-path="poisson-model.html"><a href="poisson-model.html#example-birth-rates"><i class="fa fa-check"></i><b>6.3</b> Example: Birth rates</a></li>
<li class="chapter" data-level="6.4" data-path="poisson-model.html"><a href="poisson-model.html#explaining-the-parameters-of-the-gamma-distribution"><i class="fa fa-check"></i><b>6.4</b> Explaining the parameters of the gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>7</b> Monte Carlo</a><ul>
<li class="chapter" data-level="7.1" data-path="monte-carlo.html"><a href="monte-carlo.html#example-birth-rate"><i class="fa fa-check"></i><b>7.1</b> Example: birth rate</a></li>
<li class="chapter" data-level="7.2" data-path="monte-carlo.html"><a href="monte-carlo.html#the-monte-carlo-method"><i class="fa fa-check"></i><b>7.2</b> The Monte Carlo method</a></li>
<li class="chapter" data-level="7.3" data-path="monte-carlo.html"><a href="monte-carlo.html#example-numerical-evaluation"><i class="fa fa-check"></i><b>7.3</b> Example: Numerical evaluation</a></li>
<li class="chapter" data-level="7.4" data-path="monte-carlo.html"><a href="monte-carlo.html#posterior-inference-for-arbitrary-functions"><i class="fa fa-check"></i><b>7.4</b> Posterior inference for arbitrary functions</a></li>
<li class="chapter" data-level="7.5" data-path="monte-carlo.html"><a href="monte-carlo.html#example-log-odds"><i class="fa fa-check"></i><b>7.5</b> Example: Log-odds</a></li>
<li class="chapter" data-level="7.6" data-path="monte-carlo.html"><a href="monte-carlo.html#example-functions-of-two-parameters"><i class="fa fa-check"></i><b>7.6</b> Example: Functions of two parameters</a></li>
<li class="chapter" data-level="7.7" data-path="monte-carlo.html"><a href="monte-carlo.html#how-many-monte-carlo-samples-are-needed"><i class="fa fa-check"></i><b>7.7</b> How many Monte Carlo samples are needed?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="predictive.html"><a href="predictive.html"><i class="fa fa-check"></i><b>8</b> Predictive</a><ul>
<li class="chapter" data-level="8.1" data-path="predictive.html"><a href="predictive.html#sampling-for-predictive-distribution"><i class="fa fa-check"></i><b>8.1</b> Sampling for predictive distribution</a></li>
<li class="chapter" data-level="8.2" data-path="predictive.html"><a href="predictive.html#example-let-d-tilde-y_1---tilde-y_2"><i class="fa fa-check"></i><b>8.2</b> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></a></li>
<li class="chapter" data-level="8.3" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>8.3</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="8.4" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking-for-group"><i class="fa fa-check"></i><b>8.4</b> Posterior predictive model checking for group</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics notes from summer 2021</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="predictive" class="section level1">
<h1><span class="header-section-number">Lecture 8</span> Predictive</h1>
<p><tt>The following notes are transcribed from Neath(0511,2021) lecture which summarizes Sections(4.3 and 4.4) of Hoff(2009). </tt></p>
<p>
 
</p>
<p>Last class, we talked about using simulated draws <span class="math inline">\(\theta^{(s)} \sim p(\theta | y)\)</span> to form a Monte Carlo approximation to the posterior distribution <span class="math inline">\(p(\theta | y)\)</span>.</p>
<p>Today we do Monte Carlo simulation for the posterior predictive dist!. A predictive dist is characterized by two features: (1) known quantities are conditioned on (2) unknown quantities are integrated out. For example, if we integrate <span class="math inline">\(\theta\)</span> out of this <span class="math inline">\(\int{ p(\tilde y | \theta) p(\theta ) d\theta } = p(\tilde y)\)</span>, we call this a prior predictive distribution. <strong>A result of probability theory:</strong> If the sampling model is Poisson<span class="math inline">\((\theta)\)</span> and the prior is <span class="math inline">\(\theta \sim \text{gamma}(a, b)\)</span> then the prior predictive is NegBinom<span class="math inline">\((a, b)\)</span>; an overdispersed count dist. The Poisson dist has the property that Var<span class="math inline">\((Y) = E(Y)\)</span>. For a NegBinom Var<span class="math inline">\((Y) &gt; E(Y)\)</span>. A predictive dist that accounts for the information available in the sample data <span class="math inline">\((y_1, …, y_n) = y\)</span> is called a posterior predictive dist and in the Poisson-gamma model, because the gamma is a conjugate for the poisson and the posterior on <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\text{gamma}(a+ \sum{y_i},b + n)\)</span> the posterior predictive dist(PPD) for an <span class="math inline">\((n+1)\)</span>st observation is NegBinom<span class="math inline">\((a + \sum{y_i}, b + n)\)</span>. R has <code>dnbinom, pnbinom, rnbinom</code> functions. So we don’t need to do Monte Carlo for the PPD any more than we need to for approximating the posterior of <span class="math inline">\(\theta\)</span> but we’re gonna anyway because it’s educational.</p>
<p><em>Note:</em> When I say “Poisson-gamma model” or I may say “gamma-Poisson model” that’s a shorthand for “The Bayesian statistical model where our data consists of exchangeable observations of conditionally independent Poissons with rate <span class="math inline">\(\theta\)</span> where the prior dist on <span class="math inline">\(\theta\)</span> is gamma<span class="math inline">\((a, b)\)</span>”</p>
<div id="sampling-for-predictive-distribution" class="section level2">
<h2><span class="header-section-number">8.1</span> Sampling for predictive distribution</h2>
<p>Often, not only does the PPD not have a nice closed form but it’s also not even so straightforward to sample from. But we can still do Monte Carlo for predictive dists!</p>
<p><strong>Proposition:</strong> If <span class="math inline">\(\theta^{(s)} \sim p(\theta | y)\)</span> and <span class="math inline">\(y^{(s)} \sim p(y | \theta^{(s)})\)</span> then I should say <span class="math inline">\(\tilde y^{(s)} \sim p(\tilde y | \theta^{(s)})\)</span> then jointly <span class="math inline">\((\theta^{(s)}, \tilde y^{(s)}) \sim p(\theta | y) \times p(\tilde y | \theta) = p(\theta, \tilde y | y)\)</span> the joint dist of <span class="math inline">\(\theta \text{ and } \tilde Y\)</span>. In general if <span class="math inline">\(X_1, X_2 \sim f(x_1, x_2)\)</span> then <span class="math inline">\(X_2\)</span> is a draw from the marginal of <span class="math inline">\(X_2.\)</span></p>
<p>Let’s return to the birthrates example. Our posterior for <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> were independent <span class="math inline">\(\theta_1 | y \sim\)</span> gamma(219, 112) <span class="math inline">\(\theta_2 | y \sim\)</span> gamma(68, 45). Suppose we want the posterior predictive prob <span class="math inline">\(Pr(\tilde Y_1 &gt; \tilde Y_2 |\)</span> data) <span class="math inline">\(\tilde Y_1 =\)</span> number of children for a randomly selected 40-year-old woman with less than bachelor’s. <span class="math inline">\(\tilde Y_2 =\)</span> number of children for a randomly selected woman with bachelor’s or higher.</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{Pr}\left(\tilde{Y}_{1}\right.&amp;\left.&gt;\tilde{Y}_{2} \mid\left\{y_{i,j}\right\}\right)=\sum_{\tilde y_{1}=0}^{\infty} \sum_{\tilde y_{2}=0}^{\tilde y_{1}-1} \operatorname{Pr}\left(\tilde{Y}_{1}=y_{1}, \tilde{Y}_{2}=y_{2} \mid\left\{y_{i,j}\right\}\right) \\
&amp;=\sum_{\tilde y_{1}=0}^{\infty} \texttt {pnbinom}\left(\tilde y_{1}-1 \mid 68,45\right) \times \texttt {dnbinom}\left(\tilde y_{1} \mid 219,112\right)
\end{aligned}
\]</span></p>
<p>If I did <code>pnbinom</code><span class="math inline">\((y_1)\)</span> in the last line that’d be right for <span class="math inline">\(\tilde Y_2 \le \tilde Y_1.\)</span> I want <span class="math inline">\(\tilde Y_2 &lt; \tilde Y_1.\)</span></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="predictive.html#cb47-1"></a>y1.tilde &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">100</span></span>
<span id="cb47-2"><a href="predictive.html#cb47-2"></a><span class="kw">sum</span>( <span class="kw">pnbinom</span>(y1.tilde<span class="dv">-1</span>, <span class="dt">size=</span><span class="dv">68</span>, <span class="dt">mu=</span><span class="dv">68</span><span class="op">/</span><span class="dv">45</span>)<span class="op">*</span></span>
<span id="cb47-3"><a href="predictive.html#cb47-3"></a><span class="st">       </span><span class="kw">dnbinom</span>(y1.tilde, <span class="dt">size=</span><span class="dv">219</span>, <span class="dt">mu=</span><span class="dv">219</span><span class="op">/</span><span class="dv">112</span>) )</span></code></pre></div>
<pre><code>## [1] 0.4820895</code></pre>
<p>wait a second weren’t we thinking number of children for no college &lt; number of children for college grads shouldn’t this prob be &gt; 0.5? Not really. What is <span class="math inline">\(Pr(\tilde Y_1 &lt; \tilde Y_2)?\)</span> It will also be less than 0.5 and in fact will be less than .48 (0.3 to be exact). Remember we’re talking about a discrete dist. Randomly select a women with less than bachelor’s degree and a woman with bachelor’s or higher there’s a decent chance they have the SAME number of kids.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="predictive.html#cb49-1"></a><span class="co"># Pr(Y1 tilde &gt;= Y2 tilde )</span></span>
<span id="cb49-2"><a href="predictive.html#cb49-2"></a><span class="kw">sum</span>( <span class="kw">pnbinom</span>(y1.tilde, <span class="dt">size=</span><span class="dv">68</span>,  <span class="dt">mu=</span><span class="dv">68</span><span class="op">/</span><span class="dv">45</span>)<span class="op">*</span></span>
<span id="cb49-3"><a href="predictive.html#cb49-3"></a><span class="st">     </span><span class="kw">dnbinom</span>(y1.tilde, <span class="dt">size=</span><span class="dv">219</span>, <span class="dt">mu=</span><span class="dv">219</span><span class="op">/</span><span class="dv">112</span>) )</span></code></pre></div>
<pre><code>## [1] 0.6997647</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="predictive.html#cb51-1"></a><span class="kw">plot</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">10</span>, <span class="kw">pnbinom</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">size =</span> <span class="dv">219</span>, <span class="dt">mu =</span> <span class="dv">219</span><span class="op">/</span><span class="dv">212</span>))</span>
<span id="cb51-2"><a href="predictive.html#cb51-2"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/pnbinom%20plot-1.png" width="40%" height="40%" style="display: block; margin: auto;" /></p>
<p>Here we could do Monte Carlo simulations using the <code>rnbinom</code> function. Or we could do <code>rgamma</code> and <code>rpois</code>. They are both correct the longer way generalizes to more complicated problems where the PPD may not be tractable.</p>
<ul>
<li><code>rgamma</code>(3, 1, 1), I get 3 draws from a gamma(1,1) dist.</li>
<li><code>rpois</code>(10, 5), I get 10 draws from a Poisson(5)</li>
<li><code>rpois</code>(10, 6:15), I get one Poisson(6) draw one Poisson(7) draw one Poisson(8) … one Poisson(15) draw</li>
</ul>
<p>These <span class="math inline">\(\tilde y\)</span>’s are Poisson variables but each with a different mean and since the Poisson means are gamma-distributed the <span class="math inline">\(\tilde y\)</span> will be negative binomially distributed.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="predictive.html#cb52-1"></a><span class="co"># Birth rates example # Posterior predictive simulations </span></span>
<span id="cb52-2"><a href="predictive.html#cb52-2"></a>a  &lt;-<span class="st"> </span><span class="dv">2</span>  ;  b     &lt;-<span class="st"> </span><span class="dv">1</span>;  </span>
<span id="cb52-3"><a href="predictive.html#cb52-3"></a>n1 &lt;-<span class="st"> </span><span class="dv">111</span>; sum.y1 &lt;-<span class="st"> </span><span class="dv">217</span>;</span>
<span id="cb52-4"><a href="predictive.html#cb52-4"></a>n2 &lt;-<span class="st"> </span><span class="dv">44</span> ; sum.y2 &lt;-<span class="st"> </span><span class="dv">66</span>;</span>
<span id="cb52-5"><a href="predictive.html#cb52-5"></a></span>
<span id="cb52-6"><a href="predictive.html#cb52-6"></a>theta1.sim &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">10000</span>, a<span class="op">+</span>sum.y1, b<span class="op">+</span>n1)</span>
<span id="cb52-7"><a href="predictive.html#cb52-7"></a>theta2.sim &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">10000</span>, a<span class="op">+</span>sum.y2, b<span class="op">+</span>n2)</span>
<span id="cb52-8"><a href="predictive.html#cb52-8"></a>y1.tilde   &lt;-<span class="st"> </span><span class="kw">rpois</span>(<span class="dv">10000</span>, theta1.sim)</span>
<span id="cb52-9"><a href="predictive.html#cb52-9"></a>y2.tilde   &lt;-<span class="st"> </span><span class="kw">rpois</span>(<span class="dv">10000</span>, theta2.sim)</span></code></pre></div>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="predictive.html#cb53-1"></a><span class="kw">c</span>(<span class="kw">mean</span>(y1.tilde <span class="op">==</span><span class="st"> </span>y2.tilde), <span class="kw">mean</span>(y1.tilde <span class="op">&gt;</span><span class="st"> </span>y2.tilde))</span></code></pre></div>
<pre><code>## [1] 0.2254 0.4798</code></pre>
<p>So indeed these conclusions are consistent with women without bachelors having more kids on average than women with bachelors.</p>
<p><strong>Here’s an important thing:</strong> Given a Monte Carlo sample from a prob dist, I can compute (approximations to / estimates of) any feature of that dist that interests me. E.g., moments (means, variance, etc) probabilities, quantiles. That holds for posterior predictive dists as well as posterior dists.</p>
</div>
<div id="example-let-d-tilde-y_1---tilde-y_2" class="section level2">
<h2><span class="header-section-number">8.2</span> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></h2>
<p>What does <span class="math inline">\(D\)</span> mean exactly? It means this: Randomly select a 40-year-old woman with less than bachelor’s and a 40-year-old woman with bachelors or higher <span class="math inline">\(D =\)</span> number of kids first woman has minus number of kids second woman has. It’s a difference between two negative binomial dist(indep NegBinom’s). I don’t know anything about the properties of such a distribution (it doesn’t matter, I don’t need to know). We can use our samples of <span class="math inline">\(\tilde y_1 \text{ and } \tilde y_2\)</span> to construct the predictive (posterior predictive dist) of this difference <span class="math inline">\(D\)</span>!</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="predictive.html#cb55-1"></a><span class="co"># Consider D = Y1.tilde - Y2.tilde</span></span>
<span id="cb55-2"><a href="predictive.html#cb55-2"></a>D.tilde &lt;-<span class="st"> </span>y1.tilde <span class="op">-</span><span class="st"> </span>y2.tilde;  <span class="kw">range</span>(D.tilde);</span></code></pre></div>
<pre><code>## [1] -7  9</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="predictive.html#cb57-1"></a>D.vals  &lt;-<span class="st"> </span><span class="kw">range</span>(D.tilde)[<span class="dv">1</span>]<span class="op">:</span><span class="kw">range</span>(D.tilde)[<span class="dv">2</span>]; D.vals;</span></code></pre></div>
<pre><code>##  [1] -7 -6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6  7  8  9</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="predictive.html#cb59-1"></a>ppd.D   &lt;-<span class="st"> </span>(<span class="kw">table</span>(<span class="kw">c</span>(D.tilde, D.vals))<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(D.tilde)</span>
<span id="cb59-2"><a href="predictive.html#cb59-2"></a></span>
<span id="cb59-3"><a href="predictive.html#cb59-3"></a><span class="kw">plot</span>(D.vals, <span class="kw">as.vector</span>(ppd.D), <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, </span>
<span id="cb59-4"><a href="predictive.html#cb59-4"></a> <span class="dt">xlab=</span><span class="st">&quot;D=y1.tilde-y2.tilde&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;p(D|y1,y2)&quot;</span>)</span>
<span id="cb59-5"><a href="predictive.html#cb59-5"></a><span class="kw">points</span>(D.vals, <span class="kw">as.vector</span>(ppd.D), <span class="dt">pch=</span><span class="dv">19</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/Y1.tilde%20-%20Y2.tilde-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This weird business with the <code>(table(c(D.tilde, D.vals))-1)</code> is there because the table function doesn’t count missing values.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="predictive.html#cb60-1"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>, <span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>, <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">8</span>);<span class="kw">table</span>(x)</span></code></pre></div>
<pre><code>## x
## 1 2 3 5 6 8 
## 3 3 3 1 1 1</code></pre>
<p>If we are making a histogram we would want it to show that we have zero fours.. hence the weird business.</p>
<p>The most frequently occurring value is 0 the next most frequent is +1. The non-college-grad has 1 more kid than the college grad. This plot would have been hard to do any way other than by Monte Carlo.</p>
</div>
<div id="posterior-predictive-model-checking" class="section level2">
<h2><span class="header-section-number">8.3</span> Posterior predictive model checking</h2>
<p>What we’ve done so far is assumed number of kids | <span class="math inline">\(\theta\)</span> are indep Poisson.</p>
<p>Here’s a question: Number of kids is not a Poisson variable. The Poisson dist counts events in a Poisson process which occur randomly and independently of each other. That is not how people have kids at all. <em>Why did we use it?</em> The answer is: because number of kids is a count variable and the Poisson dist is a simple model for count variables. You know the most famous quote in statistics? “All models are wrong, some models are useful”.</p>
<p>The question is: Is it useful even though it’s wrong? Posterior predictive simulations are a good tool for addressing this question.</p>
<p>Student question: I thought that Poisson was a good distribution for rare events..?
Ans: It is. But not only rare events <span class="math inline">\(Y \sim \text{Poisson}(\theta).\)</span> No rule against <span class="math inline">\(\theta\)</span> being a big number <span class="math inline">\(\theta =\)</span> expected count “rare events” means prob is low but if exposure is high then expected number of counts can high as well.</p>
<p>Additional notes:</p>
<p>Sample quantiles approximate true posterior quantiles. For example, <span class="math inline">\(S = 1000\)</span> Define <span class="math inline">\(\theta^{(1)} \le \theta^{(2)} \le … \le \theta^{(S)}\)</span>. Take the 25th one and the 975th one and that’s an approximate 95% posterior interval.</p>
</div>
<div id="posterior-predictive-model-checking-for-group" class="section level2">
<h2><span class="header-section-number">8.4</span> Posterior predictive model checking for group</h2>
<p>Let’s take the first group for illustration(less than bachelor’s degree). This is actually the first time we’ve seen these data cuz all we needed
was sufficient statistic <span class="math inline">\((n = 111,~ \sum{y_i} = 217)\)</span>. Take each of these counts divided by 111 that defines the empirical distribution. The idea of “posterior predictive model checking” is; <em>if the model is “correct” then the observed data should not appear unusual when compared to the posterior predictive distribution.</em> <span class="math inline">\(p(\tilde y | y)\)</span> is the conditional dist of number of chidren conditional on the observed data. If the data really were drawn from the model that we used the empirical dist and the PPD should show close agreement.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="predictive.html#cb62-1"></a><span class="co"># Posterior predictive model checking for group 1 </span></span>
<span id="cb62-2"><a href="predictive.html#cb62-2"></a>y      &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb62-3"><a href="predictive.html#cb62-3"></a>e.dist &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">19</span>,<span class="dv">38</span>,<span class="dv">20</span>,<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">length</span>(y)<span class="op">-</span><span class="dv">7</span>)) <span class="op">/</span><span class="st"> </span><span class="dv">111</span></span>
<span id="cb62-4"><a href="predictive.html#cb62-4"></a><span class="kw">plot</span>(y<span class="fl">-.05</span>, e.dist, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lwd=</span><span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&quot;y&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Pr(Y=y)&quot;</span>)</span>
<span id="cb62-5"><a href="predictive.html#cb62-5"></a></span>
<span id="cb62-6"><a href="predictive.html#cb62-6"></a>a &lt;-<span class="st"> </span><span class="dv">2</span>;  b &lt;-<span class="st"> </span><span class="dv">1</span>;  n &lt;-<span class="st"> </span><span class="dv">111</span>; sum.y &lt;-<span class="st"> </span><span class="dv">217</span>;</span>
<span id="cb62-7"><a href="predictive.html#cb62-7"></a>p.dist &lt;-<span class="st"> </span><span class="kw">dnbinom</span>(y, <span class="dt">size=</span>a<span class="op">+</span>sum.y, <span class="dt">mu=</span>(a<span class="op">+</span>sum.y)<span class="op">/</span>(b<span class="op">+</span>n))</span>
<span id="cb62-8"><a href="predictive.html#cb62-8"></a><span class="kw">points</span>(y<span class="fl">+.05</span>, p.dist, <span class="dt">type=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lwd=</span><span class="dv">5</span>, <span class="dt">col=</span><span class="st">&quot;gray&quot;</span>)</span>
<span id="cb62-9"><a href="predictive.html#cb62-9"></a><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span>.<span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">5</span>, <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;gray&quot;</span>),</span>
<span id="cb62-10"><a href="predictive.html#cb62-10"></a>  <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Empirical dist&quot;</span>, <span class="st">&quot;Predictive dist&quot;</span>))</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-32-1.png" width="672" style="display: block; margin: auto;" />
Should we be concerned about this? First, is it really a disagreement? There are two possible answers:</p>
<ul>
<li>It is a result of sampling variability.</li>
<li>It is indeed a feature of the populations. Our model is wrong.</li>
</ul>
<p>The answer is based on frequentist hypothesis testing. We will simulate <span class="math inline">\(S\)</span> replicated data sets (Remember <span class="math inline">\(S\)</span> is a big number). For each <span class="math inline">\(\theta^{(s)} \sim p(\theta | y).\)</span> And for each replicated data set compute the test statistic. What test statistic to use? Whatever feature of the data you supsect the model is not capturing correctly. In this case we use <span class="math inline">\(t(y_{obs}) = 38 / 19 = 2.0\)</span>, What does the posterior predictive dist of this test statistic look like? and how unusual a value is 2.0? For each sample from the posterior, <span class="math inline">\(\theta^{(s)} \sim p(\theta | y)\)</span> simulate a “replicated” data set from <span class="math inline">\(p(y | \theta^{(s)}) =\)</span></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="predictive.html#cb63-1"></a><span class="co"># Define the test statistic t = #{y_i = 2} / #{y_i = 1}</span></span>
<span id="cb63-2"><a href="predictive.html#cb63-2"></a>t.sim &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="dv">10000</span>)</span>
<span id="cb63-3"><a href="predictive.html#cb63-3"></a><span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>)</span>
<span id="cb63-4"><a href="predictive.html#cb63-4"></a>{</span>
<span id="cb63-5"><a href="predictive.html#cb63-5"></a> theta.sim &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>, a<span class="op">+</span>sum.y, b<span class="op">+</span>n)</span>
<span id="cb63-6"><a href="predictive.html#cb63-6"></a> y.tilde   &lt;-<span class="st"> </span><span class="kw">rpois</span>(n, theta.sim)</span>
<span id="cb63-7"><a href="predictive.html#cb63-7"></a> t.sim[s]  &lt;-<span class="st"> </span><span class="kw">sum</span>(y.tilde<span class="op">==</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(y.tilde<span class="op">==</span><span class="dv">1</span>)</span>
<span id="cb63-8"><a href="predictive.html#cb63-8"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="predictive.html#cb64-1"></a><span class="kw">hist</span>(t.sim, <span class="dt">freq=</span>F, <span class="dt">right=</span>F, <span class="dt">breaks=</span><span class="dv">30</span>, <span class="st">&quot;&quot;</span>)</span>
<span id="cb64-2"><a href="predictive.html#cb64-2"></a><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;" />
How often in these replicated(simulated) data sets does it happen that there are double the number of cases with <span class="math inline">\(y=2\)</span> as with <span class="math inline">\(y=1?\)</span> The answer is not very often. The observed test statistic <span class="math inline">\(t(y_{obs}) = 2.0\)</span>
is kind of out in the tail of this dist.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="predictive.html#cb65-1"></a><span class="kw">mean</span>(t.sim <span class="op">&gt;=</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.0059</code></pre>
<p>This quantity 0.0051 that’s the tail probability. This value is called a “Bayesian p-value”. This “Bayesian p-value” has nothing to do with Bayesian inference it’s strictly a tool in Bayesian model checking.</p>
<p>We did the posterior predictive checks we found Bayesian p-value = .005 (close to zero) Based on this which bullet is right one? It’s the second one! If this was to be expected due to sampling variability then it would have occurred with some frequency among the replicated data sets. But it didn’t! So this feature of the data “<span class="math inline">\(y=2\)</span> with much greater frequency than <span class="math inline">\(y=1\)</span>” is not explained by the model. So the conclusion is; This is indeed a deficiency of the Poisson model; To the extent that there our features of our data set that are not shared by the replicated data sets this MAY suggest a problem.</p>
<p>We’re still back to the original question. How concerned should we be? Is predicting frequency of one kid and two kids an important goal of our inference? If yes, then we’ve got a problem because the Poisson model is not going to give accurate predictions here. It will underpredict the frequency of <span class="math inline">\(y=2\)</span> and overpredict the frequency of <span class="math inline">\(y=1\)</span>. However if what we really care about is estimation of mean and variance of “# of kids” then this is not such a problem.</p>
<p>So in terms of what’s the next step in this data analysis? The answer is: it depends.</p>
<p>Thus endeth our discussion of Chapter 4. On to the next thing Chapter 5.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="monte-carlo.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesianS21.pdf", "bayesianS21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
