<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 7 Predictive | Bayesian Statistics lecture notes</title>
  <meta name="description" content="Lecture 7 Predictive | Bayesian Statistics lecture notes" />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 7 Predictive | Bayesian Statistics lecture notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 7 Predictive | Bayesian Statistics lecture notes" />
  
  
  

<meta name="author" content="Chisom Onyishi" />


<meta name="date" content="2021-06-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="monte-carlo.html"/>
<link rel="next" href="normal-mean.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Belief and Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i><b>1.1</b> Example:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#binomial"><i class="fa fa-check"></i><b>1.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.4</b> Poisson distribution</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#example-normalmu-10.75-sigma0.8"><i class="fa fa-check"></i><b>1.5.1</b> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exchangeability.html"><a href="exchangeability.html"><i class="fa fa-check"></i><b>2</b> Exchangeability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exchangeability.html"><a href="exchangeability.html#discrete-joint-distributions"><i class="fa fa-check"></i><b>2.1</b> Discrete joint distributions</a></li>
<li class="chapter" data-level="2.2" data-path="exchangeability.html"><a href="exchangeability.html#bayesrule"><i class="fa fa-check"></i><b>2.2</b> Bayes’ rule and parameter estimation</a></li>
<li class="chapter" data-level="2.3" data-path="exchangeability.html"><a href="exchangeability.html#independent-random-variables"><i class="fa fa-check"></i><b>2.3</b> Independent random variables</a></li>
<li class="chapter" data-level="2.4" data-path="exchangeability.html"><a href="exchangeability.html#exchangeability-1"><i class="fa fa-check"></i><b>2.4</b> Exchangeability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binomial-1.html"><a href="binomial-1.html"><i class="fa fa-check"></i><b>3</b> Binomial</a>
<ul>
<li class="chapter" data-level="3.1" data-path="binomial-1.html"><a href="binomial-1.html#example---exchangeable-binary-data"><i class="fa fa-check"></i><b>3.1</b> Example - Exchangeable binary data</a></li>
<li class="chapter" data-level="3.2" data-path="binomial-1.html"><a href="binomial-1.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.2</b> The beta distribution</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="binomial-1.html"><a href="binomial-1.html#properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Properties of the beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="binomial-1.html"><a href="binomial-1.html#binomial-distribution"><i class="fa fa-check"></i><b>3.3</b> Binomial distribution</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="binomial-1.html"><a href="binomial-1.html#posterior-inference-for-a-binomial-sampling-model"><i class="fa fa-check"></i><b>3.3.1</b> Posterior inference for a binomial sampling model</a></li>
<li class="chapter" data-level="3.3.2" data-path="binomial-1.html"><a href="binomial-1.html#conjugacy"><i class="fa fa-check"></i><b>3.3.2</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="binomial-1.html"><a href="binomial-1.html#combining-information"><i class="fa fa-check"></i><b>3.4</b> Combining information</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="binomial-1.html"><a href="binomial-1.html#example-1"><i class="fa fa-check"></i><b>3.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="binomial-1.html"><a href="binomial-1.html#prediction"><i class="fa fa-check"></i><b>3.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#how-do-we-compute-intervals"><i class="fa fa-check"></i><b>4.2</b> How do we compute intervals?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="CI.html"><a href="CI.html#example-2"><i class="fa fa-check"></i><b>4.2.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>5</b> Poisson model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="poisson-model.html"><a href="poisson-model.html#posterior-inference-for-the-poisson-model"><i class="fa fa-check"></i><b>5.1</b> Posterior inference for the Poisson model</a></li>
<li class="chapter" data-level="5.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>5.2</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="5.3" data-path="poisson-model.html"><a href="poisson-model.html#example-birth-rates"><i class="fa fa-check"></i><b>5.3</b> Example: Birth rates</a></li>
<li class="chapter" data-level="5.4" data-path="poisson-model.html"><a href="poisson-model.html#explaining-the-parameters-of-the-gamma-distribution"><i class="fa fa-check"></i><b>5.4</b> Explaining the parameters of the gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo</a>
<ul>
<li class="chapter" data-level="6.1" data-path="monte-carlo.html"><a href="monte-carlo.html#example-birth-rate"><i class="fa fa-check"></i><b>6.1</b> Example: birth rate</a></li>
<li class="chapter" data-level="6.2" data-path="monte-carlo.html"><a href="monte-carlo.html#the-monte-carlo-method"><i class="fa fa-check"></i><b>6.2</b> The Monte Carlo method</a></li>
<li class="chapter" data-level="6.3" data-path="monte-carlo.html"><a href="monte-carlo.html#example-numerical-evaluation"><i class="fa fa-check"></i><b>6.3</b> Example: Numerical evaluation</a></li>
<li class="chapter" data-level="6.4" data-path="monte-carlo.html"><a href="monte-carlo.html#posterior-inference-for-arbitrary-functions"><i class="fa fa-check"></i><b>6.4</b> Posterior inference for arbitrary functions</a></li>
<li class="chapter" data-level="6.5" data-path="monte-carlo.html"><a href="monte-carlo.html#example-log-odds"><i class="fa fa-check"></i><b>6.5</b> Example: Log-odds</a></li>
<li class="chapter" data-level="6.6" data-path="monte-carlo.html"><a href="monte-carlo.html#example-functions-of-two-parameters"><i class="fa fa-check"></i><b>6.6</b> Example: Functions of two parameters</a></li>
<li class="chapter" data-level="6.7" data-path="monte-carlo.html"><a href="monte-carlo.html#how-many-monte-carlo-samples-are-needed"><i class="fa fa-check"></i><b>6.7</b> How many Monte Carlo samples are needed?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predictive.html"><a href="predictive.html"><i class="fa fa-check"></i><b>7</b> Predictive</a>
<ul>
<li class="chapter" data-level="7.1" data-path="predictive.html"><a href="predictive.html#sampling-for-predictive-distribution"><i class="fa fa-check"></i><b>7.1</b> Sampling for predictive distribution</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="predictive.html"><a href="predictive.html#example-birth-rate-1"><i class="fa fa-check"></i><b>7.1.1</b> Example: birth rate</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="predictive.html"><a href="predictive.html#example-let-d-tilde-y_1---tilde-y_2"><i class="fa fa-check"></i><b>7.2</b> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>7.3</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="7.4" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking-1"><i class="fa fa-check"></i><b>7.4</b> Posterior predictive model checking</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normal-mean.html"><a href="normal-mean.html"><i class="fa fa-check"></i><b>8</b> Normal Mean</a>
<ul>
<li class="chapter" data-level="8.1" data-path="normal-mean.html"><a href="normal-mean.html#example-womens-height"><i class="fa fa-check"></i><b>8.1</b> Example: women’s height</a></li>
<li class="chapter" data-level="8.2" data-path="normal-mean.html"><a href="normal-mean.html#inference-for-the-mean-conditional-on-the-variance"><i class="fa fa-check"></i><b>8.2</b> Inference for the mean, conditional on the variance</a></li>
<li class="chapter" data-level="8.3" data-path="normal-mean.html"><a href="normal-mean.html#prediction-1"><i class="fa fa-check"></i><b>8.3</b> Prediction</a></li>
<li class="chapter" data-level="8.4" data-path="normal-mean.html"><a href="normal-mean.html#example-midge-wing-length"><i class="fa fa-check"></i><b>8.4</b> Example: Midge wing length</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html"><i class="fa fa-check"></i><b>9</b> Joint inference for Normal mean and variance</a>
<ul>
<li class="chapter" data-level="9.1" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#marginal-posterior-of-sigma2"><i class="fa fa-check"></i><b>9.1</b> Marginal posterior of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#example-midge-wing-length-1"><i class="fa fa-check"></i><b>9.2</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="9.3" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>9.3</b> Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.4" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#summary-of-normal-formulas"><i class="fa fa-check"></i><b>9.4</b> Summary of Normal formulas</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html"><i class="fa fa-check"></i><b>10</b> Gibbs sampler</a>
<ul>
<li class="chapter" data-level="10.1" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#review-of-conjugate-prior-for-normal-model"><i class="fa fa-check"></i><b>10.1</b> Review of conjugate prior for normal model</a></li>
<li class="chapter" data-level="10.2" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#a-semiconjugate-prior-distribution"><i class="fa fa-check"></i><b>10.2</b> A semiconjugate prior distribution</a></li>
<li class="chapter" data-level="10.3" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.3</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.4" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-midge-wing-length-2"><i class="fa fa-check"></i><b>10.4</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="10.5" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#discrete-approximation-of-posterior-distribution"><i class="fa fa-check"></i><b>10.5</b> Discrete approximation of posterior distribution</a></li>
<li class="chapter" data-level="10.6" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-3"><i class="fa fa-check"></i><b>10.6</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>11</b> MCMC diagnostics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#the-gibbs-sampler"><i class="fa fa-check"></i><b>11.1</b> The Gibbs sampler</a></li>
<li class="chapter" data-level="11.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#distinguishing-estimation-from-approximation"><i class="fa fa-check"></i><b>11.2</b> Distinguishing estimation from approximation</a></li>
<li class="chapter" data-level="11.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#introduction-to-mcmc-diagnostics"><i class="fa fa-check"></i><b>11.3</b> Introduction to MCMC diagnostics</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#example-mixture-of-normal-densities"><i class="fa fa-check"></i><b>11.3.1</b> Example: mixture of normal densities</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#discussion"><i class="fa fa-check"></i><b>11.4</b> Discussion</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#how-does-autocorrelationslow-mixing-affect-our-mcmc-approximation"><i class="fa fa-check"></i><b>11.4.1</b> How does autocorrelation(slow mixing) affect our MCMC approximation?</a></li>
<li class="chapter" data-level="11.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation"><i class="fa fa-check"></i><b>11.4.2</b> Autocorrelation</a></li>
<li class="chapter" data-level="11.4.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#sample-autocorrelation-function"><i class="fa fa-check"></i><b>11.4.3</b> Sample autocorrelation function</a></li>
<li class="chapter" data-level="11.4.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>11.4.4</b> Effective sample size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multivariate-normal.html"><a href="multivariate-normal.html"><i class="fa fa-check"></i><b>12</b> Multivariate Normal</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multivariate-normal.html"><a href="multivariate-normal.html#example-reading-comprehension"><i class="fa fa-check"></i><b>12.1</b> Example: Reading comprehension</a></li>
<li class="chapter" data-level="12.2" data-path="multivariate-normal.html"><a href="multivariate-normal.html#the-multivariate-normal-density"><i class="fa fa-check"></i><b>12.2</b> The multivariate normal density</a></li>
<li class="chapter" data-level="12.3" data-path="multivariate-normal.html"><a href="multivariate-normal.html#a-semiconjugate-prior-distribution-for-the-mean"><i class="fa fa-check"></i><b>12.3</b> A semiconjugate prior distribution for the mean</a></li>
<li class="chapter" data-level="12.4" data-path="multivariate-normal.html"><a href="multivariate-normal.html#the-inverse-wishart-distribution"><i class="fa fa-check"></i><b>12.4</b> The inverse-Wishart distribution</a></li>
<li class="chapter" data-level="12.5" data-path="multivariate-normal.html"><a href="multivariate-normal.html#full-conditional-distribution-of-the-covariance-matrix"><i class="fa fa-check"></i><b>12.5</b> Full conditional distribution of the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>13</b> Group comparisons</a>
<ul>
<li class="chapter" data-level="13.1" data-path="group-comparisons.html"><a href="group-comparisons.html#comparing-two-groups"><i class="fa fa-check"></i><b>13.1</b> Comparing two groups</a></li>
<li class="chapter" data-level="13.2" data-path="group-comparisons.html"><a href="group-comparisons.html#mathex1"><i class="fa fa-check"></i><b>13.2</b> Example: Math scores data</a></li>
<li class="chapter" data-level="13.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesmodel"><i class="fa fa-check"></i><b>13.3</b> A Bayesian model</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="group-comparisons.html"><a href="group-comparisons.html#analysis-of-the-math-scores-data"><i class="fa fa-check"></i><b>13.3.1</b> Analysis of the math scores data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html"><i class="fa fa-check"></i><b>14</b> The hierarchical normal model</a>
<ul>
<li class="chapter" data-level="14.1" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#postinf"><i class="fa fa-check"></i><b>14.1</b> Posterior inference</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-distributions-of-mu-and-tau2"><i class="fa fa-check"></i><b>14.1.1</b> Full conditional distributions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span></a></li>
<li class="chapter" data-level="14.1.2" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-of-theta_j"><i class="fa fa-check"></i><b>14.1.2</b> Full conditional of <span class="math inline">\(\theta_j\)</span></a></li>
<li class="chapter" data-level="14.1.3" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-of-sigma2"><i class="fa fa-check"></i><b>14.1.3</b> Full conditional of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#mathex2"><i class="fa fa-check"></i><b>14.2</b> Example: Math scores in U.S. public schools</a></li>
<li class="chapter" data-level="14.3" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#posterior-approximation"><i class="fa fa-check"></i><b>14.3</b> Posterior approximation</a></li>
<li class="chapter" data-level="14.4" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#mcmc-diagnostics-1"><i class="fa fa-check"></i><b>14.4</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="14.5" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#shrinkage"><i class="fa fa-check"></i><b>14.5</b> Shrinkage</a></li>
<li class="chapter" data-level="14.6" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#ranking-the-groups"><i class="fa fa-check"></i><b>14.6</b> Ranking the groups</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>15</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="linear-regression.html"><a href="linear-regression.html#example-oxygen-uptake"><i class="fa fa-check"></i><b>15.1</b> Example: Oxygen uptake</a></li>
<li class="chapter" data-level="15.2" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>15.2</b> Least squares estimation</a></li>
<li class="chapter" data-level="15.3" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimation-for-oxygen-uptake-data"><i class="fa fa-check"></i><b>15.3</b> Least squares estimation for oxygen uptake data</a></li>
<li class="chapter" data-level="15.4" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-estimation-for-a-regression-model"><i class="fa fa-check"></i><b>15.4</b> Bayesian estimation for a regression model</a></li>
<li class="chapter" data-level="15.5" data-path="linear-regression.html"><a href="linear-regression.html#unit-information-prior"><i class="fa fa-check"></i><b>15.5</b> Unit information prior</a></li>
<li class="chapter" data-level="15.6" data-path="linear-regression.html"><a href="linear-regression.html#zellners-g-prior"><i class="fa fa-check"></i><b>15.6</b> Zellner’s <span class="math inline">\(g\)</span>-prior</a></li>
<li class="chapter" data-level="15.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-analysis-using-invariant-g-prior"><i class="fa fa-check"></i><b>15.7</b> Bayesian analysis using invariant <span class="math inline">\(g\)</span>-prior</a></li>
<li class="chapter" data-level="15.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-analysis-using-semiconjugate-prior"><i class="fa fa-check"></i><b>15.8</b> Bayesian analysis using semiconjugate prior</a>
<ul>
<li class="chapter" data-level="15.8.1" data-path="linear-regression.html"><a href="linear-regression.html#prediction-problem"><i class="fa fa-check"></i><b>15.8.1</b> Prediction problem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>16</b> Model Selection</a>
<ul>
<li class="chapter" data-level="16.1" data-path="model-selection.html"><a href="model-selection.html#review"><i class="fa fa-check"></i><b>16.1</b> Review</a></li>
<li class="chapter" data-level="16.2" data-path="model-selection.html"><a href="model-selection.html#bayesian-model-comparison"><i class="fa fa-check"></i><b>16.2</b> Bayesian model comparison</a></li>
<li class="chapter" data-level="16.3" data-path="model-selection.html"><a href="model-selection.html#example-oxygen-uptake-1"><i class="fa fa-check"></i><b>16.3</b> Example: Oxygen uptake</a></li>
<li class="chapter" data-level="16.4" data-path="model-selection.html"><a href="model-selection.html#gibbs-sampling-and-model-averaging"><i class="fa fa-check"></i><b>16.4</b> Gibbs sampling and model averaging</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html"><i class="fa fa-check"></i><b>17</b> Generalized Linear Models; the Metropolis Algorithm</a>
<ul>
<li class="chapter" data-level="17.1" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#example-song-sparrow-reproductive-success"><i class="fa fa-check"></i><b>17.1</b> Example: Song sparrow reproductive success</a></li>
<li class="chapter" data-level="17.2" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#sec:poisson"><i class="fa fa-check"></i><b>17.2</b> Poisson regression</a></li>
<li class="chapter" data-level="17.3" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#logistic-regression"><i class="fa fa-check"></i><b>17.3</b> Logistic regression</a></li>
<li class="chapter" data-level="17.4" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#posterior-approximations"><i class="fa fa-check"></i><b>17.4</b> Posterior approximations</a></li>
<li class="chapter" data-level="17.5" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>17.5</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="17.6" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#example-normal-distribution-with-known-variance"><i class="fa fa-check"></i><b>17.6</b> Example: Normal distribution with known variance</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#output-of-metropolis-algorithm"><i class="fa fa-check"></i><b>17.6.1</b> Output of Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#the-metropolis-algorithm-for-poisson-regression-secmetpois"><i class="fa fa-check"></i><b>17.7</b> The Metropolis algorithm for Poisson regression {sec:metpois}</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html"><i class="fa fa-check"></i><b>18</b> Metropolis-Hastings</a>
<ul>
<li class="chapter" data-level="18.1" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#why-does-the-metropolis-hastings-algorithm-work"><i class="fa fa-check"></i><b>18.1</b> Why does the Metropolis-Hastings algorithm work?</a></li>
<li class="chapter" data-level="18.2" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#combining-the-metropolis-and-gibbs-algorithms"><i class="fa fa-check"></i><b>18.2</b> Combining the Metropolis and Gibbs algorithms</a></li>
<li class="chapter" data-level="18.3" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#example-historical-co_2-and-temperature-data"><i class="fa fa-check"></i><b>18.3</b> Example: Historical CO<span class="math inline">\(_2\)</span> and temperature data</a></li>
<li class="chapter" data-level="18.4" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#a-regression-model-with-correlated-errors"><i class="fa fa-check"></i><b>18.4</b> A regression model with correlated errors</a></li>
<li class="chapter" data-level="18.5" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#analysis-of-the-ice-core-data"><i class="fa fa-check"></i><b>18.5</b> Analysis of the ice core data</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Mixed-effects Models, aka, Hierarchical Linear Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#hierarchical-model-review"><i class="fa fa-check"></i><b>19.1</b> Hierarchical model review</a></li>
<li class="chapter" data-level="19.2" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#hierarchical-linear-regression-model-for-math-scores-data"><i class="fa fa-check"></i><b>19.2</b> Hierarchical linear regression model for math scores data</a></li>
<li class="chapter" data-level="19.3" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#bayesian-hierarchical-linear-regression-model"><i class="fa fa-check"></i><b>19.3</b> Bayesian hierarchical linear regression model</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#full-conditionals"><i class="fa fa-check"></i><b>19.3.1</b> Full conditionals</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#bayesian-analysis-of-the-math-scores-data"><i class="fa fa-check"></i><b>19.4</b> Bayesian analysis of the math scores data</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#mcmc-diagnostics-2"><i class="fa fa-check"></i><b>19.4.1</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#posterior-summaries"><i class="fa fa-check"></i><b>19.4.2</b> Posterior summaries</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#posterior-predictive-simulation"><i class="fa fa-check"></i><b>19.4.3</b> Posterior predictive simulation</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics lecture notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="predictive" class="section level1" number="7">
<h1><span class="header-section-number">Lecture 7</span> Predictive</h1>
<p><tt>The following notes, mostly transcribed from Neath(0511,2021) lecture, summarize sections(4.3 and 4.4) of Hoff(2009). </tt></p>
<p>
 
</p>
<p>Last class, we talked about using simulated draws <span class="math inline">\(\theta^{(s)} \sim p(\theta | y)\)</span> to form a Monte Carlo approximation to the posterior distribution <span class="math inline">\(p(\theta | y)\)</span>.</p>
<p>Today we do Monte Carlo simulation for the posterior predictive distribution. <strong>A predictive distribution</strong> is characterized by two features: (1) known quantities are conditioned on (2) unknown quantities are integrated out. For example, if we integrate <span class="math inline">\(\theta\)</span> out of <span class="math inline">\(\int{ p(\tilde y | \theta) p(\theta ) d\theta } = p(\tilde y)\)</span>, we call this a prior predictive distribution; A predictive distribution that integrates over unknown parameters but is not conditional on observed data.</p>
<p><strong>A result of probability theory:</strong> If the sampling model is Poisson<span class="math inline">\((\theta)\)</span> and the prior is <span class="math inline">\(\theta \sim \text{gamma}(a, b)\)</span> then the prior predictive is <span class="math inline">\(\text{NegBinom}(a, b)\)</span>; an overdispersed count distribution. The Poisson distribution has the property that <span class="math inline">\(\text{Var}(Y) = E(Y)\)</span>. For a negative binomial distribution, <span class="math inline">\(\text{Var}(Y) &gt; E(Y)\)</span>. A predictive distribution that accounts for the information available in the sample data <span class="math inline">\((y_1, …, y_n) = y\)</span> is called a <strong>posterior predictive distribution</strong> and in the Poisson-gamma model, because the gamma is a conjugate for the Poisson and the posterior on <span class="math inline">\(\theta\)</span> is gamma<span class="math inline">\((a+ \sum{y_i},b + n)\)</span> the posterior predictive distribution (PPD) for an <span class="math inline">\((n+1)\)</span>st observation is <span class="math inline">\(\text{NegBinom}(a + \sum{y_i}, b + n)\)</span>.</p>
<p>R has <code>dnbinom, pnbinom, rnbinom</code> functions. So we don’t need to do Monte Carlo for the PPD any more than we need to for approximating the posterior of <span class="math inline">\(\theta\)</span> but we’re gonna anyway because it’s educational.</p>
<p><em>Note:</em> When I say “Poisson-gamma model” or I may say “gamma-Poisson model” that’s a shorthand for “The Bayesian statistical model where our data consists of exchangeable observations of conditionally independent Poissons with rate <span class="math inline">\(\theta\)</span> where the prior distribution on <span class="math inline">\(\theta\)</span> is gamma<span class="math inline">\((a, b)\)</span>”</p>
<div id="sampling-for-predictive-distribution" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Sampling for predictive distribution</h2>
<p>Often, not only does the PPD not have a nice closed form but it’s also not even so straightforward to sample from. But we can still do Monte Carlo for predictive distributions!</p>
<p><strong>Proposition:</strong> If <span class="math inline">\(\theta^{(s)} \sim p(\theta | y)\)</span> and <span class="math inline">\(y^{(s)} \sim p(y | \theta^{(s)})\)</span> then I should say <span class="math inline">\(\tilde y^{(s)} \sim p(\tilde y | \theta^{(s)})\)</span> then jointly <span class="math inline">\((\theta^{(s)}, \tilde y^{(s)}) \sim p(\theta | y) \times p(\tilde y | \theta) = p(\theta, \tilde y | y)\)</span> the joint distribution of <span class="math inline">\(\theta \text{ and } \tilde Y\)</span>. In general if <span class="math inline">\(X_1, X_2 \sim f(x_1, x_2)\)</span> then <span class="math inline">\(X_2\)</span> is a draw from the marginal of <span class="math inline">\(X_2.\)</span></p>
<div id="example-birth-rate-1" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Example: birth rate</h3>
<p>Let’s return to the birthrates example. Our posterior for <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> were independent. <span class="math inline">\(\theta_1 | y \sim\)</span> gamma(219, 112) and <span class="math inline">\(\theta_2 | y \sim\)</span> gamma(68, 45). Suppose we want the posterior predictive probability <span class="math inline">\(Pr(\tilde Y_1 &gt; \tilde Y_2 |\)</span> data).</p>
<p><span class="math inline">\(\tilde Y_1 =\)</span> number of children for a randomly selected 40-year-old woman with less than bachelor’s degree. <span class="math inline">\(\tilde Y_2 =\)</span> number of children for a randomly selected woman with bachelor’s or higher.</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{Pr}\left(\tilde{Y}_{1}\right.&amp;\left.&gt;\tilde{Y}_{2} \mid\left\{y_{i,j}\right\}\right)=\sum_{\tilde y_{1}=0}^{\infty} \sum_{\tilde y_{2}=0}^{\tilde y_{1}-1} \operatorname{Pr}\left(\tilde{Y}_{1}=y_{1}, \tilde{Y}_{2}=y_{2} \mid\left\{y_{i,j}\right\}\right) \\
&amp;=\sum_{\tilde y_{1}=0}^{\infty} \texttt {pnbinom}\left(\tilde y_{1}-1 \mid 68,45\right) \times \texttt {dnbinom}\left(\tilde y_{1} \mid 219,112\right)
\end{aligned}
\]</span></p>
<p>Note: if I did <code>pnbinom(y1.tilde)</code> in the second line that’d be right for <span class="math inline">\(\tilde Y_2 \le \tilde Y_1.\)</span> I want <span class="math inline">\(\tilde Y_2 &lt; \tilde Y_1.\)</span></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="predictive.html#cb47-1" aria-hidden="true" tabindex="-1"></a>y1.tilde <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb47-2"><a href="predictive.html#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>( <span class="fu">pnbinom</span>(y1.tilde<span class="dv">-1</span>, <span class="at">size=</span><span class="dv">68</span>, <span class="at">mu=</span><span class="dv">68</span><span class="sc">/</span><span class="dv">45</span>)<span class="sc">*</span></span>
<span id="cb47-3"><a href="predictive.html#cb47-3" aria-hidden="true" tabindex="-1"></a>       <span class="fu">dnbinom</span>(y1.tilde, <span class="at">size=</span><span class="dv">219</span>, <span class="at">mu=</span><span class="dv">219</span><span class="sc">/</span><span class="dv">112</span>) )</span></code></pre></div>
<pre><code>## [1] 0.4820895</code></pre>
<p>Wait a second. Weren’t we thinking number of children for no college should be less than number of children for college grads so shouldn’t this probability be greater than 0.5? Not really. <span class="math inline">\(Pr(\tilde Y_1 &lt; \tilde Y_2)\)</span> will also be less than 0.5 and in fact will be less than .48 (it will be 0.3 to be exact). Remember we’re talking about a discrete distribution. if you randomly select a women with less than bachelor’s degree and a woman with bachelor’s or higher there’s a decent chance they have the SAME number of kids.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="predictive.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pr(Y1 tilde &gt;= Y2 tilde )</span></span>
<span id="cb49-2"><a href="predictive.html#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>( <span class="fu">pnbinom</span>(y1.tilde, <span class="at">size=</span><span class="dv">68</span>,  <span class="at">mu=</span><span class="dv">68</span><span class="sc">/</span><span class="dv">45</span>)<span class="sc">*</span></span>
<span id="cb49-3"><a href="predictive.html#cb49-3" aria-hidden="true" tabindex="-1"></a>     <span class="fu">dnbinom</span>(y1.tilde, <span class="at">size=</span><span class="dv">219</span>, <span class="at">mu=</span><span class="dv">219</span><span class="sc">/</span><span class="dv">112</span>) )</span></code></pre></div>
<pre><code>## [1] 0.6997647</code></pre>
<p>Notice how <span class="math inline">\(Pr(\tilde Y_2 \le \tilde Y_1|y_{i,j})\)</span> is over 0.50.</p>
<p>
 
</p>
<p>Here we could do Monte Carlo simulations using the <code>rnbinom</code> function. Or we could do <code>rgamma</code> and <code>rpois</code>. They are both correct but the longer way generalizes to more complicated problems where the PPD may not be tractable.</p>
<ul>
<li><code>rgamma(3, 1, 1)</code>; I get 3 draws from a gamma(1,1) distribution.</li>
<li><code>rpois(10, 5)</code>; I get 10 draws from a Poisson(5)</li>
<li><code>rpois(10, 6:15)</code>; I get one Poisson(6) draw one Poisson(7) draw one Poisson(8) … one Poisson(15) draw</li>
</ul>
<p>These <span class="math inline">\(\tilde y\)</span>’s are Poisson variables but each with a different mean and since the Poisson means are gamma-distributed the <span class="math inline">\(\tilde y\)</span> will be negative binomially distributed.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="predictive.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Birth rates example; Posterior predictive simulations </span></span>
<span id="cb51-2"><a href="predictive.html#cb51-2" aria-hidden="true" tabindex="-1"></a>a  <span class="ot">&lt;-</span> <span class="dv">2</span>  ;  b     <span class="ot">&lt;-</span> <span class="dv">1</span>;  </span>
<span id="cb51-3"><a href="predictive.html#cb51-3" aria-hidden="true" tabindex="-1"></a>n1 <span class="ot">&lt;-</span> <span class="dv">111</span>; sum.y1 <span class="ot">&lt;-</span> <span class="dv">217</span>;</span>
<span id="cb51-4"><a href="predictive.html#cb51-4" aria-hidden="true" tabindex="-1"></a>n2 <span class="ot">&lt;-</span> <span class="dv">44</span> ; sum.y2 <span class="ot">&lt;-</span> <span class="dv">66</span>;</span>
<span id="cb51-5"><a href="predictive.html#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="predictive.html#cb51-6" aria-hidden="true" tabindex="-1"></a>theta1.sim <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">10000</span>, a<span class="sc">+</span>sum.y1, b<span class="sc">+</span>n1)</span>
<span id="cb51-7"><a href="predictive.html#cb51-7" aria-hidden="true" tabindex="-1"></a>theta2.sim <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">10000</span>, a<span class="sc">+</span>sum.y2, b<span class="sc">+</span>n2)</span>
<span id="cb51-8"><a href="predictive.html#cb51-8" aria-hidden="true" tabindex="-1"></a>y1.tilde   <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">10000</span>, theta1.sim)</span>
<span id="cb51-9"><a href="predictive.html#cb51-9" aria-hidden="true" tabindex="-1"></a>y2.tilde   <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">10000</span>, theta2.sim)</span></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="predictive.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y1.tilde <span class="sc">==</span> y2.tilde)</span></code></pre></div>
<pre><code>## [1] 0.2254</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="predictive.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y1.tilde <span class="sc">&gt;=</span> y2.tilde)</span></code></pre></div>
<pre><code>## [1] 0.7052</code></pre>
<p>So indeed these conclusions are consistent with our calculations from above. i.e., Women without bachelors having more kids on average than women with bachelors.</p>
<p><strong>Here’s an important thing:</strong> Given a Monte Carlo sample from a probability distribution, I can compute (approximations to / estimates of) any feature of that distribution that interests me. E.g., moments (means, variance, etc) probabilities, quantiles. That holds for posterior predictive distributions as well as posterior distributions.</p>
</div>
</div>
<div id="example-let-d-tilde-y_1---tilde-y_2" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></h2>
<p>What does <span class="math inline">\(D\)</span> mean exactly? It means this: Randomly select a 40-year-old woman with less than bachelor’s and a 40-year-old woman with bachelors or higher <span class="math inline">\(D =\)</span> number of kids first woman has minus number of kids second woman has. It’s a difference between two negative binomial distributions (independent NegBinom’s). I don’t know anything about the properties of such a distribution (it doesn’t matter, I don’t need to know). We can use our samples of <span class="math inline">\(\tilde y_1 \text{ and } \tilde y_2\)</span> to construct the posterior predictive distribution of this difference <span class="math inline">\(D\)</span>!</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="predictive.html#cb56-1" aria-hidden="true" tabindex="-1"></a>D.tilde <span class="ot">&lt;-</span> y1.tilde <span class="sc">-</span> y2.tilde</span>
<span id="cb56-2"><a href="predictive.html#cb56-2" aria-hidden="true" tabindex="-1"></a>D.vals  <span class="ot">&lt;-</span> <span class="fu">range</span>(D.tilde)[<span class="dv">1</span>]<span class="sc">:</span><span class="fu">range</span>(D.tilde)[<span class="dv">2</span>]</span>
<span id="cb56-3"><a href="predictive.html#cb56-3" aria-hidden="true" tabindex="-1"></a>ppd.D   <span class="ot">&lt;-</span> (<span class="fu">table</span>(<span class="fu">c</span>(D.tilde, D.vals))<span class="sc">-</span><span class="dv">1</span>) <span class="sc">/</span> <span class="fu">length</span>(D.tilde)</span>
<span id="cb56-4"><a href="predictive.html#cb56-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-5"><a href="predictive.html#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(D.vals, <span class="fu">as.vector</span>(ppd.D), <span class="at">type=</span><span class="st">&quot;h&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb56-6"><a href="predictive.html#cb56-6" aria-hidden="true" tabindex="-1"></a> <span class="at">xlab=</span><span class="st">&quot;D=y1.tilde-y2.tilde&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;p(D|y1,y2)&quot;</span>)</span>
<span id="cb56-7"><a href="predictive.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(D.vals, <span class="fu">as.vector</span>(ppd.D), <span class="at">pch=</span><span class="dv">19</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-32"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-32-1.png" alt="The posterior predictive distribution of D = Y1.tilde - Y2.tilde, the diﬀerence in the number of children of two randomly sampled women, one from each of the two educational population" width="672" />
<p class="caption">
Figure 7.1: The posterior predictive distribution of D = Y1.tilde - Y2.tilde, the diﬀerence in the number of children of two randomly sampled women, one from each of the two educational population
</p>
</div>
<p>The most frequently occurring value is 0 the next most frequent is +1. The non-college-grad has 1 more kid than the college grad. This plot would have been hard to do any way other than by Monte Carlo.</p>
<p>
 
</p>
<p>The weird business in the code with the <code>(table(c(D.tilde, D.vals))-1)</code> is there because the table function doesn’t count missing values.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="predictive.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Case in point</span></span>
<span id="cb57-2"><a href="predictive.html#cb57-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>, <span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>, <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">8</span>);<span class="fu">table</span>(x)</span></code></pre></div>
<pre><code>## x
## 1 2 3 5 6 8 
## 3 3 3 1 1 1</code></pre>
<p>If we are making a histogram we would want it to show that we have zero fours. Hence the weird business.</p>
</div>
<div id="posterior-predictive-model-checking" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Posterior predictive model checking</h2>
<p>What we’ve done so far is assumed number of kids | <span class="math inline">\(\theta\)</span> are independent Poisson.</p>
<p>Here’s a question: Number of kids is not a Poisson variable. The Poisson distribution counts events in a Poisson process which occur randomly and independently of each other. That is not how people have kids at all. <em>Why did we use it?</em> The answer is: because number of kids is a count variable and the Poisson distribution is a simple model for count variables. You know the most famous quote in statistics? “All models are wrong, some models are useful.”</p>
<p>The real question is: Is it useful even though it’s wrong? Posterior predictive simulations (next section) are a good tool for addressing this question.</p>
<p>Student question: I thought that Poisson was a good distribution for rare events?</p>
<p>Ans: It is. But not only rare events. Suppose <span class="math inline">\(Y \sim \text{Poisson}(\theta).\)</span> No rule against <span class="math inline">\(\theta\)</span> being a big number. <span class="math inline">\(\theta =\)</span> expected count. “Rare events” just means probability is low but if exposure is high then expected number of counts can be high as well.</p>
</div>
<div id="posterior-predictive-model-checking-1" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Posterior predictive model checking</h2>
<p>Let’s take the first group for illustration(less than bachelor’s degree).</p>
<ul>
<li>No.kids = { 0,1,2,3,4,5,6 }</li>
<li>Frequency = { 20,19,38,20,10,2,2 }</li>
</ul>
<p>This is actually the first time we’ve seen these data because all we needed before was sufficient statistic <span class="math inline">\((n = 111,~ \sum{y_i} = 217)\)</span>. Take each of these counts divided by 111 that defines the empirical distribution.</p>
<p>The idea of “posterior predictive model checking” is; <em>if the model is “correct” then the observed data should not appear unusual when compared to the posterior predictive distribution.</em> <span class="math inline">\(p(\tilde y | y)\)</span> is the conditional distribution of number of chidren conditional on the observed data. If the data really were drawn from the model that we used the empirical distribution and the PPD should show close agreement.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="predictive.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior predictive model checking for group 1 </span></span>
<span id="cb59-2"><a href="predictive.html#cb59-2" aria-hidden="true" tabindex="-1"></a>y      <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb59-3"><a href="predictive.html#cb59-3" aria-hidden="true" tabindex="-1"></a>e.dist <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">19</span>,<span class="dv">38</span>,<span class="dv">20</span>,<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="fu">rep</span>(<span class="dv">0</span>,<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">7</span>)) <span class="sc">/</span> <span class="dv">111</span></span>
<span id="cb59-4"><a href="predictive.html#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y<span class="fl">-.05</span>, e.dist, <span class="at">type=</span><span class="st">&quot;h&quot;</span>, <span class="at">lwd=</span><span class="dv">5</span>, <span class="at">xlab=</span><span class="st">&quot;y&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Pr(Y=y)&quot;</span>)</span>
<span id="cb59-5"><a href="predictive.html#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="predictive.html#cb59-6" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">2</span>;  b <span class="ot">&lt;-</span> <span class="dv">1</span>;  n <span class="ot">&lt;-</span> <span class="dv">111</span>; sum.y <span class="ot">&lt;-</span> <span class="dv">217</span>;</span>
<span id="cb59-7"><a href="predictive.html#cb59-7" aria-hidden="true" tabindex="-1"></a>p.dist <span class="ot">&lt;-</span> <span class="fu">dnbinom</span>(y, <span class="at">size=</span>a<span class="sc">+</span>sum.y, <span class="at">mu=</span>(a<span class="sc">+</span>sum.y)<span class="sc">/</span>(b<span class="sc">+</span>n))</span>
<span id="cb59-8"><a href="predictive.html#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(y<span class="fl">+.05</span>, p.dist, <span class="at">type=</span><span class="st">&quot;h&quot;</span>, <span class="at">lwd=</span><span class="dv">5</span>, <span class="at">col=</span><span class="st">&quot;gray&quot;</span>)</span>
<span id="cb59-9"><a href="predictive.html#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">inset=</span>.<span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">5</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;gray&quot;</span>),</span>
<span id="cb59-10"><a href="predictive.html#cb59-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;Empirical dist&quot;</span>, <span class="st">&quot;Predictive dist&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-34"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-34-1.png" alt="Evaluation of model it. This shows the empirical and posterior predictive distributions of the number of children of women without a bachelor’s degree." width="672" />
<p class="caption">
Figure 7.2: Evaluation of model it. This shows the empirical and posterior predictive distributions of the number of children of women without a bachelor’s degree.
</p>
</div>
<p>
 
</p>
<p>the number of women with exactly two children is 38, which is twice the number of women in the sample with one child. In contrast, this group’s posterior predictive distribution, shown in gray, suggests that the probability of sampling a woman with two children is slightly less probable than sampling a woman with one.</p>
<p>Should we be concerned about this? First, is it really a disagreement? There are two possible answers:</p>
<ul>
<li>It is a result of sampling variability.</li>
<li>It is indeed a feature of the population, hence our model is wrong.</li>
</ul>
<p>The answer is based on frequentist hypothesis testing. We will simulate <span class="math inline">\(S\)</span> replicated data sets (Remember <span class="math inline">\(S\)</span> is a big number). For each <span class="math inline">\(\theta^{(s)} \sim p(\theta | y),\)</span> and for each replicated data set compute the test statistic. What test statistic to use? Whatever feature of the data you supsect the model is not capturing correctly. In this case we use <span class="math inline">\(t(y_{obs}) = 38 / 19 = 2.0\)</span>. What does the posterior predictive distribution of this test statistic look like? and how unusual a value is 2.0?</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="predictive.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the test statistic </span></span>
<span id="cb60-2"><a href="predictive.html#cb60-2" aria-hidden="true" tabindex="-1"></a>t.sim <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">10000</span>)</span>
<span id="cb60-3"><a href="predictive.html#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span>)</span>
<span id="cb60-4"><a href="predictive.html#cb60-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb60-5"><a href="predictive.html#cb60-5" aria-hidden="true" tabindex="-1"></a> theta.sim <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">1</span>, a<span class="sc">+</span>sum.y, b<span class="sc">+</span>n)</span>
<span id="cb60-6"><a href="predictive.html#cb60-6" aria-hidden="true" tabindex="-1"></a> y.tilde   <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n, theta.sim)</span>
<span id="cb60-7"><a href="predictive.html#cb60-7" aria-hidden="true" tabindex="-1"></a> t.sim[s]  <span class="ot">&lt;-</span> <span class="fu">sum</span>(y.tilde<span class="sc">==</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>(y.tilde<span class="sc">==</span><span class="dv">1</span>)</span>
<span id="cb60-8"><a href="predictive.html#cb60-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="predictive.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(t.sim, <span class="at">freq=</span>F, <span class="at">right=</span>F, <span class="at">breaks=</span><span class="dv">30</span>, <span class="st">&quot;&quot;</span>)</span>
<span id="cb61-2"><a href="predictive.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-36"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-36-1.png" alt="The posterior predictive distribution of the empirical odds of having two children versus one child in a dataset of size n = 111" width="672" />
<p class="caption">
Figure 7.3: The posterior predictive distribution of the empirical odds of having two children versus one child in a dataset of size n = 111
</p>
</div>
<p>How often in these replicated(simulated) data sets does it happen that there are double the number of cases with <span class="math inline">\(y=2\)</span> as with <span class="math inline">\(y=1?\)</span> The answer is not very often. The observed test statistic <span class="math inline">\(t(y_{obs}) = 2.0\)</span>
is kind of out in the tail of this distribution.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="predictive.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(t.sim <span class="sc">&gt;=</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.0059</code></pre>
<p>This quantity 0.0051 that’s the tail probability. This value is called a <strong>“Bayesian p-value”</strong>. This “Bayesian p-value” has nothing to do with Bayesian inference it’s strictly a tool in Bayesian model checking.</p>
<p>We did the posterior predictive checks we found Bayesian p-value = .005 (close to zero). Based on this which bullet from above is the right one? It’s the second one! If this was to be expected due to sampling variability then it would have occurred with some frequency among the replicated data sets. But it didn’t! So this feature of the data “<span class="math inline">\(y=2\)</span> with much greater frequency than <span class="math inline">\(y=1\)</span>” is not explained by the model. So the conclusion is; This is indeed a deficiency of the Poisson model. To the extent that there our features of our data set that are not shared by the replicated data sets this MAY suggest a problem.</p>
<p>We’re still back to the original question. How concerned should we be? Is predicting frequency of one kid and two kids an important goal of our inference? If yes, then we’ve got a problem because the Poisson model is not going to give accurate predictions here. It will underpredict the frequency of <span class="math inline">\(y=2\)</span> and overpredict the frequency of <span class="math inline">\(y=1\)</span>. However if what we really care about is estimation of mean and variance of “number of kids” then this is not such a problem.</p>
<p>So in terms of what’s the next step in this data analysis? The answer is: it depends.</p>
<p>Thus endeth our discussion of Chapter 4. On to the next thing Chapter 5.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="monte-carlo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="normal-mean.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesianS21.pdf", "bayesianS21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
