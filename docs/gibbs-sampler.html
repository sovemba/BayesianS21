<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 10 Gibbs sampler | Bayesian Statistics notes from summer 2021</title>
  <meta name="description" content="Lecture 10 Gibbs sampler | Bayesian Statistics notes from summer 2021" />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 10 Gibbs sampler | Bayesian Statistics notes from summer 2021" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 10 Gibbs sampler | Bayesian Statistics notes from summer 2021" />
  
  
  

<meta name="author" content="Chisom Onyishi" />


<meta name="date" content="2021-05-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="joint-inference-for-normal-mean-and-variance.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Belief and Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i><b>1.1</b> Example:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#binomial"><i class="fa fa-check"></i><b>1.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.4</b> Poisson distribution</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#example-normalmu-10.75-sigma0.8"><i class="fa fa-check"></i><b>1.5.1</b> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exchangeability.html"><a href="exchangeability.html"><i class="fa fa-check"></i><b>2</b> Exchangeability</a><ul>
<li class="chapter" data-level="2.1" data-path="exchangeability.html"><a href="exchangeability.html#discrete-joint-distributions"><i class="fa fa-check"></i><b>2.1</b> Discrete joint distributions</a></li>
<li class="chapter" data-level="2.2" data-path="exchangeability.html"><a href="exchangeability.html#bayesrule"><i class="fa fa-check"></i><b>2.2</b> Bayes’ rule and parameter estimation</a></li>
<li class="chapter" data-level="2.3" data-path="exchangeability.html"><a href="exchangeability.html#independent-random-variables"><i class="fa fa-check"></i><b>2.3</b> Independent random variables</a></li>
<li class="chapter" data-level="2.4" data-path="exchangeability.html"><a href="exchangeability.html#exchangeability-1"><i class="fa fa-check"></i><b>2.4</b> Exchangeability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binomial-1.html"><a href="binomial-1.html"><i class="fa fa-check"></i><b>3</b> Binomial</a><ul>
<li class="chapter" data-level="3.1" data-path="binomial-1.html"><a href="binomial-1.html#example---exchangeable-binary-data"><i class="fa fa-check"></i><b>3.1</b> Example - Exchangeable binary data</a></li>
<li class="chapter" data-level="3.2" data-path="binomial-1.html"><a href="binomial-1.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.2</b> The beta distribution</a><ul>
<li class="chapter" data-level="3.2.1" data-path="binomial-1.html"><a href="binomial-1.html#properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Properties of the beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="binomial-1.html"><a href="binomial-1.html#binomial-distribution"><i class="fa fa-check"></i><b>3.3</b> Binomial distribution</a><ul>
<li class="chapter" data-level="3.3.1" data-path="binomial-1.html"><a href="binomial-1.html#posterior-inference-for-a-binomial-sampling-model"><i class="fa fa-check"></i><b>3.3.1</b> Posterior inference for a binomial sampling model</a></li>
<li class="chapter" data-level="3.3.2" data-path="binomial-1.html"><a href="binomial-1.html#conjugacy"><i class="fa fa-check"></i><b>3.3.2</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="binomial-1.html"><a href="binomial-1.html#combining-information"><i class="fa fa-check"></i><b>3.4</b> Combining information</a><ul>
<li class="chapter" data-level="3.4.1" data-path="binomial-1.html"><a href="binomial-1.html#example-1"><i class="fa fa-check"></i><b>3.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="binomial-1.html"><a href="binomial-1.html#prediction"><i class="fa fa-check"></i><b>3.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#how-do-we-compute-intervals"><i class="fa fa-check"></i><b>4.2</b> How do we compute intervals?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="CI.html"><a href="CI.html#example-2"><i class="fa fa-check"></i><b>4.2.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>5</b> Poisson model</a><ul>
<li class="chapter" data-level="5.1" data-path="poisson-model.html"><a href="poisson-model.html#posterior-inference-for-the-poisson-model"><i class="fa fa-check"></i><b>5.1</b> Posterior inference for the Poisson model</a></li>
<li class="chapter" data-level="5.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>5.2</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="5.3" data-path="poisson-model.html"><a href="poisson-model.html#example-birth-rates"><i class="fa fa-check"></i><b>5.3</b> Example: Birth rates</a></li>
<li class="chapter" data-level="5.4" data-path="poisson-model.html"><a href="poisson-model.html#explaining-the-parameters-of-the-gamma-distribution"><i class="fa fa-check"></i><b>5.4</b> Explaining the parameters of the gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="monte-carlo.html"><a href="monte-carlo.html#example-birth-rate"><i class="fa fa-check"></i><b>6.1</b> Example: birth rate</a></li>
<li class="chapter" data-level="6.2" data-path="monte-carlo.html"><a href="monte-carlo.html#the-monte-carlo-method"><i class="fa fa-check"></i><b>6.2</b> The Monte Carlo method</a></li>
<li class="chapter" data-level="6.3" data-path="monte-carlo.html"><a href="monte-carlo.html#example-numerical-evaluation"><i class="fa fa-check"></i><b>6.3</b> Example: Numerical evaluation</a></li>
<li class="chapter" data-level="6.4" data-path="monte-carlo.html"><a href="monte-carlo.html#posterior-inference-for-arbitrary-functions"><i class="fa fa-check"></i><b>6.4</b> Posterior inference for arbitrary functions</a></li>
<li class="chapter" data-level="6.5" data-path="monte-carlo.html"><a href="monte-carlo.html#example-log-odds"><i class="fa fa-check"></i><b>6.5</b> Example: Log-odds</a></li>
<li class="chapter" data-level="6.6" data-path="monte-carlo.html"><a href="monte-carlo.html#example-functions-of-two-parameters"><i class="fa fa-check"></i><b>6.6</b> Example: Functions of two parameters</a></li>
<li class="chapter" data-level="6.7" data-path="monte-carlo.html"><a href="monte-carlo.html#how-many-monte-carlo-samples-are-needed"><i class="fa fa-check"></i><b>6.7</b> How many Monte Carlo samples are needed?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predictive.html"><a href="predictive.html"><i class="fa fa-check"></i><b>7</b> Predictive</a><ul>
<li class="chapter" data-level="7.1" data-path="predictive.html"><a href="predictive.html#sampling-for-predictive-distribution"><i class="fa fa-check"></i><b>7.1</b> Sampling for predictive distribution</a><ul>
<li class="chapter" data-level="7.1.1" data-path="predictive.html"><a href="predictive.html#example-birth-rate-1"><i class="fa fa-check"></i><b>7.1.1</b> Example: birth rate</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="predictive.html"><a href="predictive.html#example-let-d-tilde-y_1---tilde-y_2"><i class="fa fa-check"></i><b>7.2</b> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>7.3</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="7.4" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking-1"><i class="fa fa-check"></i><b>7.4</b> Posterior predictive model checking</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normal-mean.html"><a href="normal-mean.html"><i class="fa fa-check"></i><b>8</b> Normal Mean</a><ul>
<li class="chapter" data-level="8.1" data-path="normal-mean.html"><a href="normal-mean.html#example-womens-height"><i class="fa fa-check"></i><b>8.1</b> Example: women’s height</a></li>
<li class="chapter" data-level="8.2" data-path="normal-mean.html"><a href="normal-mean.html#inference-for-the-mean-conditional-on-the-variance"><i class="fa fa-check"></i><b>8.2</b> Inference for the mean, conditional on the variance</a></li>
<li class="chapter" data-level="8.3" data-path="normal-mean.html"><a href="normal-mean.html#prediction-1"><i class="fa fa-check"></i><b>8.3</b> Prediction</a></li>
<li class="chapter" data-level="8.4" data-path="normal-mean.html"><a href="normal-mean.html#example-midge-wing-length"><i class="fa fa-check"></i><b>8.4</b> Example: Midge wing length</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html"><i class="fa fa-check"></i><b>9</b> Joint inference for Normal mean and variance</a><ul>
<li class="chapter" data-level="9.1" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#marginal-posterior-of-sigma2"><i class="fa fa-check"></i><b>9.1</b> Marginal posterior of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#example-midge-wing-length-1"><i class="fa fa-check"></i><b>9.2</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="9.3" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>9.3</b> Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.4" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#summary-of-normal-formulas"><i class="fa fa-check"></i><b>9.4</b> Summary of Normal formulas</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html"><i class="fa fa-check"></i><b>10</b> Gibbs sampler</a><ul>
<li class="chapter" data-level="10.1" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#a-semiconjugate-prior-distribution"><i class="fa fa-check"></i><b>10.1</b> A semiconjugate prior distribution</a></li>
<li class="chapter" data-level="10.2" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.3" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-midge-wing-length-2"><i class="fa fa-check"></i><b>10.3</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="10.4" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#discrete-approximation-of-posterior-distribution"><i class="fa fa-check"></i><b>10.4</b> Discrete approximation of posterior distribution</a></li>
<li class="chapter" data-level="10.5" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-3"><i class="fa fa-check"></i><b>10.5</b> Example</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics notes from summer 2021</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gibbs-sampler" class="section level1">
<h1><span class="header-section-number">Lecture 10</span> Gibbs sampler</h1>
<p><tt>The following notes, mostly transcribed from Neath(0517,2021) lecture, summarize sections(6.1-6.4) of Hoff(2009).</tt></p>
<p>
 
</p>
<div id="a-semiconjugate-prior-distribution" class="section level2">
<h2><span class="header-section-number">10.1</span> A semiconjugate prior distribution</h2>
<p>We write our model <span class="math inline">\(n\)</span> observations from a normal population with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and we want to do Bayesian inference about <span class="math inline">\(\theta\)</span> (and maybe <span class="math inline">\(\sigma^2\)</span>). The conjugate prior for this model is: <span class="math inline">\(\sigma^2 \sim \text{InverseGamma}(a = \nu_0 / 2, b = \nu_0\sigma_0^2 / 2)\)</span></p>
<p>Review(where does this come from)</p>
<p><span class="math inline">\(X \sim\)</span> Inverse gamma means <span class="math inline">\(X = 1 / Z\)</span> where <span class="math inline">\(Z \sim\)</span> gamma dist. When we say the conjugate prior for a normal variance is the inverse gamma that’s equivalent to saying the conjugate prior for a normal precision(precision = 1 / variance) is the gamma dist. The conjugate prior for <span class="math inline">\((\theta, \sigma^2)\)</span> is completed by <span class="math inline">\(\theta | \sigma^2 \sim \text{Normal}( \mu_0, \sigma^2 / \kappa_0 )\)</span>. The variance depending on <span class="math inline">\(\sigma^2\)</span> makes sense because the distribution is specified conditionally on <span class="math inline">\(\sigma^2.\)</span></p>
<p>Back to that gamma dist. The reparameterization from the usual gamma(<span class="math inline">\(a =\)</span> shape, <span class="math inline">\(b =\)</span> rate) to <span class="math inline">\((a = \nu_0 /2, b = \nu_0 \times \sigma^2_0 / 2)\)</span>. This is strategic; <span class="math inline">\(\sigma^2_0\)</span> is the “prior best guess” and <span class="math inline">\(\nu_0\)</span> measures the strength of belief in that best guess. More precisely, this prior contributes exactly the same information to the posterior as would <span class="math inline">\(\nu_0\)</span> observations with a sample variance of <span class="math inline">\(\sigma_0^2.\)</span> This conjugate prior represents a “prior sample of size <span class="math inline">\(\nu_0\)</span>” “with a sample variance of <span class="math inline">\(\sigma_0^2\)</span>”. Similarly (and more straightforwardly) the prior on <span class="math inline">\(\theta\)</span> contributes to the posterior exactly what would be contributed by <span class="math inline">\(\kappa_0\)</span> observations with a sample mean of <span class="math inline">\(\mu_0\)</span>.</p>
<p>The parameters in the normal conjugate prior are; <span class="math inline">\(\mu_0\)</span> (prior sample mean) <span class="math inline">\(\kappa_0\)</span> (prior sample size for the mean) <span class="math inline">\(\sigma_0^2\)</span> (prior sample variance) <span class="math inline">\(\nu_0\)</span> (prior sample size for variance). Then the updating is very intuitive <span class="math inline">\(\theta | \sigma^2, y_1, …, y_n \sim \text{Normal}( \mu_n, \sigma^2 / \kappa_n), ~~\kappa_n = \kappa_0 + n,~ \mu_n = (\kappa_0 \mu_0 + n \bar y)/ (\kappa_0 + n)\)</span></p>
<p>Question: How to decide a proper <span class="math inline">\(\kappa_0\)</span>? If you’re uncertain, take <span class="math inline">\(\kappa_0\)</span> to be small relative to <span class="math inline">\(n\)</span> and it doesn’t really matter.</p>
<p>One of the strengths of the Bayesian paradigm is that it allows the incorporation of prior information. But in practice, non informative priors are much more commonly used (<span class="math inline">\(\kappa_0\)</span> is small relative to <span class="math inline">\(n\)</span>), thus the posterior is mostly determined by the results of the data, experiment, sample, etc. Though, this is not the only reason we use Bayesian Statistics. We like the updates, we like the interpretations in terms of probability.</p>
<p>Back in chapter 5 (our previous class in fact) we skipped some stuff about “improper priors”. Given this line of questioning maybe we should “un-skip” this section some time in the next few days</p>
<p>The punchline: There is a way to do Bayesian inference and not incorporate ANY prior information. You still have some decisions to make regarding the prior dist but they’re in terms of form not content. It will often be (usually be) very similar in terms of the substantive conclusions if not exactly the same to the frequentist. But there are some very complex models (maybe we get to this stuff toward the end of our course) where the Bayesian answer is actually a lot easier to get to than the frequentist.</p>
<p>Today’s class still doing normal model but suppose I don’t like this “conjugate prior”. I don’t like that the conjugate prior forces me to describe my uncertatinty about <span class="math inline">\(\theta\)</span> conditionally on <span class="math inline">\(\sigma^2\)</span> what if my prior knowledge of <span class="math inline">\(\theta\)</span> and my prior knowledge of <span class="math inline">\(\sigma^2\)</span> have nothing to do with each other and I want my prior on <span class="math inline">\(\theta\)</span> to be indep of my prior on <span class="math inline">\(\theta\)</span>. This joint prior dist is ‘semiconjugate’ for the normal model.
<span class="math display">\[
\{~\theta \sim \text{Normal}(\mu_0, \tau_0^2); \quad 1/\sigma^2 \sim \text{gamma}(\nu_0/2, \nu_0 \sigma_0^2/2) ~\}
\]</span>
With <span class="math inline">\(\sigma^2\)</span> fixed this is the conjugate prior for <span class="math inline">\(\theta,\)</span> and with <span class="math inline">\(\theta\)</span> fixed this is the conjugate prior for <span class="math inline">\(\sigma^2\)</span>. Let’s take prior:</p>
<p><span class="math inline">\(\theta \sim \text{Normal}(\mu_0 , \tau_0^2)\)</span></p>
<p><span class="math inline">\(\sigma^2 \sim \text{InverseGamma}(\nu_0/2, \nu_0\sigma_0^2/ 2),\)</span> <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma^2\)</span> are indep.</p>
<p>What posterior results from this prior? Conditionally <span class="math inline">\(\sigma^2,\)</span> <span class="math inline">\(\theta\)</span> has a normal posterior. i.e., { <span class="math inline">\(\theta | y_1, …, y_n , \sigma^2 ~ \} \sim \text{Normal}( \mu_n, \tau_n^2 )\)</span>. Remember we write <span class="math inline">\(1/\tau_n^2 = 1 / \tau_0^2 + n / \sigma^2.\)</span> i.e., posterior precision = prior precision + data precision. So this is a prior that is defined not dependent on <span class="math inline">\(\sigma^2\)</span> but the posterior is specified conditionally on <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Note: there is no <span class="math inline">\(\kappa_0\)</span> in this prior because <span class="math inline">\(\kappa_0\)</span> is a parameter of the prior <span class="math inline">\(p(\theta | \sigma^2)\)</span>. In this prior <span class="math inline">\(p(\theta | \sigma^2) = p(\theta).\)</span></p>
</div>
<div id="gibbs-sampling" class="section level2">
<h2><span class="header-section-number">10.2</span> Gibbs sampling</h2>
<p><img src="bayesianS21_files/figure-html/contour%20plot%20for%20precision-1.png" width="90%" height="90%" style="display: block; margin: auto;" /></p>
<p>This is a posterior dist for the (the flies’ wings) but pretend it’s the conjugate prior for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(1/\sigma^2\)</span>. The conditional prior <span class="math inline">\(p(\theta | \sigma^2 )\)</span> is very tight (has low variance) when precision is high (<span class="math inline">\(\sigma^2\)</span> is low) and is highly diffuse (has a high variance) when precision is low (<span class="math inline">\(\sigma^2\)</span> is big). Remember in these pictures the conditional dist <span class="math inline">\(p(\theta | \sigma^2 )\)</span> is visualized in these pictures by taking ‘horizontal slices’</p>
<p>Last week (Ch 5 in Hoff) we wrote <span class="math inline">\(p(\theta, \sigma^2) = p(\sigma^2) p(\theta | \sigma^2)\)</span>. For the semi conjugate prior we’re studying today (slide 4 of the 06a deck). This picture would not be like this. <span class="math inline">\(p(\theta | \sigma^2)\)</span> would be the same for all <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Let’s agree that this semiconjugate prior that is <span class="math inline">\(\theta \sim \text{Normal}( \mu_0, \tau_0^2 ), ~ \sigma^2 \sim IG(\nu_0 / 2, \nu_0 \sigma_0^2 / 2 ),~ \theta\)</span> and <span class="math inline">\(\sigma^2\)</span> indep i.e., <span class="math inline">\(p(\theta, \sigma^2) = \texttt{dnorm}(\theta | …) \times \texttt{dinvgamma}(\sigma^2 | … )\)</span> is worth considering. So we ask: what posterior results? and we got half way to answering the question.</p>
<p>We know that <span class="math inline">\(p(\theta | \sigma^2, y_1, …, y_n)\)</span> is Normal<span class="math inline">\((\mu_n, \tau_n^2 )\)</span> so if we can solve <span class="math inline">\(p(\sigma^2 | y_1, …, y_n)\)</span> we solve the posterior. This doesn’t have a nice solution as it turns out. However what does have a nice solution is <span class="math inline">\(p(\sigma^2 | \theta, y_1, …, y_n)=IG( \nu_n/2 , \nu_n \sigma_n^2(\theta) / 2 ).\)</span> The <span class="math inline">\(\sigma_n^2(\theta)\)</span> parameter is a weighted average of <span class="math inline">\(\sigma_0^2\)</span> and the sample variance around <span class="math inline">\(\theta\)</span>.</p>
<p>Here’s our predicament; to sample from the posterior <span class="math inline">\(p(\theta, \sigma^2 | y_1, …, y_n)\)</span> it would be sufficient to be able to sample from <span class="math inline">\(p(\theta | y_1, …, y_n)\)</span> and <span class="math inline">\(p(\sigma^2 | \theta, y_1, …, y_n)\)</span>. But the marginal of <span class="math inline">\(\theta\)</span> is not nice. We could also sample from this posterior <span class="math inline">\(p(\theta , \sigma^2 | y_1, …., y_n)\)</span> if we could sample from <span class="math inline">\(p(\theta | \sigma^2, y_1, …, y_n)\)</span> and <span class="math inline">\(p(\sigma^2 | y_1, ….,y_n)\)</span>. But we don’t have an answer for either of the marginals we have answers for both of the conditionals but not either of the marginals.</p>
<p>Just thinking about a pair of random variables call em <span class="math inline">\((X_1, X_2)\)</span>. I can write their joint density as <span class="math inline">\(f(x_1, x_2) = f(x_1)f(x_2 | x_1)\)</span> or <span class="math inline">\(f(x_1, x_2) = f(x_2 )f(x_1 | x_2)\)</span>. But if I don’t know <span class="math inline">\(f(x_1)\)</span> or <span class="math inline">\(f(x_2)\)</span> but I know both of <span class="math inline">\(f(x_1|x_2)\)</span> and <span class="math inline">\(f(x_2 | x_1)\)</span>. what can I do with this? Answer: I can do a <strong>Gibbs sampler</strong>.</p>
<p>Let’s pretend we had a draw from <span class="math inline">\(\sigma^{2(1)} \sim p(\sigma^2 | y )\)</span> and <span class="math inline">\(\theta^{(1)} \sim p(\theta | \sigma^{2(1)}, y)\)</span> so <span class="math inline">\((\theta, \sigma^2)^{(1)} \sim p(\theta, \sigma | y)\)</span>. So I have at least one draw from the joint posterior. <span class="math inline">\(\theta^{(1)} \sim p(\theta | y),\)</span> let me do <span class="math inline">\(\sigma^{2(2)} \sim p(\sigma^2 | \theta^{(1)}, y)\)</span> so now we got <span class="math inline">\((\theta^{(1)}, \sigma^{2(2)}) \sim p(\theta , \sigma^2 | y)\)</span> thus <span class="math inline">\(\sigma^{2(2)} \sim p(\sigma^2 | y).\)</span> Sample <span class="math inline">\(\theta^{(2)} \sim p(\theta | \sigma^{2(2)}, y)\)</span> etc.</p>
<p>So the answer to the question : What can we do with both conditional dists (but neither marginal dist)? “with” here means “the means to simulate samples from”. Well if we could just get a starting point we could simulate a sequence such that each element in this sequence is marginally drawn from the joint posterior dist <span class="math inline">\(p(\theta, \sigma^2 | y).\)</span> One complication here: Note that the draws would not be indep You see why? <span class="math inline">\(\theta^{(1)} \sim p(\theta | y), \theta^{(2)} \sim p(\theta | y)\)</span> but they are not independent because <span class="math inline">\(\theta^{(2)}\)</span> is drawn conditionally on <span class="math inline">\(\sigma^{2(2)}\)</span> and <span class="math inline">\(\sigma^{2(2)}\)</span> is drawn conditionally on <span class="math inline">\(\theta^{(1)}\)</span> as a result there is dependence between <span class="math inline">\(\theta^{(2)}\)</span> and <span class="math inline">\(\theta^{(1)}\)</span>. We’ll worry about that tomorrow.</p>
<p>This iterative sampling for the iteratively updated conditional distributions is called the <strong>Gibbs sampler</strong>. We’ll define it here in our 2-parameter model.</p>
<pre><code>n      &lt;- length(y);  ybar &lt;- mean(y);  s2 &lt;- var(y);  
S      &lt;- 1000
phi    &lt;- matrix(NA, S, 2)
theta  &lt;- ybar
sigma2 &lt;- (nu.0*sigma2.0 + (n-1)*s2) / (nu.0 + n)
for(s in 1:S)
{
  tau2.n &lt;- 1 / (1/tau2.0 + n/sigma2)
  mu.n   &lt;- tau2.n * (mu.0/tau2.0 + n*ybar/sigma2)
  theta  &lt;- rnorm(1, mean=mu.n, sd=sqrt(tau2.n))
  sigma2 &lt;- 1/rgamma(1, (nu.0 + n)/2, 
    (nu.0*sigma2.0 + (n-1)*s2 + n*(ybar-theta)^2)/2)
  phi[s,] &lt;- c(theta, sigma2)
}</code></pre>
<p>This R code assumes we have a data vector <span class="math inline">\(\boldsymbol{y}\)</span> and parameter variables <span class="math inline">\(\mu_0, ~ \tau_0^2\)</span> (the parameters of the normal prior on <span class="math inline">\(\theta\)</span>) <span class="math inline">\(\nu_0, ~ \sigma_0^2\)</span>(parameters of the inverse gamma prior on <span class="math inline">\(\sigma^2\)</span>).</p>
<p>As we learn more of these Markov chain Monte Carlo methods we’ll see this structure more and more. Where the simulation step is not done by</p>
<pre><code>theta.sim &lt;- r&quot;dist&quot; ( S, … )</code></pre>
<p>It’s gotta be done by for-loops because each draw depends on the previous draw.</p>
<p><span class="math inline">\(\tau_0^2\)</span> is a parameter of the prior dist but <span class="math inline">\(\tau_n^2\)</span> should write <span class="math inline">\(\tau_0^2(\sigma^2)\)</span>. <span class="math inline">\(\theta = \bar y =\)</span> sample mean, <span class="math inline">\(\sigma^2 =\)</span> sample variance is sensible. The object <span class="math inline">\(\phi\)</span>(<code>phi</code>) in this code is the matrix of simulations. The <span class="math inline">\(s\)</span>th row of <code>phi</code> is the <span class="math inline">\(s\)</span>th iteration of the Gibbs sampler the first column of <code>phi</code> is the <span class="math inline">\(\theta\)</span>-components the second column is the <span class="math inline">\(\sigma^2\)</span> components. See Hoff’s book. He does mean and precision. Note that the conditional of {<span class="math inline">\(\sigma^2 | \theta, y\)</span>} depends on <span class="math inline">\(\sum (y_i - \theta)^2.\)</span> Recalculating this for every updated <span class="math inline">\(\theta\)</span> is inefficient see slide 20. Instead of calculating <span class="math inline">\(\sum (y_i - \theta)^2\)</span> for each updated <span class="math inline">\(\theta\)</span> value only recalculate <span class="math inline">\((\bar y - \theta)^2\)</span>.</p>
</div>
<div id="example-midge-wing-length-2" class="section level2">
<h2><span class="header-section-number">10.3</span> Example: Midge wing length</h2>
<p>Our prior information about these insects leads us to expect mean of 1.9 or so. <code>mu.0 = 1.9</code></p>
<p>What variance to put on that? the logic that went into <code>tau2.0 = .95^2</code>? It was give low prior prob to <span class="math inline">\(\theta &lt; 0.\)</span> Expected sd around .10 or so, so set <span class="math inline">\(\sigma_0^2= (.10)^2 = .01\)</span> and then set <span class="math inline">\(\nu_0 = 1\)</span></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="gibbs-sampler.html#cb89-1"></a><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="dt">mgp=</span><span class="kw">c</span>(<span class="fl">1.75</span>,.<span class="dv">75</span>,<span class="dv">0</span>),<span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>,<span class="dv">0</span>))</span>
<span id="cb89-2"><a href="gibbs-sampler.html#cb89-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb89-3"><a href="gibbs-sampler.html#cb89-3"></a><span class="co"># First the joint dist of (theta, sigma2)</span></span>
<span id="cb89-4"><a href="gibbs-sampler.html#cb89-4"></a><span class="kw">plot</span>(phi, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">col=</span><span class="st">&quot;gray&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;theta&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;sigma2&quot;</span>)</span>
<span id="cb89-5"><a href="gibbs-sampler.html#cb89-5"></a><span class="kw">points</span>(phi, <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">75</span>)</span>
<span id="cb89-6"><a href="gibbs-sampler.html#cb89-6"></a><span class="co"># Now the marginal dist of theta</span></span>
<span id="cb89-7"><a href="gibbs-sampler.html#cb89-7"></a><span class="kw">plot</span>(<span class="kw">density</span>(phi[,<span class="dv">1</span>]), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">xlab=</span><span class="st">&quot;theta&quot;</span>, </span>
<span id="cb89-8"><a href="gibbs-sampler.html#cb89-8"></a>  <span class="dt">ylab=</span><span class="st">&quot;p(theta|y1,...,yn)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb89-9"><a href="gibbs-sampler.html#cb89-9"></a><span class="kw">abline</span>(<span class="dt">v=</span> <span class="kw">quantile</span>(phi[,<span class="dv">1</span>], <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>)), <span class="dt">lty=</span><span class="dv">2</span>)</span>
<span id="cb89-10"><a href="gibbs-sampler.html#cb89-10"></a><span class="co"># Now the marginal of sigma2</span></span>
<span id="cb89-11"><a href="gibbs-sampler.html#cb89-11"></a><span class="kw">plot</span>(<span class="kw">density</span>(phi[,<span class="dv">2</span>]), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">xlab=</span><span class="st">&quot;sigma2&quot;</span>, </span>
<span id="cb89-12"><a href="gibbs-sampler.html#cb89-12"></a>  <span class="dt">ylab=</span><span class="st">&quot;p(sigma2|y1,...,yn)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/Plot%20Gibbs%20output-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The left-most is a scatterplot. The “point cloud” represents the empirical joint posterior!</p>
<p>The marginal of <span class="math inline">\(\theta\)</span> is not Normal but it appears symmetric and bell shaped so that’s nice.</p>
<p>The marginal <span class="math inline">\(p(\sigma^2 | y_1, …, y_n)\)</span> is right-skewed the peak is around .02 or so. The sample variance was .017 so I guess this makes sense.</p>
<p>There’s another thing in the scatterplot that we wouldn’t normally draw (and we wouldn’t draw these lines in our data analysis reports either) they’re just to illustrate the path that the draws have taken. What would it look like if we did a plot like this but with indep draws? I believe it would be uglier than this</p>
<p>Let’s go back one class for an example where we could do indep simulations</p>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-58-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>So the difference between these two picture. In the left hand side the draws are independent, i.e., <span class="math inline">\(\theta^{(s)}\)</span> is independent of <span class="math inline">\(\theta^{(s-1)}.\)</span> In the right hand plot there is serial dependence. Is that apparent? I think not really.</p>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-59-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In this case (the normal model) the dependence between <span class="math inline">\(\theta^{(s)}\)</span> and <span class="math inline">\(\theta^{(s-1)}\)</span> is VERY weak.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="gibbs-sampler.html#cb90-1"></a><span class="co"># CI for population mean</span></span>
<span id="cb90-2"><a href="gibbs-sampler.html#cb90-2"></a><span class="kw">quantile</span>(phi[,<span class="dv">1</span>], <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.5</span>, <span class="fl">.975</span>))</span></code></pre></div>
<pre><code>##  2.5%   50% 97.5% 
## 1.716 1.806 1.903</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="gibbs-sampler.html#cb92-1"></a><span class="co"># CI for population variance </span></span>
<span id="cb92-2"><a href="gibbs-sampler.html#cb92-2"></a><span class="kw">quantile</span>(phi[,<span class="dv">2</span>], <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.5</span>, <span class="fl">.975</span>))</span></code></pre></div>
<pre><code>##     2.5%      50%    97.5% 
## 0.007533 0.017341 0.053634</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="gibbs-sampler.html#cb94-1"></a><span class="co"># CI for population standard deviation</span></span>
<span id="cb94-2"><a href="gibbs-sampler.html#cb94-2"></a><span class="kw">quantile</span>(<span class="kw">sqrt</span>(phi[,<span class="dv">2</span>]), <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.5</span>, <span class="fl">.975</span>))</span></code></pre></div>
<pre><code>##    2.5%     50%   97.5% 
## 0.08679 0.13169 0.23159</code></pre>
</div>
<div id="discrete-approximation-of-posterior-distribution" class="section level2">
<h2><span class="header-section-number">10.4</span> Discrete approximation of posterior distribution</h2>
<p>Let’s be real general here. Suppose you have a single-parameter <span class="math inline">\(\theta\)</span>. You can write the posterior <span class="math inline">\(p(\theta|y) = c\times p(\theta)p(y | \theta) = p(\theta) p(y | \theta) / p(y). ~~ p(y)\)</span> depends on integrating <span class="math inline">\(\theta\)</span> out of the numerator of this thing. That may be hard. So here’s a thing you can do. Pick a bunch of <span class="math inline">\(\theta\)</span> values, <span class="math inline">\(\theta^{(1)} &lt; \theta^{(2)} &lt; … &lt; \theta^{(S)}\)</span>. I’m using the notation of MC simulation but it’s not that these are fixed points.</p>
<p><span class="math inline">\(Pr(\theta &lt; \theta^{(1)} |y) = 0\)</span></p>
<p><span class="math inline">\(Pr(\theta &gt; \theta^{(S)} | y) = 0\)</span></p>
<p>If those two conditions are met and abs<span class="math inline">\((\theta^{(s)} - \theta^{(s-1)})\)</span> is small then the continuous dist <span class="math inline">\(p(\theta | y)\)</span> can be well approximated by the discrete dist <span class="math inline">\(p(\theta^{(s)} | y)\)</span> for <span class="math inline">\(s = 1, …, S\)</span> and the discrete distribution can be computed exactly because I can compute <span class="math inline">\(p(\theta^{(s)}) p(y | \theta^{(s)})\)</span> for each value of <span class="math inline">\(\theta^{(s)},\)</span> divide each entry by the sum of all entries and the sum of the entries becomes 1 so it’s a probability distribution! You did something like this on your first HW assignment. For the mixture dist posterior you calculated it at a bunch of points between 0 and 1.</p>
<p>Now suppose you had two parameters <span class="math inline">\(p(\theta_1, \theta_2 | y)\)</span>. You can do the exact same thing except it doesn’t require double the computation. What does it require? If I split the range of <span class="math inline">\(\theta\)</span> into 100 points. I had to compute the posterior 100 times. If I split the range of <span class="math inline">\(\theta^{(1)}\)</span> and the range of <span class="math inline">\(\theta^{(2)}\)</span> into 100 points each, I have to calculate the posterior <span class="math inline">\(100^2\)</span> times. Still fine. What if I had 16 parameters <span class="math inline">\((\theta_1, \theta_2, …., \theta_{16} ) = \boldsymbol\theta\)</span>. Then I couldn’t do discrete approximation any more but I could still do a Gibbs sampler (or some other MCMC approach). So that’s going to become our go-to method.</p>
</div>
<div id="example-3" class="section level2">
<h2><span class="header-section-number">10.5</span> Example</h2>
<p>The R-code below evaluates <span class="math inline">\(p(\theta, 1/\sigma^2| y_1 ,..., y_n )\)</span> on a <span class="math inline">\(121\times250\)</span> grid of evenly <span class="math inline">\(1/\sigma^2\)</span> spaced parameter values, with <span class="math inline">\(\theta \in\)</span>{ 1.500, 1.505, . . . , 2.095, 2.100 } and <span class="math inline">\(1/\sigma^2 \in\)</span> { 1, 2, . . . , 249, 250 }. The first panel gives the discrete approximation to the joint distribution of { <span class="math inline">\(\theta, 1/\sigma^2\)</span> }</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="gibbs-sampler.html#cb96-1"></a>theta    &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">1.5</span>, <span class="fl">2.1</span>, <span class="fl">.005</span>)</span>
<span id="cb96-2"><a href="gibbs-sampler.html#cb96-2"></a>I.sig2   &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">250</span>, <span class="dv">1</span>)</span>
<span id="cb96-3"><a href="gibbs-sampler.html#cb96-3"></a>G        &lt;-<span class="st"> </span><span class="kw">length</span>(theta);  H &lt;-<span class="st"> </span><span class="kw">length</span>(I.sig2); <span class="co">#121 #250</span></span>
<span id="cb96-4"><a href="gibbs-sampler.html#cb96-4"></a>log.post &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, G, H); </span>
<span id="cb96-5"><a href="gibbs-sampler.html#cb96-5"></a></span>
<span id="cb96-6"><a href="gibbs-sampler.html#cb96-6"></a><span class="cf">for</span>(g <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>G)</span>
<span id="cb96-7"><a href="gibbs-sampler.html#cb96-7"></a>{ <span class="cf">for</span>(h <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H)</span>
<span id="cb96-8"><a href="gibbs-sampler.html#cb96-8"></a>  { log.post[g,h] &lt;-<span class="st"> </span></span>
<span id="cb96-9"><a href="gibbs-sampler.html#cb96-9"></a><span class="st">         </span><span class="kw">dnorm</span>(theta[g], mu<span class="fl">.0</span>, <span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(tau2<span class="fl">.0</span>), <span class="dt">log=</span>T) <span class="op">+</span><span class="st"> </span></span>
<span id="cb96-10"><a href="gibbs-sampler.html#cb96-10"></a><span class="st">         </span><span class="kw">dgamma</span>(I.sig2[h], nu<span class="fl">.0</span><span class="op">/</span><span class="dv">2</span>, nu<span class="fl">.0</span><span class="op">*</span>sigma2<span class="fl">.0</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">log=</span>T) <span class="op">+</span><span class="st"> </span></span>
<span id="cb96-11"><a href="gibbs-sampler.html#cb96-11"></a><span class="st">         </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(y, theta[g], <span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(I.sig2[h]), <span class="dt">log=</span>T)) </span>
<span id="cb96-12"><a href="gibbs-sampler.html#cb96-12"></a>}} </span>
<span id="cb96-13"><a href="gibbs-sampler.html#cb96-13"></a>post.grid &lt;-<span class="st"> </span><span class="kw">exp</span>(log.post);  <span class="kw">rm</span>(log.post);</span>
<span id="cb96-14"><a href="gibbs-sampler.html#cb96-14"></a>post.grid &lt;-<span class="st"> </span>post.grid <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(post.grid)  </span></code></pre></div>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="gibbs-sampler.html#cb97-1"></a><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>),<span class="dt">mgp=</span><span class="kw">c</span>(<span class="fl">1.75</span>,.<span class="dv">75</span>,<span class="dv">0</span>),<span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>,<span class="dv">0</span>))</span>
<span id="cb97-2"><a href="gibbs-sampler.html#cb97-2"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb97-3"><a href="gibbs-sampler.html#cb97-3"></a>contours &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">001</span>, <span class="fl">.01</span>, <span class="kw">seq</span>(.<span class="dv">05</span>, <span class="fl">.95</span>, <span class="fl">.10</span>)) <span class="op">*</span><span class="st"> </span><span class="kw">max</span>(post.grid)</span>
<span id="cb97-4"><a href="gibbs-sampler.html#cb97-4"></a></span>
<span id="cb97-5"><a href="gibbs-sampler.html#cb97-5"></a><span class="kw">contour</span>(theta, I.sig2, post.grid, <span class="dt">levels=</span>contours, <span class="dt">drawlabels=</span>F, </span>
<span id="cb97-6"><a href="gibbs-sampler.html#cb97-6"></a>  <span class="dt">xlab=</span><span class="st">&quot;theta&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;1/sigma2&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Joint distribution&quot;</span>)</span>
<span id="cb97-7"><a href="gibbs-sampler.html#cb97-7"></a><span class="kw">plot</span>(theta, <span class="kw">apply</span>(post.grid, <span class="dv">1</span>, sum), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, </span>
<span id="cb97-8"><a href="gibbs-sampler.html#cb97-8"></a>  <span class="dt">xlab=</span><span class="st">&quot;theta&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Probability&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Marginal of mean&quot;</span>)</span>
<span id="cb97-9"><a href="gibbs-sampler.html#cb97-9"></a><span class="kw">plot</span>(I.sig2, <span class="kw">apply</span>(post.grid, <span class="dv">2</span>, sum), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, </span>
<span id="cb97-10"><a href="gibbs-sampler.html#cb97-10"></a>  <span class="dt">xlab=</span><span class="st">&quot;1/sigma2&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Probability&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Marginal of precision&quot;</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/discrete%20approximation%20plot-1.png" width="672" style="display: block; margin: auto;" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="joint-inference-for-normal-mean-and-variance.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesianS21.pdf", "bayesianS21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
