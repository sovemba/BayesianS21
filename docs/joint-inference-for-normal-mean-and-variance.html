<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 9 Joint inference for Normal mean and variance | Bayesian Statistics notes from summer 2021</title>
  <meta name="description" content="Lecture 9 Joint inference for Normal mean and variance | Bayesian Statistics notes from summer 2021" />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 9 Joint inference for Normal mean and variance | Bayesian Statistics notes from summer 2021" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 9 Joint inference for Normal mean and variance | Bayesian Statistics notes from summer 2021" />
  
  
  

<meta name="author" content="Chisom Onyishi" />


<meta name="date" content="2021-05-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="normal-mean.html"/>
<link rel="next" href="gibbs-sampler.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Belief and Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i><b>1.1</b> Example:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#binomial"><i class="fa fa-check"></i><b>1.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.4</b> Poisson distribution</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.5</b> Continuous Random Variables</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#example-normalmu-10.75-sigma0.8"><i class="fa fa-check"></i><b>1.5.1</b> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exchangeability.html"><a href="exchangeability.html"><i class="fa fa-check"></i><b>2</b> Exchangeability</a><ul>
<li class="chapter" data-level="2.1" data-path="exchangeability.html"><a href="exchangeability.html#discrete-joint-distributions"><i class="fa fa-check"></i><b>2.1</b> Discrete joint distributions</a></li>
<li class="chapter" data-level="2.2" data-path="exchangeability.html"><a href="exchangeability.html#bayesrule"><i class="fa fa-check"></i><b>2.2</b> Bayes’ rule and parameter estimation</a></li>
<li class="chapter" data-level="2.3" data-path="exchangeability.html"><a href="exchangeability.html#independent-random-variables"><i class="fa fa-check"></i><b>2.3</b> Independent random variables</a></li>
<li class="chapter" data-level="2.4" data-path="exchangeability.html"><a href="exchangeability.html#exchangeability-1"><i class="fa fa-check"></i><b>2.4</b> Exchangeability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binomial-1.html"><a href="binomial-1.html"><i class="fa fa-check"></i><b>3</b> Binomial</a><ul>
<li class="chapter" data-level="3.1" data-path="binomial-1.html"><a href="binomial-1.html#example---exchangeable-binary-data"><i class="fa fa-check"></i><b>3.1</b> Example - Exchangeable binary data</a></li>
<li class="chapter" data-level="3.2" data-path="binomial-1.html"><a href="binomial-1.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.2</b> The beta distribution</a><ul>
<li class="chapter" data-level="3.2.1" data-path="binomial-1.html"><a href="binomial-1.html#properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Properties of the beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="binomial-1.html"><a href="binomial-1.html#binomial-distribution"><i class="fa fa-check"></i><b>3.3</b> Binomial distribution</a><ul>
<li class="chapter" data-level="3.3.1" data-path="binomial-1.html"><a href="binomial-1.html#posterior-inference-for-a-binomial-sampling-model"><i class="fa fa-check"></i><b>3.3.1</b> Posterior inference for a binomial sampling model</a></li>
<li class="chapter" data-level="3.3.2" data-path="binomial-1.html"><a href="binomial-1.html#conjugacy"><i class="fa fa-check"></i><b>3.3.2</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="binomial-1.html"><a href="binomial-1.html#combining-information"><i class="fa fa-check"></i><b>3.4</b> Combining information</a><ul>
<li class="chapter" data-level="3.4.1" data-path="binomial-1.html"><a href="binomial-1.html#example-1"><i class="fa fa-check"></i><b>3.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="binomial-1.html"><a href="binomial-1.html#prediction"><i class="fa fa-check"></i><b>3.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#how-do-we-compute-intervals"><i class="fa fa-check"></i><b>4.2</b> How do we compute intervals?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="CI.html"><a href="CI.html#example-2"><i class="fa fa-check"></i><b>4.2.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>5</b> Poisson model</a><ul>
<li class="chapter" data-level="5.1" data-path="poisson-model.html"><a href="poisson-model.html#posterior-inference-for-the-poisson-model"><i class="fa fa-check"></i><b>5.1</b> Posterior inference for the Poisson model</a></li>
<li class="chapter" data-level="5.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>5.2</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="5.3" data-path="poisson-model.html"><a href="poisson-model.html#example-birth-rates"><i class="fa fa-check"></i><b>5.3</b> Example: Birth rates</a></li>
<li class="chapter" data-level="5.4" data-path="poisson-model.html"><a href="poisson-model.html#explaining-the-parameters-of-the-gamma-distribution"><i class="fa fa-check"></i><b>5.4</b> Explaining the parameters of the gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="monte-carlo.html"><a href="monte-carlo.html#example-birth-rate"><i class="fa fa-check"></i><b>6.1</b> Example: birth rate</a></li>
<li class="chapter" data-level="6.2" data-path="monte-carlo.html"><a href="monte-carlo.html#the-monte-carlo-method"><i class="fa fa-check"></i><b>6.2</b> The Monte Carlo method</a></li>
<li class="chapter" data-level="6.3" data-path="monte-carlo.html"><a href="monte-carlo.html#example-numerical-evaluation"><i class="fa fa-check"></i><b>6.3</b> Example: Numerical evaluation</a></li>
<li class="chapter" data-level="6.4" data-path="monte-carlo.html"><a href="monte-carlo.html#posterior-inference-for-arbitrary-functions"><i class="fa fa-check"></i><b>6.4</b> Posterior inference for arbitrary functions</a></li>
<li class="chapter" data-level="6.5" data-path="monte-carlo.html"><a href="monte-carlo.html#example-log-odds"><i class="fa fa-check"></i><b>6.5</b> Example: Log-odds</a></li>
<li class="chapter" data-level="6.6" data-path="monte-carlo.html"><a href="monte-carlo.html#example-functions-of-two-parameters"><i class="fa fa-check"></i><b>6.6</b> Example: Functions of two parameters</a></li>
<li class="chapter" data-level="6.7" data-path="monte-carlo.html"><a href="monte-carlo.html#how-many-monte-carlo-samples-are-needed"><i class="fa fa-check"></i><b>6.7</b> How many Monte Carlo samples are needed?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predictive.html"><a href="predictive.html"><i class="fa fa-check"></i><b>7</b> Predictive</a><ul>
<li class="chapter" data-level="7.1" data-path="predictive.html"><a href="predictive.html#sampling-for-predictive-distribution"><i class="fa fa-check"></i><b>7.1</b> Sampling for predictive distribution</a><ul>
<li class="chapter" data-level="7.1.1" data-path="predictive.html"><a href="predictive.html#example-birth-rate-1"><i class="fa fa-check"></i><b>7.1.1</b> Example: birth rate</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="predictive.html"><a href="predictive.html#example-let-d-tilde-y_1---tilde-y_2"><i class="fa fa-check"></i><b>7.2</b> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>7.3</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="7.4" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking-1"><i class="fa fa-check"></i><b>7.4</b> Posterior predictive model checking</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normal-mean.html"><a href="normal-mean.html"><i class="fa fa-check"></i><b>8</b> Normal Mean</a><ul>
<li class="chapter" data-level="8.1" data-path="normal-mean.html"><a href="normal-mean.html#example-womens-height"><i class="fa fa-check"></i><b>8.1</b> Example: women’s height</a></li>
<li class="chapter" data-level="8.2" data-path="normal-mean.html"><a href="normal-mean.html#inference-for-the-mean-conditional-on-the-variance"><i class="fa fa-check"></i><b>8.2</b> Inference for the mean, conditional on the variance</a></li>
<li class="chapter" data-level="8.3" data-path="normal-mean.html"><a href="normal-mean.html#prediction-1"><i class="fa fa-check"></i><b>8.3</b> Prediction</a></li>
<li class="chapter" data-level="8.4" data-path="normal-mean.html"><a href="normal-mean.html#example-midge-wing-length"><i class="fa fa-check"></i><b>8.4</b> Example: Midge wing length</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html"><i class="fa fa-check"></i><b>9</b> Joint inference for Normal mean and variance</a><ul>
<li class="chapter" data-level="9.1" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#marginal-posterior-of-sigma2"><i class="fa fa-check"></i><b>9.1</b> Marginal posterior of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#example-midge-wing-length-1"><i class="fa fa-check"></i><b>9.2</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="9.3" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>9.3</b> Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.4" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#summary-of-normal-formulas"><i class="fa fa-check"></i><b>9.4</b> Summary of Normal formulas</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html"><i class="fa fa-check"></i><b>10</b> Gibbs sampler</a><ul>
<li class="chapter" data-level="10.1" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#a-semiconjugate-prior-distribution"><i class="fa fa-check"></i><b>10.1</b> A semiconjugate prior distribution</a></li>
<li class="chapter" data-level="10.2" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.3" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-midge-wing-length-2"><i class="fa fa-check"></i><b>10.3</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="10.4" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#discrete-approximation-of-posterior-distribution"><i class="fa fa-check"></i><b>10.4</b> Discrete approximation of posterior distribution</a></li>
<li class="chapter" data-level="10.5" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-3"><i class="fa fa-check"></i><b>10.5</b> Example</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics notes from summer 2021</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="joint-inference-for-normal-mean-and-variance" class="section level1">
<h1><span class="header-section-number">Lecture 9</span> Joint inference for Normal mean and variance</h1>
<p><tt>The following notes, mostly transcribed from Neath(0513,2021) lecture, summarize section(5.3) of Hoff(2009).</tt></p>
<p>
 
</p>
<p>What if we don’t really care about the variance? We’re only interested in doing inference about the mean anyway? Can we use the methods from the last lesson? NO we should not. Unless of course we genuinely do know the population variance. So even if our inferential goals don’t include the variance and we’re only interested in the mean we still have to account for the fact that the variance is unknown(there’s uncertainty about the variance) to do valid inference about the mean. In this case we say the variance is a <strong>nuisance parameter</strong>. So let’s talk about how to do that.</p>
<p>
 
</p>
<p>Model: We have <span class="math inline">\(n\)</span> exchangeable observations from a population that is normal with a mean of <span class="math inline">\(\theta\)</span> and a variance of <span class="math inline">\(\sigma^2\)</span> both of which are unknown. Given a prior distribution on <span class="math inline">\((\theta, \sigma^2)\)</span>, we use Bayes rule to compute a posterior distribution where the prior describes our belief before observing the sample data the posterior will describe our belief after observing the data. Is there a conjugate distribution for <span class="math inline">\((\theta, \sigma^2)\)</span> together?</p>
<p>Last class we saw that conditional on <span class="math inline">\(\sigma^2\)</span> the conjugate prior for <span class="math inline">\(\theta\)</span> was the normal distribution. Let’s use that fact going forward. Let’s write our prior distribution as the joint density <span class="math inline">\(p(\theta, \sigma^2) = p(\theta|\sigma^2)p(\sigma^2) = \texttt{dnorm} (\theta|\mu_0,\tau_0^2)p(\sigma^2).\)</span> Since we are conditioning on <span class="math inline">\(\sigma^2\)</span> anyway let’s set <span class="math inline">\(\tau_0^2=\sigma^2/\kappa_0,\)</span> where <span class="math inline">\(\kappa_0 =\)</span> number of prior observations, this way we have described(parameterized) our uncertainty about <span class="math inline">\(\theta\)</span> conditionally on <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If <span class="math inline">\(\theta | \sigma^2 \sim \text{Normal}( \mu_0 , \sigma^2 / \kappa_0 )\)</span> and data are <span class="math inline">\(\{y_1, …, y_n | \theta, \sigma^2\} \sim \text{ iid Normal}( \theta, \sigma^2)\)</span>. Equivalently in terms of the sufficient statistic <span class="math inline">\(\{\bar y | \theta, \sigma^2\} \sim \text{Normal}( \theta, \sigma^2 / n)\)</span></p>
<p>The posterior of <span class="math inline">\(\theta, \{\theta | y, \sigma^2\} \sim \text{Normal}\)</span> with mean <span class="math inline">\(\mu_n = (\kappa_0\mu_0+n\bar y)/\kappa_n\)</span> where <span class="math inline">\(\mu_n\)</span> is a weighted average of prior mean <span class="math inline">\(\mu_0\)</span> and data mean <span class="math inline">\(\bar y\)</span> and these two weights are proportional to <span class="math inline">\(\kappa_0\)</span> (prior sample size) and <span class="math inline">\(n\)</span> (“real data” sample size).</p>
<p>From previous lecture we found that the posterior variance <span class="math inline">\(\tau_n^2 = 1/a=1/(1/\tau_0^2+n/\sigma^2)\)</span>. Now plug in <span class="math inline">\(\tau_0^2 = \sigma_0^2 / \kappa_0\)</span> and you get <span class="math inline">\(\tau_n^2 = 1 / ( \kappa_0 / \sigma^2 + n / \sigma^2 ) = \sigma^2 / (\kappa_0 + n)=\sigma^2/\kappa_n\)</span>. This makes perfect sense! Our data consist of <span class="math inline">\(n\)</span> observations. Our prior consists of <span class="math inline">\(\kappa_0\)</span> “observations”.</p>
<p>So finally, we have
<span class="math display">\[\{\theta | y_1,...,y_n, \sigma^2\} \sim \text{Normal}( \mu_n = (\kappa_0\mu_0+n\bar y)/\kappa_n, ~\sigma^2/\kappa_n)\]</span></p>
<p>We’re half way there. <span class="math inline">\(p(\theta , \sigma^2) = p(\theta | \sigma^2)p(\sigma^2) = \texttt{dnorm}( \theta | \mu_0 , \sqrt{\sigma^2/\kappa_0} ) p(\sigma^2)\)</span></p>
<p><strong>What might be the conjugate prior for the variance <span class="math inline">\(\sigma^2?\)</span></strong> The gamma distribution has support (0, infinity). Might this work? It does not. It turns out. The gamma distribution is not conjugate for the variance, <span class="math inline">\(\sigma^2\)</span>. However, the gamma distribution is conjugate for the precision = 1 / variance.</p>
<p>How did we get this?</p>
<p>Condition on the likelihood <span class="math inline">\(p(\sigma^2 | y) \propto p(y | \sigma^2) p(\sigma^2),\)</span> and look at what the likelihood contributes. Recall, <span class="math inline">\(p(y|\theta,\sigma^2) = (\sqrt{2\pi\sigma^2})^{-1}\text{exp}\{-(1/2\sigma^2)(y-\theta)^2\} \propto (\sigma^2)^{-1}\text{exp}(-1/\sigma^2),\)</span> and the gamma distribution has the form <span class="math inline">\(p(\theta) \propto \theta^{a-1}e^{-b\theta}.\)</span> So what the likelihood contributes in this case for a normal variance is <span class="math inline">\((\sigma^2)^{\text{-something}} \times e^{- \text{something} / \sigma^2}.\)</span> Since it’s not <span class="math inline">\((\sigma^2)^{\text{something}} \times e^{- \sigma^2/\text{something} },\)</span> then it’s not <span class="math inline">\(\sigma^2\)</span> that has a gamma distribution but rather the precision, <span class="math inline">\(1/\sigma^2\)</span>.</p>
<p>Definition: If <span class="math inline">\(X \sim\)</span> gamma<span class="math inline">\((a, b)\)</span> and <span class="math inline">\(Y = 1/X\)</span> then <span class="math inline">\(Y \sim\)</span> InvGamma<span class="math inline">\((a, b).\)</span></p>
<p>The conjugate prior for the normal variance, <span class="math inline">\(\sigma^2\)</span> is to say <span class="math inline">\(1/\sigma^2 \sim \text{gamma}(a, b)\)</span> and since the parameter <span class="math inline">\(a &gt; 0, ~ b &gt; 0\)</span> are arbitrary we can reparameterize
this distribution to gamma<span class="math inline">\(( \nu_0 / 2 , ~ \nu_0\sigma_0^2 / 2)\)</span>. We could do this by <span class="math inline">\(\nu_0 = 2a, ~ \sigma_0^2 = b / a\)</span>. The reason we did this reparameterization is that this gives a more natural way to think about the variance. We know the mean and variance of the gamma distribution are <span class="math inline">\(a/b\)</span> and <span class="math inline">\(a/b^2\)</span> so we have;</p>
<ul>
<li><span class="math inline">\(E(1/\sigma^2) = 1/ \sigma_0^2\)</span></li>
<li>Var<span class="math inline">\(( 1 / \sigma^2) = 2 / [ \nu_0 \times (\sigma_0^2)^2 ]\)</span></li>
</ul>
<p>What do all these parameters in this prior distribution represent?</p>
<ul>
<li><span class="math inline">\(\mu_0\)</span> is prior best guess at <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(\kappa_0\)</span> measures the strength of that belief</li>
<li><span class="math inline">\(\sigma^2_0\)</span> is our prior best guess at <span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(\nu_0\)</span> measures the strength of that belief</li>
</ul>
<p>Remember that in inference for the normal distribution, inference about the mean and inference about the variance proceed “independently” in a sense <span class="math inline">\((\bar Y \perp s^2)\)</span>. So we’re allowed to have different prior sample sizes for the mean and the variance. Our data consist of <span class="math inline">\(n\)</span> observations from the population so the data will contribute <span class="math inline">\(n\)</span> to both the mean and the variance. The prior contributes <span class="math inline">\(\kappa_0\)</span> to the mean and <span class="math inline">\(\nu_0\)</span> to the variance. No requirement that these be equal.</p>
<p>The posterior density satisfies <span class="math inline">\(p(\theta, \sigma^2 | y) = p(\theta | \sigma^2, y)p(\sigma^2 | y)\)</span>. The first piece is already solved! <span class="math inline">\(p(\theta | \sigma^2, y) = \texttt{dnorm}( \theta | \mu_n , \sqrt{\sigma^2 / \kappa_n })\)</span>. Now for the second piece.</p>
<div id="marginal-posterior-of-sigma2" class="section level2">
<h2><span class="header-section-number">9.1</span> Marginal posterior of <span class="math inline">\(\sigma^2\)</span></h2>
<p>The result is that <span class="math inline">\(1/\sigma^2 | y \sim \text{gamma}( \nu_n / 2 , \nu_n \sigma_n^2 / 2 )\)</span>.
So that’s why that reparameterization was so useful. <span class="math inline">\(\nu_0\)</span> in the prior becomes <span class="math inline">\(\nu_n = \nu_0 + n\)</span> in the posterior the <span class="math inline">\(\sigma_0^2\)</span> in the prior becomes <span class="math inline">\(\sigma_n^2\)</span> in the posterior which is:
<span class="math display">\[\sigma_n^2 = \frac{1}{\nu_n}[\nu_0\sigma_0^2 + (n-1)s^2+\frac{\kappa_0n} {\kappa_n}(\bar y - \mu_0)^2]\]</span>
It’s almost a weighted average of the “prior” variance <span class="math inline">\(\sigma_0^2\)</span> and the “data variance” <span class="math inline">\(s^2\)</span>.</p>
<p><span class="math inline">\(\nu_n = \nu_0 + n = \nu_0 + (n-1) + 1\)</span>. The prior variance gets weight proportional to <span class="math inline">\(\nu_0\)</span>. The sample variance <span class="math inline">\(s^2\)</span> gets weight proportional to <span class="math inline">\((n-1)\)</span>. That extra piece is weird. It only gets weight of <span class="math inline">\(1 / \nu_n &lt; (n-1) / n\)</span>.</p>
</div>
<div id="example-midge-wing-length-1" class="section level2">
<h2><span class="header-section-number">9.2</span> Example: Midge wing length</h2>
<p>Our prior best guesses at the mean and variance for this population are <span class="math inline">\(\mu_0 = 1.9\)</span> and <span class="math inline">\(\sigma_0 = 0.01\)</span> based on studies of other populations.</p>
<p>Our data consist of <span class="math inline">\(n\)</span> observations. Our prior belief is based on not anything we have a whole lot of confidence in, but as long as we set <span class="math inline">\(\kappa_0\)</span> and <span class="math inline">\(\nu_0\)</span> to be small relative to <span class="math inline">\(n=9\)</span> they won’t get much weight in the posterior anyway. Set <span class="math inline">\(\nu_0 = 1, ~ \kappa_0 = 1\)</span> so prior gets 10% weight and data gets 90% weight in the posterior. That seems reasonable.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-1"></a>y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.64</span>, <span class="fl">1.70</span>, <span class="fl">1.72</span>, <span class="fl">1.74</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.90</span>, <span class="fl">2.08</span>)</span>
<span id="cb76-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-2"></a><span class="co"># Prior </span></span>
<span id="cb76-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-3"></a>mu<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="fl">1.9</span>;  sigma<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="fl">0.1</span>;  sigma2<span class="fl">.0</span> &lt;-<span class="st"> </span>sigma<span class="fl">.0</span><span class="op">^</span><span class="dv">2</span>;</span>
<span id="cb76-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-4"></a>nu<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="dv">1</span>;    kappa<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="dv">1</span>  ;</span>
<span id="cb76-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-5"></a><span class="co"># Calculations</span></span>
<span id="cb76-6"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-6"></a>n    &lt;-<span class="st"> </span><span class="kw">length</span>(y);   ybar   &lt;-<span class="st"> </span><span class="kw">mean</span>(y);  s2 &lt;-<span class="st"> </span><span class="kw">var</span>(y);  </span>
<span id="cb76-7"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-7"></a>nu.n &lt;-<span class="st"> </span>nu<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>n ;  kappa.n &lt;-<span class="st"> </span>kappa<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>n;</span>
<span id="cb76-8"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-8"></a>mu.n &lt;-<span class="st"> </span>(kappa<span class="fl">.0</span><span class="op">*</span>mu<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>n<span class="op">*</span>ybar) <span class="op">/</span><span class="st"> </span>kappa.n</span>
<span id="cb76-9"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-9"></a>sigma2.n &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>nu.n) <span class="op">*</span><span class="st"> </span>(nu<span class="fl">.0</span><span class="op">*</span>sigma2<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>(n<span class="dv">-1</span>)<span class="op">*</span>s2 <span class="op">+</span><span class="st"> </span></span>
<span id="cb76-10"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-10"></a><span class="st">   </span>kappa<span class="fl">.0</span><span class="op">*</span>n<span class="op">*</span>(ybar<span class="op">-</span>mu<span class="fl">.0</span>)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>kappa.n) </span>
<span id="cb76-11"><a href="joint-inference-for-normal-mean-and-variance.html#cb76-11"></a><span class="kw">c</span>(mu.n,  sigma2.n,  <span class="kw">sqrt</span>(sigma2.n))</span></code></pre></div>
<pre><code>## [1] 1.81400 0.01532 0.12379</code></pre>
<p>Our posterior belief about <span class="math inline">\((\theta , \sigma^2)\)</span> is described by the joint probability distribution:</p>
<p><span class="math inline">\(\{\theta|y_1,...,y_n,\sigma^2\} \sim \text{Normal}(\mu_n, \sigma^2/\kappa_n) = \text{Normal}(1.814, \sigma^2/10)\)</span></p>
<p><span class="math inline">\(\{1/\sigma^2|y_1,...,y_n\} \sim \text{gamma}(\nu_n/2, \nu_n\sigma^2_n/2)=\text{gamma}(10/2, 10\times 0.015/2)\)</span></p>
<p>
 
</p>
<p><strong>Joint density of <span class="math inline">\((\theta, \sigma^2)\)</span></strong></p>
<p>I want to draw a picture of the joint density <span class="math inline">\(p(\theta, \sigma^2 | y)\)</span> but this is a three-dimensional figure and I have a two-dimensional dimensional monitor on my laptop. There’s lots of ways to do this. The one I happen to like is kind of old fashioned and that is the contour plot.</p>
<p>A <strong>contour plot</strong> is defined by; If <span class="math inline">\(f(x, y)\)</span> is a joint density for random variables <span class="math inline">\((X, Y)\)</span> then find the mode of this density (the peak) and then find all the points <span class="math inline">\((x, y)\)</span> such that <span class="math inline">\(f(x, y) = 0.95 \times f(x.\text{mode}, y.\text{mode})\)</span>. That is all the points where the density is 95% of the peak value. Then draw a line connecting those points. Assuming a unimodal distribution, that will be the innermost contour. Then do this again for 90% of the peak value, and 85% of the peak value down to 0.001% of the peak value.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb78-1"></a><span class="co"># These values arrived at by lots of trial and error</span></span>
<span id="cb78-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb78-2"></a>gs     &lt;-<span class="st"> </span><span class="dv">800</span></span>
<span id="cb78-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb78-3"></a>theta  &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">1.5</span>, <span class="fl">2.1</span>, <span class="dt">length=</span>gs)</span>
<span id="cb78-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb78-4"></a>I.sig2 &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(<span class="dv">1</span>), <span class="kw">log</span>(<span class="dv">250</span>), <span class="dt">length=</span>gs))</span>
<span id="cb78-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb78-5"></a>sigma2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(<span class="dv">1000</span>), <span class="kw">log</span>(<span class="dv">11</span>), <span class="dt">length=</span>gs))</span></code></pre></div>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-1"></a><span class="co"># Do mean and precision first</span></span>
<span id="cb79-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-2"></a>log.post &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, gs, gs); </span>
<span id="cb79-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-3"></a></span>
<span id="cb79-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-4"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>gs){ <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>gs){ log.post[i,j] &lt;-<span class="st"> </span></span>
<span id="cb79-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-5"></a><span class="st">     </span><span class="kw">dnorm</span>(theta[i], mu.n, <span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(I.sig2[j]<span class="op">*</span>kappa.n), <span class="dt">log=</span>T) <span class="op">+</span></span>
<span id="cb79-6"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-6"></a><span class="st">     </span><span class="kw">dgamma</span>(I.sig2[j], nu.n<span class="op">/</span><span class="dv">2</span>, nu.n<span class="op">*</span>sigma2.n<span class="op">/</span><span class="dv">2</span>, <span class="dt">log=</span>T) }}</span>
<span id="cb79-7"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-7"></a></span>
<span id="cb79-8"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-8"></a>maxie    &lt;-<span class="st"> </span><span class="kw">max</span>(log.post)</span>
<span id="cb79-9"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-9"></a>log.post &lt;-<span class="st"> </span>log.post <span class="op">-</span><span class="st"> </span>maxie</span>
<span id="cb79-10"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-10"></a>post.P   &lt;-<span class="st"> </span><span class="kw">exp</span>(log.post)</span>
<span id="cb79-11"><a href="joint-inference-for-normal-mean-and-variance.html#cb79-11"></a><span class="kw">rm</span>(maxie); <span class="kw">rm</span>(log.post)</span></code></pre></div>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-1"></a><span class="co"># Now do mean and variance </span></span>
<span id="cb80-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-2"></a>log.post &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, gs, gs);</span>
<span id="cb80-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-3"></a></span>
<span id="cb80-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-4"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>gs){ <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>gs){ log.post[i,j] &lt;-<span class="st"> </span></span>
<span id="cb80-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-5"></a><span class="st">     </span><span class="kw">dnorm</span>(theta[i], mu.n, <span class="kw">sqrt</span>(sigma2[j]<span class="op">/</span>kappa.n), <span class="dt">log=</span>T) <span class="op">+</span><span class="st"> </span></span>
<span id="cb80-6"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-6"></a><span class="st">     </span><span class="kw">dgamma</span>(<span class="dv">1</span><span class="op">/</span>sigma2[j], nu.n<span class="op">/</span><span class="dv">2</span>, nu.n<span class="op">*</span>sigma2.n<span class="op">/</span><span class="dv">2</span>, <span class="dt">log=</span>T) <span class="op">-</span><span class="st"> </span></span>
<span id="cb80-7"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-7"></a><span class="st">     </span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>(sigma2[j])        }}          </span>
<span id="cb80-8"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-8"></a></span>
<span id="cb80-9"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-9"></a>maxie    &lt;-<span class="st"> </span><span class="kw">max</span>(log.post)</span>
<span id="cb80-10"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-10"></a>log.post &lt;-<span class="st"> </span>log.post <span class="op">-</span><span class="st"> </span>maxie</span>
<span id="cb80-11"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-11"></a>post.V   &lt;-<span class="st"> </span><span class="kw">exp</span>(log.post)</span>
<span id="cb80-12"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-12"></a>contours &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">001</span>, <span class="fl">.01</span>, <span class="kw">seq</span>(.<span class="dv">05</span>, <span class="fl">.95</span>, <span class="fl">.10</span>))</span>
<span id="cb80-13"><a href="joint-inference-for-normal-mean-and-variance.html#cb80-13"></a><span class="kw">rm</span>(maxie);<span class="kw">rm</span>(log.post)</span></code></pre></div>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb81-1"></a><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>),<span class="dt">mgp=</span><span class="kw">c</span>(<span class="fl">1.75</span>,.<span class="dv">75</span>,<span class="dv">0</span>),<span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>,<span class="dv">0</span>),<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb81-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb81-2"></a></span>
<span id="cb81-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb81-3"></a><span class="kw">contour</span>(theta, I.sig2, post.P, <span class="dt">levels=</span>contours, <span class="dt">drawlabels=</span>F, </span>
<span id="cb81-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb81-4"></a>  <span class="dt">xlab=</span>xlab, <span class="dt">ylab=</span>ylab2, <span class="dt">main=</span><span class="st">&quot;Mean and precision&quot;</span>)</span>
<span id="cb81-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb81-5"></a><span class="kw">contour</span>(theta, sigma2, post.V, <span class="dt">levels=</span>contours, <span class="dt">drawlabels=</span>F, </span>
<span id="cb81-6"><a href="joint-inference-for-normal-mean-and-variance.html#cb81-6"></a>  <span class="dt">xlab=</span>xlab, <span class="dt">ylab=</span>ylab1, <span class="dt">main=</span><span class="st">&quot;Mean and variance&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-53"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-53-1.png" alt="Joint posterior distributions of (theta,precision) and (theta, sigma^2)." width="672" />
<p class="caption">
Figure 9.1: Joint posterior distributions of (theta,precision) and (theta, sigma^2).
</p>
</div>
<p>The posterior mode of (mean, precision) <span class="math inline">\((\theta, 1/\sigma^{2} )\)</span> is at (1.8 and 60) or so. The innermost contour is the set of all points whose joint density is .95 times that peak value. The next contour shows all the points whose joint density is .85 times the max value. There should be 12 contours in this picture. They are the .95, .85, … .15, .05, .01, .001. This is the equivalent of Fig 5.4 in the book but the author doesn’t show as much as I have here there’s no .001 contour. I always like to see the tails.</p>
<p>What do we know about this distribution? We know that conditional on <span class="math inline">\(\sigma^2, ~ \theta\)</span> is Normal. What that means is that every horizontal slice from this joint distribution is a bell curve. Remember that if <span class="math inline">\(f(x,y)\)</span> is the joint density of <span class="math inline">\((X,Y)\)</span> the conditional density of <span class="math inline">\(Y | X=x_0\)</span> is found by taking the vertical slice of the joint density at the point <span class="math inline">\(x=x_0\)</span>. Similarly, I’ve got in this picture the joint posterior of <span class="math inline">\((\theta, \sigma^{-2})\)</span> the conditional of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(\sigma^2\)</span> is a horizontal slice. Given that the shape makes perfect sense. The bigger is <span class="math inline">\(\sigma^2\)</span> (smaller is <span class="math inline">\(1/\sigma^2\)</span>) the weaker is our belief about <span class="math inline">\(\theta\)</span>, hence the wideness of the LHS plot. The smaller is <span class="math inline">\(\sigma^2\)</span> (the bigger is <span class="math inline">\(1/\sigma^2\)</span>) the stronger our belief about <span class="math inline">\(\theta\)</span>, hence the peak on the LHS plot. What the belief is does not depend on <span class="math inline">\(\sigma^2\)</span> that belief in “<span class="math inline">\(\theta\)</span> is about 1.805 or so”.</p>
<p><strong>Notes on the code</strong></p>
<p>I did <span class="math inline">\(800 \times 800\)</span> calculations of the joint density.</p>
<p>For numerical reasons it is a good practice to compute log-densities then subtract the max log-density off of every value THEN exponentiate back.</p>
<p><tt>post.P</tt> represents joint posterior of mean and precision</p>
<p><tt>post.V</tt> is the joint posterior of mean and variance</p>
<p>The calculation is;</p>
<p><span class="math inline">\(p(\theta, \sigma^2) = p(\theta | \sigma^2, y) p(\sigma^2 | y)\)</span> or <span class="math inline">\(\log[ p(\theta, \sigma^2 | y) ] = \log[ p(\theta | \sigma^2, y) ] + \log[ p(\sigma^2 | y) ]\)</span></p>
<p>Question: why do we need to subtract off <span class="math inline">\(2 \log(\sigma^2)?\)</span> in the calculation of <tt>post.V</tt>?</p>
<p>Let <span class="math inline">\(V =\)</span> variance, <span class="math inline">\(P =\)</span> precision, <span class="math inline">\(V = 1/P\)</span>, <span class="math inline">\(P = 1/V\)</span>. The random variable <span class="math inline">\(P\)</span> has a gamma distribution the random variable <span class="math inline">\(V\)</span> has an inverse-gamma distribution. The probability density of <span class="math inline">\(V\)</span> is the gamma pdf evaluated at <span class="math inline">\(1/v \times 1/v^2\)</span> because that’s the Jacobian. Look at “nonlinear transformations of random variables” from probability theory for more insight.</p>
</div>
<div id="monte-carlo-sampling" class="section level2">
<h2><span class="header-section-number">9.3</span> Monte Carlo sampling</h2>
<p>Of course, In the above problem the calculations were not easy
but possible. But going forward when we get to the really messy problems we’re gonna have no choice but to do Monte Carlo. I want a Monte carlo sample that is <span class="math inline">\(\theta^{(1)} , ..., \theta^{(S)}\)</span> such that <span class="math inline">\(\theta^{(s)} \sim p(\theta |y)\)</span>. The problem is; I don’t know the marginal posterior of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(p(\theta | y).\)</span> I know <span class="math inline">\(p(\theta | y, \sigma^2)\)</span> and I know <span class="math inline">\(p(\sigma^2 | y)\)</span>, and that’s enough. It just means each simulation is gonna require two steps. First simulate <span class="math inline">\(\sigma^{2(s)}\)</span> and then simulate <span class="math inline">\(\theta^{(s)} \sim p(\theta |y, \sigma^{2(s)}).\)</span> The result is <span class="math inline">\((\sigma^{2(s)} , \theta^{(s)}) \sim p(\theta , \sigma^2 | y)\)</span> which means marginally <span class="math inline">\(\theta^{(s)} \sim p(\theta | y).\)</span></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb82-1"></a>S          &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb82-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb82-2"></a>sigma2.sim &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">rgamma</span>(S, nu.n<span class="op">/</span><span class="dv">2</span>, nu.n<span class="op">*</span>sigma2.n<span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb82-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb82-3"></a>theta.sim  &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, mu.n, <span class="kw">sqrt</span>(sigma2.sim<span class="op">/</span>kappa.n))</span></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb83-1"></a><span class="co"># Scatterplot; empirical joint distribution of MC sample</span></span>
<span id="cb83-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb83-2"></a><span class="kw">contour</span>(theta, sigma2, post.V, <span class="dt">levels=</span>contours, <span class="dt">drawlabels=</span>F, </span>
<span id="cb83-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb83-3"></a>  <span class="dt">xlab=</span>xlab, <span class="dt">ylab=</span>ylab1, <span class="dt">main=</span><span class="st">&quot;Mean and variance&quot;</span>)</span>
<span id="cb83-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb83-4"></a><span class="kw">points</span>(theta.sim, sigma2.sim, <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">25</span>, </span>
<span id="cb83-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb83-5"></a>  <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">1.51</span>, <span class="fl">2.11</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">.10</span>))</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-55-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb84-1"></a><span class="co"># Marginal density estimates, and 95% CI for theta</span></span>
<span id="cb84-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb84-2"></a>(CI.theta &lt;-<span class="st"> </span><span class="kw">quantile</span>(theta.sim, <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>))) </span></code></pre></div>
<pre><code>##  2.5% 97.5% 
## 1.727 1.901</code></pre>
<p>I actually know the theoretical marginal distribution of <span class="math inline">\(\sigma^2\)</span>. But if I didn’t I could draw a histogram of the Monte Carlo sample or a kernel density estimate based on the monte carlo samples as below.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-1"></a><span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">1</span>),<span class="dt">mgp=</span><span class="kw">c</span>(<span class="fl">1.75</span>,.<span class="dv">75</span>,<span class="dv">0</span>),<span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,.<span class="dv">5</span>,<span class="dv">0</span>),<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb86-2"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-2"></a></span>
<span id="cb86-3"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-3"></a><span class="kw">hist</span>(sigma2.sim, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">.08</span>), <span class="dt">main=</span><span class="st">&quot;&quot;</span>, <span class="dt">freq=</span>F, <span class="dt">xlab=</span>ylab1,</span>
<span id="cb86-4"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-4"></a>     <span class="dt">ylab=</span>ylab3, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">60</span>), <span class="dt">border=</span><span class="st">&quot;lightpink1&quot;</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>)</span>
<span id="cb86-5"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-5"></a><span class="kw">lines</span>(<span class="kw">density</span>(sigma2.sim), <span class="dt">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb86-6"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-6"></a></span>
<span id="cb86-7"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-7"></a><span class="kw">hist</span>(theta.sim, <span class="dt">freq=</span>F, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="fl">1.6</span>, <span class="fl">2.0</span>), <span class="dt">main=</span><span class="st">&quot;&quot;</span>, <span class="dt">xlab=</span>xlab,</span>
<span id="cb86-8"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-8"></a>      <span class="dt">ylab=</span>ylab4, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>), <span class="dt">border=</span><span class="st">&quot;lightpink1&quot;</span>,<span class="dt">col=</span><span class="st">&quot;pink&quot;</span>)</span>
<span id="cb86-9"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-9"></a><span class="kw">lines</span>(<span class="kw">density</span>(theta.sim), <span class="dt">lwd=</span><span class="dv">2</span>)</span>
<span id="cb86-10"><a href="joint-inference-for-normal-mean-and-variance.html#cb86-10"></a><span class="kw">abline</span>(<span class="dt">v=</span>CI.theta, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-57-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The 95% Bayesian confidence interval is <span class="math inline">\([1.72, 1.90]\)</span>.</p>
<p>In the last lecture, when we were pretending <span class="math inline">\(\sigma^2\)</span> was known, the CI was <span class="math inline">\([1.720, 1.889]\)</span>. Furthermore, the frequentist interval based on the t-distribution is <span class="math inline">\([1.70, 1.90]\)</span>.</p>
<p>Our Bayesian interval brings the lower bound up just a tad (because the prior mean was 1.9 vs data mean of 1.8 or so) and it shortens the CI a bit because our prior counts for one observation so our “Bayesian sample size” was 10 not just 9.</p>
</div>
<div id="summary-of-normal-formulas" class="section level2">
<h2><span class="header-section-number">9.4</span> Summary of Normal formulas</h2>
<p><span class="math inline">\(p(\theta,\sigma^2 | y_1,...,y_n) \propto p(\theta,\sigma^2) p(y_1,...,y_n| \theta,\sigma^2)\)</span></p>
<p><span class="math inline">\(p(\theta,\sigma^2)=p(\theta|\sigma^2)p(\sigma^2) = \texttt{dnorm} (\theta,\mu_0,\tau_0 = \sigma/\sqrt{\kappa_0})p(\sigma^2)\)</span></p>
<p><span class="math inline">\(1/\sigma^2 \sim\)</span> gamma<span class="math inline">\((\nu_0/2, \nu_0 \sigma_0^2/2)\)</span></p>
<p><span class="math inline">\(\{\theta|\sigma^2\} \sim\)</span> normal<span class="math inline">\((\mu_0, \sigma^2/\kappa_0) \equiv \text{normal}(\mu_0, \tau_0^2)\)</span></p>
<p>if <span class="math inline">\(\{Y_1,\ldots,Y_n\} \sim\)</span> i.i.d. normal<span class="math inline">\((\theta,\sigma^2)\)</span> then</p>
<p><span class="math inline">\(\{1/\sigma^2|y_1,...,y_n\} \sim \text{gamma}(\nu_n/2, \nu_n \sigma^2_n/2) \equiv\sigma^2_{mc} \sim 1 / \texttt{rgamma}(S, \nu_n/2, \nu_n \sigma^2_n/2)\)</span> where <span class="math inline">\(\nu_n = \nu_0 + n\)</span></p>
<p><span class="math inline">\(\{ \theta|y_1,...,y_n, \sigma^2\} \sim \text{normal}(\mu_n, \sigma^2 / \kappa_n) \equiv \theta_{mc}\sim\texttt{rnorm}(S, \mu_n, \sqrt{\sigma^2_{mc}/\kappa_n})\)</span> where <span class="math inline">\(\kappa_n = \kappa_0 + n\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mu_{n} &amp;= \frac{1/{\tau}_{0}^{2}}{1/{\tau}_{0}^{2}+n/{\sigma}^{2}} \mu_{0}+\frac{n/{\sigma}^{2}}{1/{\tau}_{0}^{2}+n /{\sigma}^{2}} \bar{y}\\[0.3cm]
\text{ if }\tau_0^2=\sigma^2/\kappa_0, ~~\mu_n &amp;= \frac{\kappa_{0}}{\kappa_{0}+n} \mu_{0}+\frac{n}{\kappa_{0}+n} \bar{y} = \frac{\kappa_0\mu_0+n\bar y}{\kappa_n}\\
\frac{1}{\tau_n^2} &amp;= \frac{1}{\tau_0^2}+\frac{n}{\sigma^2}\\[0.3cm]
\end{aligned}
\]</span></p>
<p><span class="math display">\[\sigma_n^2 = \frac{1}{\nu_n}[\nu_0\sigma_0^2 + (n-1)s^2+\frac{\kappa_0n} {\kappa_n}(\bar y - \mu_0)^2]\]</span></p>
<p><span class="math inline">\(\{\tilde Y|\sigma^2, y_1,...,y_n\} \sim \text{normal}(\mu_n,\tau_n^2+\sigma^2) \equiv \texttt{rnorm}(S, \theta_{mc}, \sqrt{\sigma^2_{mc}})\)</span></p>
<p>
 
</p>
<p><span class="math inline">\(\mu_0 \text{ and }\kappa_0\)</span> is the mean and sample size from a prior set of observations.</p>
<p><span class="math inline">\(\nu_0\)</span> prior sample size, from which a prior sample variance <span class="math inline">\(\sigma_0^2\)</span> has been obtained.</p>
<p><span class="math inline">\(s^2 = \sum_{i=1}^n (y_i - \bar y)/(n-1) = \texttt{var}(\boldsymbol{y})\)</span> sample variance</p>
<p><span class="math inline">\((n-1)s^2\)</span> is the sum of squared observations from the sample mean</p>
<p><span class="math inline">\(\nu_0 \sigma_0^2 \text{ and } \nu_n\sigma_n^2\)</span> as prior and posterior sum of squares, respectively.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="normal-mean.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gibbs-sampler.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesianS21.pdf", "bayesianS21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
