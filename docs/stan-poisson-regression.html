<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 20 Stan; Poisson regression | Bayesian Statistics lecture notes</title>
  <meta name="description" content="Lecture 20 Stan; Poisson regression | Bayesian Statistics lecture notes" />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 20 Stan; Poisson regression | Bayesian Statistics lecture notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 20 Stan; Poisson regression | Bayesian Statistics lecture notes" />
  
  
  

<meta name="author" content="Chisom Onyishi" />


<meta name="date" content="2021-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-mixed-effects-models-aka-hierarchical-linear-models.html"/>
<link rel="next" href="summary.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian statistics notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Belief and Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i><b>1.1</b> Example:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#binomial"><i class="fa fa-check"></i><b>1.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.4</b> Poisson distribution</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#example-normalmu-10.75-sigma0.8"><i class="fa fa-check"></i><b>1.5.1</b> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exchangeability.html"><a href="exchangeability.html"><i class="fa fa-check"></i><b>2</b> Exchangeability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exchangeability.html"><a href="exchangeability.html#discrete-joint-distributions"><i class="fa fa-check"></i><b>2.1</b> Discrete joint distributions</a></li>
<li class="chapter" data-level="2.2" data-path="exchangeability.html"><a href="exchangeability.html#bayesrule"><i class="fa fa-check"></i><b>2.2</b> Bayes’ rule and parameter estimation</a></li>
<li class="chapter" data-level="2.3" data-path="exchangeability.html"><a href="exchangeability.html#independent-random-variables"><i class="fa fa-check"></i><b>2.3</b> Independent random variables</a></li>
<li class="chapter" data-level="2.4" data-path="exchangeability.html"><a href="exchangeability.html#exchangeability-1"><i class="fa fa-check"></i><b>2.4</b> Exchangeability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binomial-1.html"><a href="binomial-1.html"><i class="fa fa-check"></i><b>3</b> Binomial</a>
<ul>
<li class="chapter" data-level="3.1" data-path="binomial-1.html"><a href="binomial-1.html#example---exchangeable-binary-data"><i class="fa fa-check"></i><b>3.1</b> Example - Exchangeable binary data</a></li>
<li class="chapter" data-level="3.2" data-path="binomial-1.html"><a href="binomial-1.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.2</b> The beta distribution</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="binomial-1.html"><a href="binomial-1.html#properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Properties of the beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="binomial-1.html"><a href="binomial-1.html#binomial-distribution"><i class="fa fa-check"></i><b>3.3</b> Binomial distribution</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="binomial-1.html"><a href="binomial-1.html#posterior-inference-for-a-binomial-sampling-model"><i class="fa fa-check"></i><b>3.3.1</b> Posterior inference for a binomial sampling model</a></li>
<li class="chapter" data-level="3.3.2" data-path="binomial-1.html"><a href="binomial-1.html#conjugacy"><i class="fa fa-check"></i><b>3.3.2</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="binomial-1.html"><a href="binomial-1.html#combining-information"><i class="fa fa-check"></i><b>3.4</b> Combining information</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="binomial-1.html"><a href="binomial-1.html#example-1"><i class="fa fa-check"></i><b>3.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="binomial-1.html"><a href="binomial-1.html#prediction"><i class="fa fa-check"></i><b>3.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#how-do-we-compute-intervals"><i class="fa fa-check"></i><b>4.2</b> How do we compute intervals?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="CI.html"><a href="CI.html#example-2"><i class="fa fa-check"></i><b>4.2.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>5</b> Poisson model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="poisson-model.html"><a href="poisson-model.html#posterior-inference-for-the-poisson-model"><i class="fa fa-check"></i><b>5.1</b> Posterior inference for the Poisson model</a></li>
<li class="chapter" data-level="5.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>5.2</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="5.3" data-path="poisson-model.html"><a href="poisson-model.html#example-birth-rates"><i class="fa fa-check"></i><b>5.3</b> Example: Birth rates</a></li>
<li class="chapter" data-level="5.4" data-path="poisson-model.html"><a href="poisson-model.html#explaining-the-parameters-of-the-gamma-distribution"><i class="fa fa-check"></i><b>5.4</b> Explaining the parameters of the gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo</a>
<ul>
<li class="chapter" data-level="6.1" data-path="monte-carlo.html"><a href="monte-carlo.html#example-birth-rate"><i class="fa fa-check"></i><b>6.1</b> Example: birth rate</a></li>
<li class="chapter" data-level="6.2" data-path="monte-carlo.html"><a href="monte-carlo.html#the-monte-carlo-method"><i class="fa fa-check"></i><b>6.2</b> The Monte Carlo method</a></li>
<li class="chapter" data-level="6.3" data-path="monte-carlo.html"><a href="monte-carlo.html#example-numerical-evaluation"><i class="fa fa-check"></i><b>6.3</b> Example: Numerical evaluation</a></li>
<li class="chapter" data-level="6.4" data-path="monte-carlo.html"><a href="monte-carlo.html#posterior-inference-for-arbitrary-functions"><i class="fa fa-check"></i><b>6.4</b> Posterior inference for arbitrary functions</a></li>
<li class="chapter" data-level="6.5" data-path="monte-carlo.html"><a href="monte-carlo.html#example-log-odds"><i class="fa fa-check"></i><b>6.5</b> Example: Log-odds</a></li>
<li class="chapter" data-level="6.6" data-path="monte-carlo.html"><a href="monte-carlo.html#example-functions-of-two-parameters"><i class="fa fa-check"></i><b>6.6</b> Example: Functions of two parameters</a></li>
<li class="chapter" data-level="6.7" data-path="monte-carlo.html"><a href="monte-carlo.html#how-many-monte-carlo-samples-are-needed"><i class="fa fa-check"></i><b>6.7</b> How many Monte Carlo samples are needed?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predictive.html"><a href="predictive.html"><i class="fa fa-check"></i><b>7</b> Predictive</a>
<ul>
<li class="chapter" data-level="7.1" data-path="predictive.html"><a href="predictive.html#sampling-for-predictive-distribution"><i class="fa fa-check"></i><b>7.1</b> Sampling for predictive distribution</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="predictive.html"><a href="predictive.html#example-birth-rate-1"><i class="fa fa-check"></i><b>7.1.1</b> Example: birth rate</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="predictive.html"><a href="predictive.html#example-let-d-tilde-y_1---tilde-y_2"><i class="fa fa-check"></i><b>7.2</b> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>7.3</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="7.4" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking-1"><i class="fa fa-check"></i><b>7.4</b> Posterior predictive model checking</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normal-mean.html"><a href="normal-mean.html"><i class="fa fa-check"></i><b>8</b> Normal Mean</a>
<ul>
<li class="chapter" data-level="8.1" data-path="normal-mean.html"><a href="normal-mean.html#example-womens-height"><i class="fa fa-check"></i><b>8.1</b> Example: women’s height</a></li>
<li class="chapter" data-level="8.2" data-path="normal-mean.html"><a href="normal-mean.html#inference-for-the-mean-conditional-on-the-variance"><i class="fa fa-check"></i><b>8.2</b> Inference for the mean, conditional on the variance</a></li>
<li class="chapter" data-level="8.3" data-path="normal-mean.html"><a href="normal-mean.html#prediction-1"><i class="fa fa-check"></i><b>8.3</b> Prediction</a></li>
<li class="chapter" data-level="8.4" data-path="normal-mean.html"><a href="normal-mean.html#example-midge-wing-length"><i class="fa fa-check"></i><b>8.4</b> Example: Midge wing length</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html"><i class="fa fa-check"></i><b>9</b> Joint inference for Normal mean and variance</a>
<ul>
<li class="chapter" data-level="9.1" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#marginal-posterior-of-sigma2"><i class="fa fa-check"></i><b>9.1</b> Marginal posterior of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#example-midge-wing-length-1"><i class="fa fa-check"></i><b>9.2</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="9.3" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>9.3</b> Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.4" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#summary-of-normal-formulas"><i class="fa fa-check"></i><b>9.4</b> Summary of Normal formulas</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html"><i class="fa fa-check"></i><b>10</b> Gibbs sampler</a>
<ul>
<li class="chapter" data-level="10.1" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#review-of-conjugate-prior-for-normal-model"><i class="fa fa-check"></i><b>10.1</b> Review of conjugate prior for normal model</a></li>
<li class="chapter" data-level="10.2" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#a-semiconjugate-prior-distribution"><i class="fa fa-check"></i><b>10.2</b> A semiconjugate prior distribution</a></li>
<li class="chapter" data-level="10.3" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.3</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.4" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-midge-wing-length-2"><i class="fa fa-check"></i><b>10.4</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="10.5" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#discrete-approximation-of-posterior-distribution"><i class="fa fa-check"></i><b>10.5</b> Discrete approximation of posterior distribution</a></li>
<li class="chapter" data-level="10.6" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-3"><i class="fa fa-check"></i><b>10.6</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>11</b> MCMC diagnostics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#the-gibbs-sampler"><i class="fa fa-check"></i><b>11.1</b> The Gibbs sampler</a></li>
<li class="chapter" data-level="11.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#distinguishing-estimation-from-approximation"><i class="fa fa-check"></i><b>11.2</b> Distinguishing estimation from approximation</a></li>
<li class="chapter" data-level="11.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#introduction-to-mcmc-diagnostics"><i class="fa fa-check"></i><b>11.3</b> Introduction to MCMC diagnostics</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#example-mixture-of-normal-densities"><i class="fa fa-check"></i><b>11.3.1</b> Example: mixture of normal densities</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#discussion"><i class="fa fa-check"></i><b>11.4</b> Discussion</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#how-does-autocorrelationslow-mixing-affect-our-mcmc-approximation"><i class="fa fa-check"></i><b>11.4.1</b> How does autocorrelation(slow mixing) affect our MCMC approximation?</a></li>
<li class="chapter" data-level="11.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation"><i class="fa fa-check"></i><b>11.4.2</b> Autocorrelation</a></li>
<li class="chapter" data-level="11.4.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#sample-autocorrelation-function"><i class="fa fa-check"></i><b>11.4.3</b> Sample autocorrelation function</a></li>
<li class="chapter" data-level="11.4.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>11.4.4</b> Effective sample size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multivariate-normal.html"><a href="multivariate-normal.html"><i class="fa fa-check"></i><b>12</b> Multivariate Normal</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multivariate-normal.html"><a href="multivariate-normal.html#example-reading-comprehension"><i class="fa fa-check"></i><b>12.1</b> Example: Reading comprehension</a></li>
<li class="chapter" data-level="12.2" data-path="multivariate-normal.html"><a href="multivariate-normal.html#the-multivariate-normal-density"><i class="fa fa-check"></i><b>12.2</b> The multivariate normal density</a></li>
<li class="chapter" data-level="12.3" data-path="multivariate-normal.html"><a href="multivariate-normal.html#a-semiconjugate-prior-distribution-for-the-mean"><i class="fa fa-check"></i><b>12.3</b> A semiconjugate prior distribution for the mean</a></li>
<li class="chapter" data-level="12.4" data-path="multivariate-normal.html"><a href="multivariate-normal.html#the-inverse-wishart-distribution"><i class="fa fa-check"></i><b>12.4</b> The inverse-Wishart distribution</a></li>
<li class="chapter" data-level="12.5" data-path="multivariate-normal.html"><a href="multivariate-normal.html#full-conditional-distribution-of-the-covariance-matrix"><i class="fa fa-check"></i><b>12.5</b> Full conditional distribution of the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>13</b> Group comparisons</a>
<ul>
<li class="chapter" data-level="13.1" data-path="group-comparisons.html"><a href="group-comparisons.html#comparing-two-groups"><i class="fa fa-check"></i><b>13.1</b> Comparing two groups</a></li>
<li class="chapter" data-level="13.2" data-path="group-comparisons.html"><a href="group-comparisons.html#mathex1"><i class="fa fa-check"></i><b>13.2</b> Example: Math scores data</a></li>
<li class="chapter" data-level="13.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesmodel"><i class="fa fa-check"></i><b>13.3</b> A Bayesian model</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="group-comparisons.html"><a href="group-comparisons.html#analysis-of-the-math-scores-data"><i class="fa fa-check"></i><b>13.3.1</b> Analysis of the math scores data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html"><i class="fa fa-check"></i><b>14</b> The hierarchical normal model</a>
<ul>
<li class="chapter" data-level="14.1" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#postinf"><i class="fa fa-check"></i><b>14.1</b> Posterior inference</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-distributions-of-mu-and-tau2"><i class="fa fa-check"></i><b>14.1.1</b> Full conditional distributions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span></a></li>
<li class="chapter" data-level="14.1.2" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-of-theta_j"><i class="fa fa-check"></i><b>14.1.2</b> Full conditional of <span class="math inline">\(\theta_j\)</span></a></li>
<li class="chapter" data-level="14.1.3" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-of-sigma2"><i class="fa fa-check"></i><b>14.1.3</b> Full conditional of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#mathex2"><i class="fa fa-check"></i><b>14.2</b> Example: Math scores in U.S. public schools</a></li>
<li class="chapter" data-level="14.3" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#posterior-approximation"><i class="fa fa-check"></i><b>14.3</b> Posterior approximation</a></li>
<li class="chapter" data-level="14.4" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#mcmc-diagnostics-1"><i class="fa fa-check"></i><b>14.4</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="14.5" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#shrinkage"><i class="fa fa-check"></i><b>14.5</b> Shrinkage</a></li>
<li class="chapter" data-level="14.6" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#ranking-the-groups"><i class="fa fa-check"></i><b>14.6</b> Ranking the groups</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>15</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="linear-regression.html"><a href="linear-regression.html#example-oxygen-uptake"><i class="fa fa-check"></i><b>15.1</b> Example: Oxygen uptake</a></li>
<li class="chapter" data-level="15.2" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>15.2</b> Least squares estimation</a></li>
<li class="chapter" data-level="15.3" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimation-for-oxygen-uptake-data"><i class="fa fa-check"></i><b>15.3</b> Least squares estimation for oxygen uptake data</a></li>
<li class="chapter" data-level="15.4" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-estimation-for-a-regression-model"><i class="fa fa-check"></i><b>15.4</b> Bayesian estimation for a regression model</a></li>
<li class="chapter" data-level="15.5" data-path="linear-regression.html"><a href="linear-regression.html#unit-information-prior"><i class="fa fa-check"></i><b>15.5</b> Unit information prior</a></li>
<li class="chapter" data-level="15.6" data-path="linear-regression.html"><a href="linear-regression.html#zellners-g-prior"><i class="fa fa-check"></i><b>15.6</b> Zellner’s <span class="math inline">\(g\)</span>-prior</a></li>
<li class="chapter" data-level="15.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-analysis-using-invariant-g-prior"><i class="fa fa-check"></i><b>15.7</b> Bayesian analysis using invariant <span class="math inline">\(g\)</span>-prior</a></li>
<li class="chapter" data-level="15.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-analysis-using-semiconjugate-prior"><i class="fa fa-check"></i><b>15.8</b> Bayesian analysis using semiconjugate prior</a>
<ul>
<li class="chapter" data-level="15.8.1" data-path="linear-regression.html"><a href="linear-regression.html#prediction-problem"><i class="fa fa-check"></i><b>15.8.1</b> Prediction problem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>16</b> Model Selection</a>
<ul>
<li class="chapter" data-level="16.1" data-path="model-selection.html"><a href="model-selection.html#review"><i class="fa fa-check"></i><b>16.1</b> Review</a></li>
<li class="chapter" data-level="16.2" data-path="model-selection.html"><a href="model-selection.html#bayesian-model-comparison"><i class="fa fa-check"></i><b>16.2</b> Bayesian model comparison</a></li>
<li class="chapter" data-level="16.3" data-path="model-selection.html"><a href="model-selection.html#example-oxygen-uptake-1"><i class="fa fa-check"></i><b>16.3</b> Example: Oxygen uptake</a></li>
<li class="chapter" data-level="16.4" data-path="model-selection.html"><a href="model-selection.html#gibbs-sampling-and-model-averaging"><i class="fa fa-check"></i><b>16.4</b> Gibbs sampling and model averaging</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html"><i class="fa fa-check"></i><b>17</b> Generalized Linear Models; the Metropolis Algorithm</a>
<ul>
<li class="chapter" data-level="17.1" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#example-song-sparrow-reproductive-success"><i class="fa fa-check"></i><b>17.1</b> Example: Song sparrow reproductive success</a></li>
<li class="chapter" data-level="17.2" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#sec:poisson"><i class="fa fa-check"></i><b>17.2</b> Poisson regression</a></li>
<li class="chapter" data-level="17.3" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#logistic-regression"><i class="fa fa-check"></i><b>17.3</b> Logistic regression</a></li>
<li class="chapter" data-level="17.4" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#posterior-approximations"><i class="fa fa-check"></i><b>17.4</b> Posterior approximations</a></li>
<li class="chapter" data-level="17.5" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>17.5</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="17.6" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#example-normal-distribution-with-known-variance"><i class="fa fa-check"></i><b>17.6</b> Example: Normal distribution with known variance</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#output-of-metropolis-algorithm"><i class="fa fa-check"></i><b>17.6.1</b> Output of Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#the-metropolis-algorithm-for-poisson-regression-secmetpois"><i class="fa fa-check"></i><b>17.7</b> The Metropolis algorithm for Poisson regression {sec:metpois}</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html"><i class="fa fa-check"></i><b>18</b> Metropolis-Hastings</a>
<ul>
<li class="chapter" data-level="18.1" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#why-does-the-metropolis-hastings-algorithm-work"><i class="fa fa-check"></i><b>18.1</b> Why does the Metropolis-Hastings algorithm work?</a></li>
<li class="chapter" data-level="18.2" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#combining-the-metropolis-and-gibbs-algorithms"><i class="fa fa-check"></i><b>18.2</b> Combining the Metropolis and Gibbs algorithms</a></li>
<li class="chapter" data-level="18.3" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#example-historical-co_2-and-temperature-data"><i class="fa fa-check"></i><b>18.3</b> Example: Historical CO<span class="math inline">\(_2\)</span> and temperature data</a></li>
<li class="chapter" data-level="18.4" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#a-regression-model-with-correlated-errors"><i class="fa fa-check"></i><b>18.4</b> A regression model with correlated errors</a></li>
<li class="chapter" data-level="18.5" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#analysis-of-the-ice-core-data"><i class="fa fa-check"></i><b>18.5</b> Analysis of the ice core data</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Mixed-effects Models, aka, Hierarchical Linear Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#hierarchical-model-review"><i class="fa fa-check"></i><b>19.1</b> Hierarchical model review</a></li>
<li class="chapter" data-level="19.2" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#hierarchical-linear-regression-model-for-math-scores-data"><i class="fa fa-check"></i><b>19.2</b> Hierarchical linear regression model for math scores data</a></li>
<li class="chapter" data-level="19.3" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#bayesian-hierarchical-linear-regression-model"><i class="fa fa-check"></i><b>19.3</b> Bayesian hierarchical linear regression model</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#full-conditionals"><i class="fa fa-check"></i><b>19.3.1</b> Full conditionals</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#bayesian-analysis-of-the-math-scores-data"><i class="fa fa-check"></i><b>19.4</b> Bayesian analysis of the math scores data</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#mcmc-diagnostics-2"><i class="fa fa-check"></i><b>19.4.1</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#posterior-summaries"><i class="fa fa-check"></i><b>19.4.2</b> Posterior summaries</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#posterior-predictive-simulation"><i class="fa fa-check"></i><b>19.4.3</b> Posterior predictive simulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="stan-poisson-regression.html"><a href="stan-poisson-regression.html"><i class="fa fa-check"></i><b>20</b> Stan; Poisson regression</a>
<ul>
<li class="chapter" data-level="20.1" data-path="stan-poisson-regression.html"><a href="stan-poisson-regression.html#intro-to-stan"><i class="fa fa-check"></i><b>20.1</b> Intro to Stan</a></li>
<li class="chapter" data-level="20.2" data-path="stan-poisson-regression.html"><a href="stan-poisson-regression.html#song-sparrows-reproductive-success-example"><i class="fa fa-check"></i><b>20.2</b> Song sparrows reproductive success example</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>21</b> Summary</a>
<ul>
<li class="chapter" data-level="21.1" data-path="summary.html"><a href="summary.html#missing-data-and-imputation-7b"><i class="fa fa-check"></i><b>21.1</b> Missing data and imputation; 7b</a></li>
<li class="chapter" data-level="21.2" data-path="summary.html"><a href="summary.html#generalized-linear-mixed-eﬀects-models-11b"><i class="fa fa-check"></i><b>21.2</b> Generalized linear mixed eﬀects models; 11b</a></li>
<li class="chapter" data-level="21.3" data-path="summary.html"><a href="summary.html#improper-priors"><i class="fa fa-check"></i><b>21.3</b> Improper priors</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics lecture notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stan-poisson-regression" class="section level1" number="20">
<h1><span class="header-section-number">Lecture 20</span> Stan; Poisson regression</h1>
<p><tt>The following notes are mostly transcribed from Neath(0607,2021) lecture.</tt></p>
<p>
 
</p>
<div id="intro-to-stan" class="section level2" number="20.1">
<h2><span class="header-section-number">20.1</span> Intro to Stan</h2>
<p>Do you agree with this statement: Bayesian statistics is cool?</p>
<p>Do you agree with this statement: Bayesian model-fitting is a lot more work than just fitting likelihood methods using <span class="math inline">\(\texttt{lm()}\)</span> and <span class="math inline">\(\texttt{glm()}\)</span> and such?</p>
<p>In recent years (beginning around 2000 or so) there is an effort under way to change this. The question is: Is it possible to create user-friendly software with reasonable defaults that will allow users who aren’t programming experts to still fit such models? The first serious effort in this movement was a package called BUGS “Bayesian Inference using Gibbs Sampler.” There’s been others such as JAGS “just another Gibbs sampler” and NIMBLE (created and maintained by a group at UC Berkeley).</p>
<p>The current state of the art among such programs is a program called Stan. It’s very general and very powerful. The output of Stan is a set of simulations from a posterior distribution in other words <em>everything we need</em> to do Bayesian inference. As an instructor for a course in Bayesian Statistics, there’s a question I struggle with a lot in the same way that if I were teaching Calculus, a question I would struggle with is do I allow graphing calculators? The art of Calculus is learning how to analyze functions from studying but how much is that relevant today when we hold machines in the palm of our hands that do those things for us? As an instructor for Bayesian Statistics, I want for us to get a good understanding of bayesian statistical models and how the simulation is done.</p>
<p>Is Stan user-friendly? Simple? No. It takes a considerable investment of effort to become a proficient Stan user.</p>
<p>The big news for today is that for pretty much everything we’ve done in this course there’s an easier way Or at least a way that is potentially easier if you’re willing to invest some effort in becoming proficient in Stan. I am more than willing to invest the effort to become proficient. I just haven’t yet. Some of you are not going to be doing much of these types of analyses after this course is over. For those of you who are it pays to become proficient in Stan.</p>
<p>How does Stan work? You have to input the data (obviously), you have to describe your model to stan e.g., the prior, the sampling models etc. But that’s it. You do not have to derive a posterior distribution. You do not have to try a whole bunch of different values of tuning parameters in your Metropolis samplers. Stan effectively does that stuff for you.</p>
<p>What’s the down side? You gotta learn it. It’s a new language. On this front the situation is better today than even just a few years ago in that it runs quite nicely within R and not just R, there’s Python Stan and others.</p>
<p>Stan was developed at and is maintained at Columbia! Professor Gelman is the project leader. There are a whole community of contributors and there are 3 or 4 or 5 or so full time employees of Stan. They regularly meet in Watson Hall and if you become a Stan user you become part of this community and you can pose questions to these people and participate in different conferences and activities and such. Where Stan is lagging right now; there does not exist at this time at a level appropriate for our course a textbook with a large number of worked Stan examples. But there very likely will in a couple of years.</p>
</div>
<div id="song-sparrows-reproductive-success-example" class="section level2" number="20.2">
<h2><span class="header-section-number">20.2</span> Song sparrows reproductive success example</h2>
<p>Remember the Poisson regression problem we did in section <a href="generalized-linear-models-the-metropolis-algorithm.html#sec:poisson">17.2</a>.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="stan-poisson-regression.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Section 10.1 of Hoff (2009)</span></span>
<span id="cb349-2"><a href="stan-poisson-regression.html#cb349-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 52 birds, response is &#39;fledged&#39; -- number of offspring</span></span>
<span id="cb349-3"><a href="stan-poisson-regression.html#cb349-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictor variable is age in years (1 to 5)</span></span>
<span id="cb349-4"><a href="stan-poisson-regression.html#cb349-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-5"><a href="stan-poisson-regression.html#cb349-5" aria-hidden="true" tabindex="-1"></a>fledged <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, </span>
<span id="cb349-6"><a href="stan-poisson-regression.html#cb349-6" aria-hidden="true" tabindex="-1"></a>  <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>, </span>
<span id="cb349-7"><a href="stan-poisson-regression.html#cb349-7" aria-hidden="true" tabindex="-1"></a>  <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb349-8"><a href="stan-poisson-regression.html#cb349-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb349-9"><a href="stan-poisson-regression.html#cb349-9" aria-hidden="true" tabindex="-1"></a>age <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, </span>
<span id="cb349-10"><a href="stan-poisson-regression.html#cb349-10" aria-hidden="true" tabindex="-1"></a>   <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, </span>
<span id="cb349-11"><a href="stan-poisson-regression.html#cb349-11" aria-hidden="true" tabindex="-1"></a>   <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="stan-poisson-regression.html#cb350-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Graphical summary of data: Boxplots by age</span></span>
<span id="cb350-2"><a href="stan-poisson-regression.html#cb350-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(fledged <span class="sc">~</span> <span class="fu">as.factor</span>(age), <span class="at">range=</span><span class="dv">0</span>, <span class="at">col=</span><span class="st">&quot;pink&quot;</span>, </span>
<span id="cb350-3"><a href="stan-poisson-regression.html#cb350-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab=</span><span class="st">&quot;age&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;offspring&quot;</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-231-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[
\log E(Y_i|x_i) = \beta_1+\beta_2x_i+\beta_3x_i^2
\]</span>
where <span class="math inline">\(Y_i\)</span> is a sample from a Poisson distribution.</p>
<p>In this model <span class="math inline">\(x =\)</span> age of female bird, <span class="math inline">\(Y =\)</span> number of offspring that bird had. We observed two obvious things about this association (1) it’s positive then negative i.e., it’s not monotone. So it will require a quadratic term so we have <span class="math inline">\(\beta_1 + \beta_2x + \beta_3x^2.\)</span> (2) Expected number of offspring cannot be negative so maybe we don’t say <span class="math inline">\(E(Y|x) = \beta x\)</span> because that could be negative but rather <span class="math inline">\(E(Y|x) = \exp( \beta x ).\)</span> So that’s how we got to our model. Also, a probability distribution is not completely determined by its mean hence we need a distribution assumption also. So we’ll use Poisson.</p>
<p>Let’s see if we get similar results as before using Stan. You need to install the R package <span class="math inline">\(\texttt{rstan}\)</span>.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="stan-poisson-regression.html#cb351-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstan)</span></code></pre></div>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="stan-poisson-regression.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="co"># description of our model</span></span>
<span id="cb352-2"><a href="stan-poisson-regression.html#cb352-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> fledged  </span>
<span id="cb352-3"><a href="stan-poisson-regression.html#cb352-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb352-4"><a href="stan-poisson-regression.html#cb352-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n), age, age<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb352-5"><a href="stan-poisson-regression.html#cb352-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(X) <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n;  </span>
<span id="cb352-6"><a href="stan-poisson-regression.html#cb352-6" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;x3&quot;</span>)</span>
<span id="cb352-7"><a href="stan-poisson-regression.html#cb352-7" aria-hidden="true" tabindex="-1"></a>n           <span class="ot">&lt;-</span> <span class="fu">dim</span>(X)[<span class="dv">1</span>]</span>
<span id="cb352-8"><a href="stan-poisson-regression.html#cb352-8" aria-hidden="true" tabindex="-1"></a>p           <span class="ot">&lt;-</span> <span class="fu">dim</span>(X)[<span class="dv">2</span>]</span></code></pre></div>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="stan-poisson-regression.html#cb353-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model using Stan</span></span>
<span id="cb353-2"><a href="stan-poisson-regression.html#cb353-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior is beta_j ~ indep Normal(mn=0, sd=10)</span></span>
<span id="cb353-3"><a href="stan-poisson-regression.html#cb353-3" aria-hidden="true" tabindex="-1"></a>run.time <span class="ot">&lt;-</span> <span class="fu">proc.time</span>()</span>
<span id="cb353-4"><a href="stan-poisson-regression.html#cb353-4" aria-hidden="true" tabindex="-1"></a>stan_model <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb353-5"><a href="stan-poisson-regression.html#cb353-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb353-6"><a href="stan-poisson-regression.html#cb353-6" aria-hidden="true" tabindex="-1"></a><span class="st">  data{</span></span>
<span id="cb353-7"><a href="stan-poisson-regression.html#cb353-7" aria-hidden="true" tabindex="-1"></a><span class="st">   int&lt;lower=0&gt; n; </span></span>
<span id="cb353-8"><a href="stan-poisson-regression.html#cb353-8" aria-hidden="true" tabindex="-1"></a><span class="st">   int&lt;lower=0&gt; p;</span></span>
<span id="cb353-9"><a href="stan-poisson-regression.html#cb353-9" aria-hidden="true" tabindex="-1"></a><span class="st">   int&lt;lower=0&gt; y[n]; </span></span>
<span id="cb353-10"><a href="stan-poisson-regression.html#cb353-10" aria-hidden="true" tabindex="-1"></a><span class="st">   matrix[n,p] X;</span></span>
<span id="cb353-11"><a href="stan-poisson-regression.html#cb353-11" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb353-12"><a href="stan-poisson-regression.html#cb353-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb353-13"><a href="stan-poisson-regression.html#cb353-13" aria-hidden="true" tabindex="-1"></a><span class="st">  parameters{</span></span>
<span id="cb353-14"><a href="stan-poisson-regression.html#cb353-14" aria-hidden="true" tabindex="-1"></a><span class="st">   vector[p] beta;</span></span>
<span id="cb353-15"><a href="stan-poisson-regression.html#cb353-15" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb353-16"><a href="stan-poisson-regression.html#cb353-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb353-17"><a href="stan-poisson-regression.html#cb353-17" aria-hidden="true" tabindex="-1"></a><span class="st">  model{</span></span>
<span id="cb353-18"><a href="stan-poisson-regression.html#cb353-18" aria-hidden="true" tabindex="-1"></a><span class="st">   beta ~ normal(0, 10);</span></span>
<span id="cb353-19"><a href="stan-poisson-regression.html#cb353-19" aria-hidden="true" tabindex="-1"></a><span class="st">   y ~ poisson_log(X*beta);</span></span>
<span id="cb353-20"><a href="stan-poisson-regression.html#cb353-20" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb353-21"><a href="stan-poisson-regression.html#cb353-21" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;</span></span></code></pre></div>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="stan-poisson-regression.html#cb354-1" aria-hidden="true" tabindex="-1"></a>data     <span class="ot">&lt;-</span>  <span class="fu">list</span>(<span class="at">n=</span>n, <span class="at">p=</span>p, <span class="at">y=</span>y, <span class="at">X=</span>X);</span>
<span id="cb354-2"><a href="stan-poisson-regression.html#cb354-2" aria-hidden="true" tabindex="-1"></a>fit_stan <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">model_code=</span>stan_model, <span class="at">data=</span>data, </span>
<span id="cb354-3"><a href="stan-poisson-regression.html#cb354-3" aria-hidden="true" tabindex="-1"></a> <span class="at">chains=</span><span class="dv">1</span>, <span class="at">iter=</span><span class="dv">6000</span>, <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">refresh =</span> <span class="dv">0</span>) </span>
<span id="cb354-4"><a href="stan-poisson-regression.html#cb354-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb354-5"><a href="stan-poisson-regression.html#cb354-5" aria-hidden="true" tabindex="-1"></a>(<span class="fu">proc.time</span>() <span class="sc">-</span> run.time)<span class="sc">/</span><span class="dv">60</span> <span class="co"># elapsed time in minutes</span></span></code></pre></div>
<pre><code>##    user  system elapsed 
##  0.4337  0.0276  0.4857</code></pre>
<p>A stan model requires three components: (1) data statement (Tell stan what values are known), (2) parameters statements (tell stan what’s unknown) and (3) the model statement <span class="math inline">\(\texttt{y ~ Poisson_log(X*beta)}\)</span> <span class="math inline">\(y\)</span> is Poisson with log expected value of <span class="math inline">\(\beta x\)</span>.</p>
<p>“warmup” in Stan is the same thing we called “burn-in” before.</p>
<p>
 
</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="stan-poisson-regression.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_stan)<span class="sc">$</span>summary</span></code></pre></div>
<pre><code>##             mean  se_mean      sd      2.5%       25%      50%      75%
## beta[1]   0.2234 0.014585 0.43475  -0.67380  -0.06354   0.2371   0.5271
## beta[2]   0.7190 0.011342 0.33310   0.09595   0.48920   0.7101   0.9465
## beta[3]  -0.1412 0.001888 0.05707  -0.25690  -0.17946  -0.1395  -0.1030
## lp__    -12.6975 0.036345 1.17219 -15.85503 -13.26755 -12.4025 -11.8346
##             97.5%  n_eff Rhat
## beta[1]   1.02925  888.5    1
## beta[2]   1.39610  862.6    1
## beta[3]  -0.03493  913.3    1
## lp__    -11.35609 1040.2    1</code></pre>
<p>We have 5,000 samples for the posterior distribution <span class="math inline">\(p(\beta | y, X)\)</span></p>
<ul>
<li><span class="math inline">\(\texttt{se_mean = sd/sqrt{n_eff}}\)</span>.</li>
<li><span class="math inline">\(\texttt{Rhat}\)</span> is a thing called the Gelman-Rubin statistic it’s a diagnostic of a not-yet-converged markov chain. If it’s bigger than 1.10 or so indicates you haven’t run the chain long enough.</li>
<li>The last row of this table is a thing we haven’t talked about (and won’t)</li>
</ul>
<p>
 
</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="stan-poisson-regression.html#cb358-1" aria-hidden="true" tabindex="-1"></a>Results <span class="ot">&lt;-</span> <span class="fu">summary</span>(fit_stan)<span class="sc">$</span>summary[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">4</span><span class="sc">:</span><span class="dv">8</span>]</span>
<span id="cb358-2"><a href="stan-poisson-regression.html#cb358-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(Results, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##          2.5%   25%   50%   75% 97.5%
## beta[1] -0.67 -0.06  0.24  0.53  1.03
## beta[2]  0.10  0.49  0.71  0.95  1.40
## beta[3] -0.26 -0.18 -0.14 -0.10 -0.03</code></pre>
<p>
 
</p>
<p>If you want the samples themselves easy enough</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="stan-poisson-regression.html#cb360-1" aria-hidden="true" tabindex="-1"></a>beta.sims <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_stan)<span class="sc">$</span>beta;<span class="fu">head</span>(beta.sims,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##           
## iterations    [,1]   [,2]    [,3]
##       [1,] -0.1908 0.8781 -0.1561
##       [2,]  0.1824 0.6834 -0.1295
##       [3,]  0.2664 0.6443 -0.1268</code></pre>
<p>The rows are iterations of the sampler the columns are the three components of <span class="math inline">\(\boldsymbol\beta\)</span>.</p>
<p>
 
</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="stan-poisson-regression.html#cb362-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make trace plots of the first 1000 updates</span></span>
<span id="cb362-2"><a href="stan-poisson-regression.html#cb362-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,p))</span>
<span id="cb362-3"><a href="stan-poisson-regression.html#cb362-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb362-4"><a href="stan-poisson-regression.html#cb362-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p){ </span>
<span id="cb362-5"><a href="stan-poisson-regression.html#cb362-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>(beta.sims[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,j], <span class="at">type=</span><span class="st">&quot;l&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;iteration&quot;</span>, </span>
<span id="cb362-6"><a href="stan-poisson-regression.html#cb362-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">ylab=</span><span class="fu">paste</span>(<span class="st">&quot;beta_&quot;</span>, j, <span class="at">sep=</span><span class="st">&quot;&quot;</span>), <span class="at">main=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb362-7"><a href="stan-poisson-regression.html#cb362-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-239-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="stan-poisson-regression.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample autocorrelation functions</span></span>
<span id="cb363-2"><a href="stan-poisson-regression.html#cb363-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,p))</span>
<span id="cb363-3"><a href="stan-poisson-regression.html#cb363-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb363-4"><a href="stan-poisson-regression.html#cb363-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p){</span>
<span id="cb363-5"><a href="stan-poisson-regression.html#cb363-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">acf</span>(beta.sims[,j], <span class="at">main=</span><span class="fu">paste</span>(<span class="st">&quot;beta_&quot;</span>, j, <span class="at">sep=</span><span class="st">&quot;&quot;</span>))</span>
<span id="cb363-6"><a href="stan-poisson-regression.html#cb363-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-240-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Whatever Stan is doing is better than the Metropolis algorithm. the autocorrelation is basically zero! Stan is doing a thing called <strong>Hamiltonian Monte Carlo</strong> which is not unrelated to Metropolis-Hastings but it’s not Metropolis-Hastings.</p>
<p>
 
</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="stan-poisson-regression.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Approximate posterior densites of beta1, beta2, beta3</span></span>
<span id="cb364-2"><a href="stan-poisson-regression.html#cb364-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,p))</span>
<span id="cb364-3"><a href="stan-poisson-regression.html#cb364-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-4"><a href="stan-poisson-regression.html#cb364-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p){ </span>
<span id="cb364-5"><a href="stan-poisson-regression.html#cb364-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>(<span class="fu">density</span>(beta.sims[,j], <span class="at">adj=</span><span class="dv">2</span>), <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">main=</span><span class="st">&quot;&quot;</span>,</span>
<span id="cb364-6"><a href="stan-poisson-regression.html#cb364-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">xlab=</span><span class="fu">paste</span>(<span class="st">&quot;beta_&quot;</span>, j, <span class="at">sep=</span><span class="st">&quot;&quot;</span>), <span class="at">ylab=</span><span class="st">&quot;p(beta|y)&quot;</span>)</span>
<span id="cb364-7"><a href="stan-poisson-regression.html#cb364-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-241-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
 
</p>
<p>Posterior inference for <span class="math inline">\(E(Y|x) = \beta_1 + \beta_2x + \beta_3x^2\)</span></p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="stan-poisson-regression.html#cb365-1" aria-hidden="true" tabindex="-1"></a>S                    <span class="ot">&lt;-</span> <span class="fu">dim</span>(beta.sims)[<span class="dv">1</span>]</span>
<span id="cb365-2"><a href="stan-poisson-regression.html#cb365-2" aria-hidden="true" tabindex="-1"></a>theta.sims           <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, S, <span class="dv">6</span>)</span>
<span id="cb365-3"><a href="stan-poisson-regression.html#cb365-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(theta.sims) <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>S;  </span>
<span id="cb365-4"><a href="stan-poisson-regression.html#cb365-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(theta.sims) <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;theta_&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">sep=</span><span class="st">&quot;&quot;</span>);</span>
<span id="cb365-5"><a href="stan-poisson-regression.html#cb365-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb365-6"><a href="stan-poisson-regression.html#cb365-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (x <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb365-7"><a href="stan-poisson-regression.html#cb365-7" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb365-8"><a href="stan-poisson-regression.html#cb365-8" aria-hidden="true" tabindex="-1"></a> theta.sims[,x] <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="fu">as.vector</span>(beta.sims <span class="sc">%*%</span> <span class="fu">c</span>(<span class="dv">1</span>, x, x<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb365-9"><a href="stan-poisson-regression.html#cb365-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb365-10"><a href="stan-poisson-regression.html#cb365-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb365-11"><a href="stan-poisson-regression.html#cb365-11" aria-hidden="true" tabindex="-1"></a>quants <span class="ot">&lt;-</span> <span class="fu">apply</span>(theta.sims, <span class="dv">2</span>, quantile, <span class="at">probs=</span><span class="fu">c</span>(.<span class="dv">025</span>, .<span class="dv">5</span>, .<span class="dv">975</span>))</span>
<span id="cb365-12"><a href="stan-poisson-regression.html#cb365-12" aria-hidden="true" tabindex="-1"></a>quants</span></code></pre></div>
<pre><code>##       theta_1 theta_2 theta_3 theta_4 theta_5 theta_6
## 2.5%    1.498   2.370   2.362   1.831  0.8006  0.1942
## 50%     2.251   2.999   3.042   2.326  1.3426  0.5873
## 97.5%   3.156   3.764   3.845   2.915  2.1010  1.5463</code></pre>
<p>Do the results agree with what we did in a previous lecture – <a href="#sec:metpois"><strong>??</strong></a>?</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="stan-poisson-regression.html#cb367-1" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(<span class="fu">t</span>(quants), <span class="at">type=</span><span class="st">&quot;b&quot;</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb367-2"><a href="stan-poisson-regression.html#cb367-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;pink&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;pink&quot;</span>), <span class="at">xlab=</span><span class="st">&quot;age x&quot;</span>, </span>
<span id="cb367-3"><a href="stan-poisson-regression.html#cb367-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab=</span><span class="st">&quot;E(Y|x)&quot;</span>, <span class="at">main=</span><span class="st">&quot;Expected offspring by age&quot;</span>)</span>
<span id="cb367-4"><a href="stan-poisson-regression.html#cb367-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-5"><a href="stan-poisson-regression.html#cb367-5" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">inset=</span>.<span class="dv">05</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb367-6"><a href="stan-poisson-regression.html#cb367-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;pink&quot;</span>), </span>
<span id="cb367-7"><a href="stan-poisson-regression.html#cb367-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;Posterior median&quot;</span>, <span class="st">&quot;95% posterior interval&quot;</span>),<span class="at">cex=</span><span class="fl">0.7</span>)</span></code></pre></div>
<p><img src="bayesianS21_files/figure-html/unnamed-chunk-243-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>They sure do. Pretty much perfect agreement!</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesianS21.pdf", "bayesianS21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
