<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Bayesian Statistics lecture notes</title>
  <meta name="description" content="Bayesian Statistics lecture notes" />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Bayesian Statistics lecture notes" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Statistics lecture notes" />
  
  
  

<meta name="author" content="Chisom Onyishi" />


<meta name="date" content="2021-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="exchangeability.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian statistics notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Belief and Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i><b>1.1</b> Example:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.2</b> Random Variables</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#binomial"><i class="fa fa-check"></i><b>1.3</b> Binomial distribution</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.4</b> Poisson distribution</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#example-normalmu-10.75-sigma0.8"><i class="fa fa-check"></i><b>1.5.1</b> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exchangeability.html"><a href="exchangeability.html"><i class="fa fa-check"></i><b>2</b> Exchangeability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exchangeability.html"><a href="exchangeability.html#discrete-joint-distributions"><i class="fa fa-check"></i><b>2.1</b> Discrete joint distributions</a></li>
<li class="chapter" data-level="2.2" data-path="exchangeability.html"><a href="exchangeability.html#bayesrule"><i class="fa fa-check"></i><b>2.2</b> Bayes’ rule and parameter estimation</a></li>
<li class="chapter" data-level="2.3" data-path="exchangeability.html"><a href="exchangeability.html#independent-random-variables"><i class="fa fa-check"></i><b>2.3</b> Independent random variables</a></li>
<li class="chapter" data-level="2.4" data-path="exchangeability.html"><a href="exchangeability.html#exchangeability-1"><i class="fa fa-check"></i><b>2.4</b> Exchangeability</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="binomial-1.html"><a href="binomial-1.html"><i class="fa fa-check"></i><b>3</b> Binomial</a>
<ul>
<li class="chapter" data-level="3.1" data-path="binomial-1.html"><a href="binomial-1.html#example---exchangeable-binary-data"><i class="fa fa-check"></i><b>3.1</b> Example - Exchangeable binary data</a></li>
<li class="chapter" data-level="3.2" data-path="binomial-1.html"><a href="binomial-1.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.2</b> The beta distribution</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="binomial-1.html"><a href="binomial-1.html#properties-of-the-beta-distribution"><i class="fa fa-check"></i><b>3.2.1</b> Properties of the beta distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="binomial-1.html"><a href="binomial-1.html#binomial-distribution"><i class="fa fa-check"></i><b>3.3</b> Binomial distribution</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="binomial-1.html"><a href="binomial-1.html#posterior-inference-for-a-binomial-sampling-model"><i class="fa fa-check"></i><b>3.3.1</b> Posterior inference for a binomial sampling model</a></li>
<li class="chapter" data-level="3.3.2" data-path="binomial-1.html"><a href="binomial-1.html#conjugacy"><i class="fa fa-check"></i><b>3.3.2</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="binomial-1.html"><a href="binomial-1.html#combining-information"><i class="fa fa-check"></i><b>3.4</b> Combining information</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="binomial-1.html"><a href="binomial-1.html#example-1"><i class="fa fa-check"></i><b>3.4.1</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="binomial-1.html"><a href="binomial-1.html#prediction"><i class="fa fa-check"></i><b>3.5</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="CI.html"><a href="CI.html"><i class="fa fa-check"></i><b>4</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.1" data-path="CI.html"><a href="CI.html#confidence-intervals"><i class="fa fa-check"></i><b>4.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="4.2" data-path="CI.html"><a href="CI.html#how-do-we-compute-intervals"><i class="fa fa-check"></i><b>4.2</b> How do we compute intervals?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="CI.html"><a href="CI.html#example-2"><i class="fa fa-check"></i><b>4.2.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>5</b> Poisson model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="poisson-model.html"><a href="poisson-model.html#posterior-inference-for-the-poisson-model"><i class="fa fa-check"></i><b>5.1</b> Posterior inference for the Poisson model</a></li>
<li class="chapter" data-level="5.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>5.2</b> Posterior predictive distribution</a></li>
<li class="chapter" data-level="5.3" data-path="poisson-model.html"><a href="poisson-model.html#example-birth-rates"><i class="fa fa-check"></i><b>5.3</b> Example: Birth rates</a></li>
<li class="chapter" data-level="5.4" data-path="poisson-model.html"><a href="poisson-model.html#explaining-the-parameters-of-the-gamma-distribution"><i class="fa fa-check"></i><b>5.4</b> Explaining the parameters of the gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Monte Carlo</a>
<ul>
<li class="chapter" data-level="6.1" data-path="monte-carlo.html"><a href="monte-carlo.html#example-birth-rate"><i class="fa fa-check"></i><b>6.1</b> Example: birth rate</a></li>
<li class="chapter" data-level="6.2" data-path="monte-carlo.html"><a href="monte-carlo.html#the-monte-carlo-method"><i class="fa fa-check"></i><b>6.2</b> The Monte Carlo method</a></li>
<li class="chapter" data-level="6.3" data-path="monte-carlo.html"><a href="monte-carlo.html#example-numerical-evaluation"><i class="fa fa-check"></i><b>6.3</b> Example: Numerical evaluation</a></li>
<li class="chapter" data-level="6.4" data-path="monte-carlo.html"><a href="monte-carlo.html#posterior-inference-for-arbitrary-functions"><i class="fa fa-check"></i><b>6.4</b> Posterior inference for arbitrary functions</a></li>
<li class="chapter" data-level="6.5" data-path="monte-carlo.html"><a href="monte-carlo.html#example-log-odds"><i class="fa fa-check"></i><b>6.5</b> Example: Log-odds</a></li>
<li class="chapter" data-level="6.6" data-path="monte-carlo.html"><a href="monte-carlo.html#example-functions-of-two-parameters"><i class="fa fa-check"></i><b>6.6</b> Example: Functions of two parameters</a></li>
<li class="chapter" data-level="6.7" data-path="monte-carlo.html"><a href="monte-carlo.html#how-many-monte-carlo-samples-are-needed"><i class="fa fa-check"></i><b>6.7</b> How many Monte Carlo samples are needed?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="predictive.html"><a href="predictive.html"><i class="fa fa-check"></i><b>7</b> Predictive</a>
<ul>
<li class="chapter" data-level="7.1" data-path="predictive.html"><a href="predictive.html#sampling-for-predictive-distribution"><i class="fa fa-check"></i><b>7.1</b> Sampling for predictive distribution</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="predictive.html"><a href="predictive.html#example-birth-rate-1"><i class="fa fa-check"></i><b>7.1.1</b> Example: birth rate</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="predictive.html"><a href="predictive.html#example-let-d-tilde-y_1---tilde-y_2"><i class="fa fa-check"></i><b>7.2</b> Example: Let <span class="math inline">\(D = \tilde Y_1 - \tilde Y_2\)</span></a></li>
<li class="chapter" data-level="7.3" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking"><i class="fa fa-check"></i><b>7.3</b> Posterior predictive model checking</a></li>
<li class="chapter" data-level="7.4" data-path="predictive.html"><a href="predictive.html#posterior-predictive-model-checking-1"><i class="fa fa-check"></i><b>7.4</b> Posterior predictive model checking</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normal-mean.html"><a href="normal-mean.html"><i class="fa fa-check"></i><b>8</b> Normal Mean</a>
<ul>
<li class="chapter" data-level="8.1" data-path="normal-mean.html"><a href="normal-mean.html#example-womens-height"><i class="fa fa-check"></i><b>8.1</b> Example: women’s height</a></li>
<li class="chapter" data-level="8.2" data-path="normal-mean.html"><a href="normal-mean.html#inference-for-the-mean-conditional-on-the-variance"><i class="fa fa-check"></i><b>8.2</b> Inference for the mean, conditional on the variance</a></li>
<li class="chapter" data-level="8.3" data-path="normal-mean.html"><a href="normal-mean.html#prediction-1"><i class="fa fa-check"></i><b>8.3</b> Prediction</a></li>
<li class="chapter" data-level="8.4" data-path="normal-mean.html"><a href="normal-mean.html#example-midge-wing-length"><i class="fa fa-check"></i><b>8.4</b> Example: Midge wing length</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html"><i class="fa fa-check"></i><b>9</b> Joint inference for Normal mean and variance</a>
<ul>
<li class="chapter" data-level="9.1" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#marginal-posterior-of-sigma2"><i class="fa fa-check"></i><b>9.1</b> Marginal posterior of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="9.2" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#example-midge-wing-length-1"><i class="fa fa-check"></i><b>9.2</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="9.3" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>9.3</b> Monte Carlo sampling</a></li>
<li class="chapter" data-level="9.4" data-path="joint-inference-for-normal-mean-and-variance.html"><a href="joint-inference-for-normal-mean-and-variance.html#summary-of-normal-formulas"><i class="fa fa-check"></i><b>9.4</b> Summary of Normal formulas</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html"><i class="fa fa-check"></i><b>10</b> Gibbs sampler</a>
<ul>
<li class="chapter" data-level="10.1" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#review-of-conjugate-prior-for-normal-model"><i class="fa fa-check"></i><b>10.1</b> Review of conjugate prior for normal model</a></li>
<li class="chapter" data-level="10.2" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#a-semiconjugate-prior-distribution"><i class="fa fa-check"></i><b>10.2</b> A semiconjugate prior distribution</a></li>
<li class="chapter" data-level="10.3" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.3</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.4" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-midge-wing-length-2"><i class="fa fa-check"></i><b>10.4</b> Example: Midge wing length</a></li>
<li class="chapter" data-level="10.5" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#discrete-approximation-of-posterior-distribution"><i class="fa fa-check"></i><b>10.5</b> Discrete approximation of posterior distribution</a></li>
<li class="chapter" data-level="10.6" data-path="gibbs-sampler.html"><a href="gibbs-sampler.html#example-3"><i class="fa fa-check"></i><b>10.6</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html"><i class="fa fa-check"></i><b>11</b> MCMC diagnostics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#the-gibbs-sampler"><i class="fa fa-check"></i><b>11.1</b> The Gibbs sampler</a></li>
<li class="chapter" data-level="11.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#distinguishing-estimation-from-approximation"><i class="fa fa-check"></i><b>11.2</b> Distinguishing estimation from approximation</a></li>
<li class="chapter" data-level="11.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#introduction-to-mcmc-diagnostics"><i class="fa fa-check"></i><b>11.3</b> Introduction to MCMC diagnostics</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#example-mixture-of-normal-densities"><i class="fa fa-check"></i><b>11.3.1</b> Example: mixture of normal densities</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#discussion"><i class="fa fa-check"></i><b>11.4</b> Discussion</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#how-does-autocorrelationslow-mixing-affect-our-mcmc-approximation"><i class="fa fa-check"></i><b>11.4.1</b> How does autocorrelation(slow mixing) affect our MCMC approximation?</a></li>
<li class="chapter" data-level="11.4.2" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#autocorrelation"><i class="fa fa-check"></i><b>11.4.2</b> Autocorrelation</a></li>
<li class="chapter" data-level="11.4.3" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#sample-autocorrelation-function"><i class="fa fa-check"></i><b>11.4.3</b> Sample autocorrelation function</a></li>
<li class="chapter" data-level="11.4.4" data-path="mcmc-diagnostics.html"><a href="mcmc-diagnostics.html#effective-sample-size"><i class="fa fa-check"></i><b>11.4.4</b> Effective sample size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multivariate-normal.html"><a href="multivariate-normal.html"><i class="fa fa-check"></i><b>12</b> Multivariate Normal</a>
<ul>
<li class="chapter" data-level="12.1" data-path="multivariate-normal.html"><a href="multivariate-normal.html#example-reading-comprehension"><i class="fa fa-check"></i><b>12.1</b> Example: Reading comprehension</a></li>
<li class="chapter" data-level="12.2" data-path="multivariate-normal.html"><a href="multivariate-normal.html#the-multivariate-normal-density"><i class="fa fa-check"></i><b>12.2</b> The multivariate normal density</a></li>
<li class="chapter" data-level="12.3" data-path="multivariate-normal.html"><a href="multivariate-normal.html#a-semiconjugate-prior-distribution-for-the-mean"><i class="fa fa-check"></i><b>12.3</b> A semiconjugate prior distribution for the mean</a></li>
<li class="chapter" data-level="12.4" data-path="multivariate-normal.html"><a href="multivariate-normal.html#the-inverse-wishart-distribution"><i class="fa fa-check"></i><b>12.4</b> The inverse-Wishart distribution</a></li>
<li class="chapter" data-level="12.5" data-path="multivariate-normal.html"><a href="multivariate-normal.html#full-conditional-distribution-of-the-covariance-matrix"><i class="fa fa-check"></i><b>12.5</b> Full conditional distribution of the covariance matrix</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="group-comparisons.html"><a href="group-comparisons.html"><i class="fa fa-check"></i><b>13</b> Group comparisons</a>
<ul>
<li class="chapter" data-level="13.1" data-path="group-comparisons.html"><a href="group-comparisons.html#comparing-two-groups"><i class="fa fa-check"></i><b>13.1</b> Comparing two groups</a></li>
<li class="chapter" data-level="13.2" data-path="group-comparisons.html"><a href="group-comparisons.html#mathex1"><i class="fa fa-check"></i><b>13.2</b> Example: Math scores data</a></li>
<li class="chapter" data-level="13.3" data-path="group-comparisons.html"><a href="group-comparisons.html#bayesmodel"><i class="fa fa-check"></i><b>13.3</b> A Bayesian model</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="group-comparisons.html"><a href="group-comparisons.html#analysis-of-the-math-scores-data"><i class="fa fa-check"></i><b>13.3.1</b> Analysis of the math scores data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html"><i class="fa fa-check"></i><b>14</b> The hierarchical normal model</a>
<ul>
<li class="chapter" data-level="14.1" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#postinf"><i class="fa fa-check"></i><b>14.1</b> Posterior inference</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-distributions-of-mu-and-tau2"><i class="fa fa-check"></i><b>14.1.1</b> Full conditional distributions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span></a></li>
<li class="chapter" data-level="14.1.2" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-of-theta_j"><i class="fa fa-check"></i><b>14.1.2</b> Full conditional of <span class="math inline">\(\theta_j\)</span></a></li>
<li class="chapter" data-level="14.1.3" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#full-conditional-of-sigma2"><i class="fa fa-check"></i><b>14.1.3</b> Full conditional of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#mathex2"><i class="fa fa-check"></i><b>14.2</b> Example: Math scores in U.S. public schools</a></li>
<li class="chapter" data-level="14.3" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#posterior-approximation"><i class="fa fa-check"></i><b>14.3</b> Posterior approximation</a></li>
<li class="chapter" data-level="14.4" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#mcmc-diagnostics-1"><i class="fa fa-check"></i><b>14.4</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="14.5" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#shrinkage"><i class="fa fa-check"></i><b>14.5</b> Shrinkage</a></li>
<li class="chapter" data-level="14.6" data-path="the-hierarchical-normal-model.html"><a href="the-hierarchical-normal-model.html#ranking-the-groups"><i class="fa fa-check"></i><b>14.6</b> Ranking the groups</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>15</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="linear-regression.html"><a href="linear-regression.html#example-oxygen-uptake"><i class="fa fa-check"></i><b>15.1</b> Example: Oxygen uptake</a></li>
<li class="chapter" data-level="15.2" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimation"><i class="fa fa-check"></i><b>15.2</b> Least squares estimation</a></li>
<li class="chapter" data-level="15.3" data-path="linear-regression.html"><a href="linear-regression.html#least-squares-estimation-for-oxygen-uptake-data"><i class="fa fa-check"></i><b>15.3</b> Least squares estimation for oxygen uptake data</a></li>
<li class="chapter" data-level="15.4" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-estimation-for-a-regression-model"><i class="fa fa-check"></i><b>15.4</b> Bayesian estimation for a regression model</a></li>
<li class="chapter" data-level="15.5" data-path="linear-regression.html"><a href="linear-regression.html#unit-information-prior"><i class="fa fa-check"></i><b>15.5</b> Unit information prior</a></li>
<li class="chapter" data-level="15.6" data-path="linear-regression.html"><a href="linear-regression.html#zellners-g-prior"><i class="fa fa-check"></i><b>15.6</b> Zellner’s <span class="math inline">\(g\)</span>-prior</a></li>
<li class="chapter" data-level="15.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-analysis-using-invariant-g-prior"><i class="fa fa-check"></i><b>15.7</b> Bayesian analysis using invariant <span class="math inline">\(g\)</span>-prior</a></li>
<li class="chapter" data-level="15.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-analysis-using-semiconjugate-prior"><i class="fa fa-check"></i><b>15.8</b> Bayesian analysis using semiconjugate prior</a>
<ul>
<li class="chapter" data-level="15.8.1" data-path="linear-regression.html"><a href="linear-regression.html#prediction-problem"><i class="fa fa-check"></i><b>15.8.1</b> Prediction problem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>16</b> Model Selection</a>
<ul>
<li class="chapter" data-level="16.1" data-path="model-selection.html"><a href="model-selection.html#review"><i class="fa fa-check"></i><b>16.1</b> Review</a></li>
<li class="chapter" data-level="16.2" data-path="model-selection.html"><a href="model-selection.html#bayesian-model-comparison"><i class="fa fa-check"></i><b>16.2</b> Bayesian model comparison</a></li>
<li class="chapter" data-level="16.3" data-path="model-selection.html"><a href="model-selection.html#example-oxygen-uptake-1"><i class="fa fa-check"></i><b>16.3</b> Example: Oxygen uptake</a></li>
<li class="chapter" data-level="16.4" data-path="model-selection.html"><a href="model-selection.html#gibbs-sampling-and-model-averaging"><i class="fa fa-check"></i><b>16.4</b> Gibbs sampling and model averaging</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html"><i class="fa fa-check"></i><b>17</b> Generalized Linear Models; the Metropolis Algorithm</a>
<ul>
<li class="chapter" data-level="17.1" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#example-song-sparrow-reproductive-success"><i class="fa fa-check"></i><b>17.1</b> Example: Song sparrow reproductive success</a></li>
<li class="chapter" data-level="17.2" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#sec:poisson"><i class="fa fa-check"></i><b>17.2</b> Poisson regression</a></li>
<li class="chapter" data-level="17.3" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#logistic-regression"><i class="fa fa-check"></i><b>17.3</b> Logistic regression</a></li>
<li class="chapter" data-level="17.4" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#posterior-approximations"><i class="fa fa-check"></i><b>17.4</b> Posterior approximations</a></li>
<li class="chapter" data-level="17.5" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>17.5</b> The Metropolis algorithm</a></li>
<li class="chapter" data-level="17.6" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#example-normal-distribution-with-known-variance"><i class="fa fa-check"></i><b>17.6</b> Example: Normal distribution with known variance</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#output-of-metropolis-algorithm"><i class="fa fa-check"></i><b>17.6.1</b> Output of Metropolis Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="generalized-linear-models-the-metropolis-algorithm.html"><a href="generalized-linear-models-the-metropolis-algorithm.html#the-metropolis-algorithm-for-poisson-regression-secmetpois"><i class="fa fa-check"></i><b>17.7</b> The Metropolis algorithm for Poisson regression {sec:metpois}</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html"><i class="fa fa-check"></i><b>18</b> Metropolis-Hastings</a>
<ul>
<li class="chapter" data-level="18.1" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#why-does-the-metropolis-hastings-algorithm-work"><i class="fa fa-check"></i><b>18.1</b> Why does the Metropolis-Hastings algorithm work?</a></li>
<li class="chapter" data-level="18.2" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#combining-the-metropolis-and-gibbs-algorithms"><i class="fa fa-check"></i><b>18.2</b> Combining the Metropolis and Gibbs algorithms</a></li>
<li class="chapter" data-level="18.3" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#example-historical-co_2-and-temperature-data"><i class="fa fa-check"></i><b>18.3</b> Example: Historical CO<span class="math inline">\(_2\)</span> and temperature data</a></li>
<li class="chapter" data-level="18.4" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#a-regression-model-with-correlated-errors"><i class="fa fa-check"></i><b>18.4</b> A regression model with correlated errors</a></li>
<li class="chapter" data-level="18.5" data-path="metropolis-hastings.html"><a href="metropolis-hastings.html#analysis-of-the-ice-core-data"><i class="fa fa-check"></i><b>18.5</b> Analysis of the ice core data</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Mixed-effects Models, aka, Hierarchical Linear Models</a>
<ul>
<li class="chapter" data-level="19.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#hierarchical-model-review"><i class="fa fa-check"></i><b>19.1</b> Hierarchical model review</a></li>
<li class="chapter" data-level="19.2" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#hierarchical-linear-regression-model-for-math-scores-data"><i class="fa fa-check"></i><b>19.2</b> Hierarchical linear regression model for math scores data</a></li>
<li class="chapter" data-level="19.3" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#bayesian-hierarchical-linear-regression-model"><i class="fa fa-check"></i><b>19.3</b> Bayesian hierarchical linear regression model</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#full-conditionals"><i class="fa fa-check"></i><b>19.3.1</b> Full conditionals</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#bayesian-analysis-of-the-math-scores-data"><i class="fa fa-check"></i><b>19.4</b> Bayesian analysis of the math scores data</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#mcmc-diagnostics-2"><i class="fa fa-check"></i><b>19.4.1</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#posterior-summaries"><i class="fa fa-check"></i><b>19.4.2</b> Posterior summaries</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-mixed-effects-models-aka-hierarchical-linear-models.html"><a href="linear-mixed-effects-models-aka-hierarchical-linear-models.html#posterior-predictive-simulation"><i class="fa fa-check"></i><b>19.4.3</b> Posterior predictive simulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="stan-poisson-regression.html"><a href="stan-poisson-regression.html"><i class="fa fa-check"></i><b>20</b> Stan; Poisson regression</a>
<ul>
<li class="chapter" data-level="20.1" data-path="stan-poisson-regression.html"><a href="stan-poisson-regression.html#intro-to-stan"><i class="fa fa-check"></i><b>20.1</b> Intro to Stan</a></li>
<li class="chapter" data-level="20.2" data-path="stan-poisson-regression.html"><a href="stan-poisson-regression.html#song-sparrows-reproductive-success-example"><i class="fa fa-check"></i><b>20.2</b> Song sparrows reproductive success example</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>21</b> Summary</a>
<ul>
<li class="chapter" data-level="21.1" data-path="summary.html"><a href="summary.html#missing-data-and-imputation-7b"><i class="fa fa-check"></i><b>21.1</b> Missing data and imputation; 7b</a></li>
<li class="chapter" data-level="21.2" data-path="summary.html"><a href="summary.html#generalized-linear-mixed-eﬀects-models-11b"><i class="fa fa-check"></i><b>21.2</b> Generalized linear mixed eﬀects models; 11b</a></li>
<li class="chapter" data-level="21.3" data-path="summary.html"><a href="summary.html#improper-priors"><i class="fa fa-check"></i><b>21.3</b> Improper priors</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Statistics lecture notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Bayesian Statistics lecture notes</h1>
<p class="author"><em>Chisom Onyishi</em></p>
<p class="date" style="margin-top: 1.5em;"><em>2021-06-12</em></p>
</div>
<div id="intro" class="section level1" number="1">
<h1><span class="header-section-number">Lecture 1</span> Belief and Probability</h1>
<p>This is a compilation of <em>transcribed</em> lectures notes from <a href="http://stat.columbia.edu/department-directory/name/ronald-neath/">Ronald Neath, June 2021</a> class on <strong>BAYESIAN STATISTICS STATGR5224</strong>. The course materials follow mostly from <a href="https://pdhoff.github.io/book/">A First Course in Bayesian Statistical Methods, Hoff(2009)</a>.</p>
<p>
 
</p>
<p><tt>The following notes, mostly transcribed from Neath(0503,2021) lecture, summarize sections(2.1-2.4) of Hoff(2009).</tt></p>
<ul>
<li>I like to use the chat</li>
<li>I will post lecture slides to Courseworks before 9am each class day</li>
<li>I will share the slides as I talk and I will type in the chat as I talk</li>
<li>My hope is that by typing in the chat as we go along I will sort of reign myself in from going too fast. Also, there’s a printed record of the chat which is nice</li>
</ul>
<p>Gelman is on the third edition and there’s no reason to be looking at the 2nd or 1st edition of Gelman. However, for this course, I recommend get your hands on the book by Peter D. Hoff (2009) “A First Course in Bayesian Statistical Methods.” I will occasionally reference stuff in the Gelman text but the Hoff text is what we’re gonna follow. If you access “Springer link” using the Columbia network you can get a free copy of Hoff’s book by legitimate means! Gelman’s book is also (legitimately) free online because he has posted it! Find a link on <a href="http://www.stat.columbia.edu/~gelman/book/">Prof Gelman’s Columbia web page</a>!</p>
<p>Again, lecture will follow Hoff’s text. We’re starting in Chapter 2 and we’ll finish it or come very close (maybe skipping a few things along the way). We’ll leave some time at the end to talk about a few things that aren’t in Hoff’s book e.g. Stan software (which we’ll introduce toward the end of the course). But we’ll be doing computing from the get-go.</p>
<p>Your first assignment (due next week) (not posted yet) will involve computing. I like R, but you may use Python if you know it. The Hoff book has some examples given in R.</p>
<p>You know all about probability from the ‘relative frequency’ interpretation. If I roll a die a gazillion times it will land on the 3-side one-sixth of those rolls <span class="math inline">\(Pr(3) = 1/6\)</span>. In Bayesian statistics we use probability more generally than that. We use probability in a way that’s consistent with the informal use of probability. E.g., the probability that there will be criminal charges brought on former president is x%. In a strict frequency-based sense it doesn’t even make sense to speak of ‘probability’ for something like this. In a Bayesian sense it does and that is because in Bayesian statistics we use a different (more general) interpretation of probability where the probability of an event represents our degree of belief in that event. In Bayesian statistics any statement about the world can have a probability attached to it. E.g., The probability that climate change is a hoax brought by China and the Deep State. Though we might say this is absurd, some might assign this a probability other than zero. The only rule we’ll have for probabilities in our course is that they be internally consistent i.e., coherent – that they follow the rules of probability (mathematical laws of probability). We will use probability as a measure of our degree of belief in a statement about the world.</p>
<p>Forget about probability for a second and just think about this idea of Belief. Let <span class="math inline">\(F\)</span> be a statement about the world <span class="math inline">\(Be(F)\)</span> is our degree of belief in that statement as measured by a numerical value. What’s a reasonable set of requirements on <span class="math inline">\(Be()\)</span> for it to be a reasonable belief function?</p>
<p>The higher the value, the higher the degree of belief.</p>
<p>Let’s make this more concrete by thinking about bets</p>
<ul>
<li><p><span class="math inline">\(Be(F) &gt; Be(G)\)</span> means prefer betting on <span class="math inline">\(F\)</span> to betting on <span class="math inline">\(G\)</span></p></li>
<li><p><span class="math inline">\(Be(F | H) &gt; Be(G | H)\)</span> means if we know <span class="math inline">\(H\)</span> to be true we prefer betting on <span class="math inline">\(F\)</span> to betting on <span class="math inline">\(G\)</span></p></li>
<li><p><span class="math inline">\(Be(F | G) &gt; Be(F | H)\)</span> means if forced to bet on <span class="math inline">\(F\)</span> we prefer to do it under the condition that <span class="math inline">\(G\)</span> is true than that <span class="math inline">\(H\)</span> is true.</p></li>
</ul>
<p>The following <strong>Axioms of belief</strong> have been proposed as a set of conditions that any rational belief function must satisfy</p>
<p>B1. <span class="math inline">\(Be(\text{not } H | H) \le Be(F|H) \le Be(H | H)\)</span>.</p>
<p>If <span class="math inline">\(H\)</span> is known to be true there is no other function that I have higher belief in than <span class="math inline">\(H\)</span> itself, and there is no statement that I have lower belief in other than not <span class="math inline">\(H\)</span>.</p>
<p>B2. <span class="math inline">\(Be(F \text{ or } G | H) \ge Be(F | H), ~~ Be(F \text{ or } G | H) \ge Be(G | H)\)</span></p>
<p>B3. <span class="math inline">\(Be(F \text{ and } G | H)\)</span> can be derived from <span class="math inline">\(Be(G | H)\)</span> and <span class="math inline">\(Be(F | G \text{ and } H)\)</span></p>
<p>We propose as Bayesian statisticians to use probability as our measure of belief. Probability has its own set of axioms. Here they are!</p>
<p>P1. <span class="math inline">\(0 \le Pr(F | H) \le 1\)</span>, where <span class="math inline">\(Pr(\text{ not } H | H) = 0, ~~ Pr( H | H ) = 1\)</span> so that takes care of B1</p>
<p>P2. If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are disjoint events then <span class="math inline">\(Pr(F \text{ or } G | H) = Pr(F | H) + Pr(G | H)\)</span></p>
<p>P3. <span class="math inline">\(Pr(F \text{ and } G | H) = Pr(G | H) \times Pr(F | G \text{ and } H)\)</span></p>
<p>You can verify that any probability function that satisfies the axioms of probability also satisfies the axioms of belief. So using probability as a language for measuring our belief in statements about the world is justified. And that’s what we do in Bayesian statistics.</p>
<div id="example" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Example:</h2>
<p><span class="math inline">\(H_j =\)</span> the event that { randomly selected person is in quartile <span class="math inline">\(j\)</span> of income } <span class="math inline">\(j = 1, 2, 3, 4\)</span></p>
<p><span class="math inline">\(H_1 =\)</span> { lower <span class="math inline">\(25\%\)</span> }, <span class="math inline">\(H_4\)</span> = { upper <span class="math inline">\(25\%\)</span> }</p>
<p>Let <span class="math inline">\(E\)</span> = event that { randomly selected person has college degree }</p>
<p>From survey data (a very large survey; the General Social Survey for a particular year). We have <span class="math inline">\(Pr(E | H_j)\)</span> for each of <span class="math inline">\(j = 1, 2, 3, 4\)</span> <span class="math inline">\(.11 + .19 + .31 + .53 = 1.14\)</span>. You might think uh oh. However, not uh oh at all. These numbers aren’t expected to add to <span class="math inline">\(1.\)</span> These numbers don’t add to anything particularly meaningful. Using Bayes rule, we can obtain <span class="math inline">\(Pr(H_j | E)\)</span> for each <span class="math inline">\(j = 1, 2, 3, 4\)</span> and those had better add to <span class="math inline">\(1\)</span>.</p>
<p>Let’s do <span class="math inline">\(Pr(H_3 | E)\)</span>; the probability that a person is in the 3rd quartile of income (between <span class="math inline">\(50\)</span>th and <span class="math inline">\(75\)</span>th percentile) given that they have a college degree. Using Bayes’ rule</p>
<p><span class="math display">\[
\begin{aligned}
Pr(H_3 | E)  &amp;= \frac{Pr(H_3 \text{ and } E)}{Pr(E)}\\
&amp;=\frac{Pr(H_3)Pr(E | H_3)}{Pr(E)}\\
&amp;= \frac{Pr(H_3)Pr(E | H_3)}{Pr(H_1 \text{ and } E) + Pr(H_2 \text{ and } E) + Pr(H_3 \text{ and } E) + Pr(H_4 \text{ and } E)}\\
&amp;= \frac{.25 \times .31}{0.28} = \frac{0.0775}{0.28} = 0.272
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
Pr(H_1 \text{ and } E) = Pr(H_1 ) Pr(E | H_1)  = .25 \times .11\\
Pr(H_2 \text{ and } E) = Pr(H_2 ) Pr(E | H_2)  = .25 \times .19\\
Pr(H_3 \text{ and } E) = Pr(H_3) Pr(E | H3) = .25 \times .31\\Pr(H_4 \text{ and } E) = Pr(H_4) Pr(E | H_4)  = .25 \times .53
\end{aligned}
\]</span></p>
<p><span class="math inline">\(Pr(E) =\)</span> the sum of these four products. I get <span class="math inline">\(Pr(E) = 0.285\)</span></p>
<p>This is a problem in Bayesian inference! Did you think this was just a fun little probability exercise? No no no no no, this was serious business. This was our first real Bayesian learning problem. I tell you I have a randomly selected person from this survey. What is your belief about their income? You think they’re in 1st, 2nd, 3rd or 4th quartile? Your belief is { <span class="math inline">\(.25 , .25 , .25, .25\)</span> }. Now I tell you they have a college degree so you will update your belief! <em>You will update your belief which is measured by a probability which is updated using Bayes rule</em>.</p>
<p><strong>Bayesian inference:</strong> is the discipline of updating our belief about the world based on further observation of the world.</p>
<p>If we know they have college degree our belief is not { <span class="math inline">\(.25, .25, .25, .25\)</span> } anymore it’s skewed more toward the higher income groups { <span class="math inline">\(.09, .17, .27, .47\)</span> }.</p>
<ul>
<li>The <span class="math inline">\(H_k\)</span>’s in this set-up are usually ‘states of nature’ and the <span class="math inline">\(E\)</span> in this set-up is the observed data</li>
</ul>
</div>
<div id="random-variables" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Random Variables</h2>
<p>In Bayesian statistics a <em>random variable</em> is any numerical quantity whose value is uncertain that includes things like experimental results (before the experiment is conducted) survey results (before the sample is taken). But it also includes model parameters states of nature.</p>
<p>Let <span class="math inline">\(Y\)</span> be a random variable <span class="math inline">\(\mathcal{Y}\)</span> is the set of possible values. If the set of possible values is a <em>countable</em> set { <span class="math inline">\(y_1, y_2, \ldots\)</span> }, then <span class="math inline">\(Y\)</span> is a discrete random variable.</p>
<p>We can compute <span class="math inline">\(Pr(Y = y)\)</span> for any value of <span class="math inline">\(y\)</span>. We’ll define the pdf <span class="math inline">\(p(y) = Pr(Y = y)\)</span>. <span class="math inline">\(Y\)</span> is the random variable, <span class="math inline">\(y\)</span> is a possible realized value for <span class="math inline">\(Y\)</span>. Note we are using the pdf (density) terminology even for a discrete r.v. (Hoff, 2009). If you know the pdf you know <span class="math inline">\(p(y) = Pr(Y = y)\)</span> for every possible value of <span class="math inline">\(y\)</span> then you know the whole probability distribution.</p>
<p>Two key properties for the pdf of a discrete r.v.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(0 \le p(y) \le 1\)</span></p></li>
<li><p>They sum to <span class="math inline">\(1\)</span></p></li>
</ol>
<p>The two most important discrete probability distributions are the Binomial and Poisson distributions. Here are their definitions:</p>
</div>
<div id="binomial" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Binomial distribution</h2>
<p><span class="math display">\[
Pr(Y=y|\theta) = \texttt{dbinom}(y,n,\theta)=\left(\begin{array}{l}n\\y
\end{array}\right)\theta^y(1-\theta)^{n-y}
\]</span></p>
<p><span class="math inline">\(Y\)</span> counts the number of successes in <span class="math inline">\(n\)</span> independent trials where the probability of success on each trial is <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(Pr(Y = y | \theta)\)</span> is the probability of <span class="math inline">\(y\)</span> successes in <span class="math inline">\(n\)</span> trials = probability of <span class="math inline">\(y\)</span> successes and <span class="math inline">\(n-y\)</span> failures. Well the probability of <span class="math inline">\(y\)</span> successes followed by <span class="math inline">\(n-y\)</span> failures is <span class="math inline">\(\theta^y \times (1-\theta)^{n-y}\)</span>. But <span class="math inline">\(Pr(Y = y| \theta)\)</span> is the probability of ANY possible sequence of <span class="math inline">\(y\)</span> successes and <span class="math inline">\(n-y\)</span> failures. There are “<span class="math inline">\(n\)</span> choose <span class="math inline">\(y\)</span>” such sequences; there are “<span class="math inline">\(n\)</span> choose <span class="math inline">\(y\)</span>” ways to arrange a sequence of <span class="math inline">\(y\)</span> successes and <span class="math inline">\(n-y\)</span> failures. Each sequence has the same probability <span class="math inline">\(\theta^y \times (1-\theta)^{n-y}\)</span>. The probability that it’s one of these sequences is the sum of those probabilities <span class="math inline">\(\left(\begin{array}{l}n\\y\end{array}\right)\theta^y(1-\theta)^{n-y}\)</span>. We’ll use this <code>dbinom</code> notation for the binomial probability function which is also the R function to calculate these! Calculating binomial probabilities in R is easy. Suppose <span class="math inline">\(n = 60\)</span> and <span class="math inline">\(\theta = .20\)</span>. This is the binomial distribution; the probability distribution for number of successes in <span class="math inline">\(60\)</span> trials where the success probability is <span class="math inline">\(0.20\)</span>.</p>
</div>
<div id="poisson" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Poisson distribution</h2>
<p><span class="math display">\[
Pr(Y = y|\theta) = \texttt{dpois}(y,\theta) = e^{-\theta} \frac{\theta^y}{y!}
\]</span></p>
<p><span class="math inline">\(\theta =\)</span> mean (expected value of <span class="math inline">\(Y\)</span>). <span class="math inline">\(Y\)</span> counts the number of events. <span class="math inline">\(\theta\)</span> is the expected number. <span class="math inline">\(y = \{0, 1, 2, \ldots\}\)</span> In R <code>dpois</code> does this! Let’s do a Poisson distribution with expected value of <span class="math inline">\(12\)</span> (will look not so different from that binomial distribution. That’s the Poisson distribution with expected value of <span class="math inline">\(12\)</span>.</p>
<p>They’re pretty similar. But careful on plots like these. When we make comparative plots like this we should take care to put them on the same scale! You can see that the binomial distribution assigns higher probabilities close to the expected value whereas the Poisson distribution gives more probability mass farther away from the expected value.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="index.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb1-2"><a href="index.html#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">30</span></span>
<span id="cb1-3"><a href="index.html#cb1-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(y, <span class="at">size =</span> <span class="dv">60</span>, <span class="at">prob =</span> <span class="fl">0.2</span>)</span>
<span id="cb1-4"><a href="index.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, p, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;Binomial(60, 0.2)&quot;</span>,</span>
<span id="cb1-5"><a href="index.html#cb1-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.13</span>))</span>
<span id="cb1-6"><a href="index.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(y,<span class="dv">0</span>,y,p)</span>
<span id="cb1-7"><a href="index.html#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="index.html#cb1-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">dpois</span>(y, <span class="dv">12</span>)</span>
<span id="cb1-9"><a href="index.html#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, p, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">main =</span> <span class="st">&quot;Poisson(12)&quot;</span>,</span>
<span id="cb1-10"><a href="index.html#cb1-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.13</span>))</span>
<span id="cb1-11"><a href="index.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(y,<span class="dv">0</span>,y,p)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-3-1.png" alt="Binomial and Poisson Distributions" width="672" />
<p class="caption">
Figure 1.1: Binomial and Poisson Distributions
</p>
</div>
<p>They’re applicable to different situations. The binomial distribution is most appropriate when we have a fixed number of trials. However, if you’re not clear on what <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span> should be but you know what <span class="math inline">\(n \times \theta\)</span> should be then you can’t do Binomial<span class="math inline">\((n, \theta)\)</span> but you can do Poisson<span class="math inline">\((n\times\theta)\)</span>. If there’s no upper bound then the binomial distribution is not appropriate (you can’t have <span class="math inline">\(150\)</span> successes in <span class="math inline">\(100\)</span> trials - in this case there’s no upper bound, and a Poisson model is more appropriate).</p>
</div>
<div id="continuous-random-variables" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Continuous Random Variables</h2>
<p>Where the set of possible values is not a countable set and it’s, say, the whole real line.</p>
<p><span class="math inline">\(F(y)\)</span> is the cumulative probability up to and including <span class="math inline">\(y\)</span> <span class="math inline">\(F(y) = Pr(Y \le y)\)</span>. If I know the cdf (cumulative distribution function) of a random variable, I know the whole distribution. Because I can find <span class="math inline">\(Pr( a &lt; Y \le b) = F(b) - F(a).\)</span> If <span class="math inline">\(Y\)</span> is a discrete random variable it’s cdf is a step function. If <span class="math inline">\(F\)</span> is a monotone continuous function then <span class="math inline">\(Y\)</span> is a continuous random variable. In that case (maybe) there exists a function <span class="math inline">\(p()\)</span> such that for any set <span class="math inline">\(A,\)</span> <span class="math inline">\(Pr(Y \in A) = \int_A { p(y) dy}.\)</span> <span class="math inline">\(p(y)\)</span> is called the density function or pdf.</p>
<p>If <span class="math inline">\(p()\)</span> is the density function of a continuous random variable then <span class="math inline">\(p(y) \ge 0\)</span> for all <span class="math inline">\(y\)</span> and <span class="math inline">\(\int_{-\infty}^\infty p(y) dy = 1\)</span>.</p>
<p>This is slightly different from the conditions on the pdf of a discrete random variable. If <span class="math inline">\(p(y\)</span>) is the pdf of a continuous random variable, it is possible that <span class="math inline">\(p(y) &gt; 1\)</span>. <span class="math inline">\(p(y)\)</span> does not represent a probability. The areas under the density curve are probabilities as long as that total area under the curve is <span class="math inline">\(1\)</span> then it’s a valid probability distribution. The most important continuous distribution is the Normal distribution. Let’s look at the pdf and cdf of a Normal dist! In R these are computed by <code>dnorm</code> (for the density) and <code>pnorm</code> (for the cdf).</p>
<div id="example-normalmu-10.75-sigma0.8" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Example: Normal(<span class="math inline">\(\mu = 10.75, \sigma=0.8\)</span>)</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="index.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb2-2"><a href="index.html#cb2-2" aria-hidden="true" tabindex="-1"></a>mu    <span class="ot">&lt;-</span> <span class="fl">10.75</span></span>
<span id="cb2-3"><a href="index.html#cb2-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb2-4"><a href="index.html#cb2-4" aria-hidden="true" tabindex="-1"></a>y     <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">7.5</span>, <span class="dv">14</span>, <span class="fl">0.05</span>)</span>
<span id="cb2-5"><a href="index.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># pdf</span></span>
<span id="cb2-6"><a href="index.html#cb2-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(y, mu, sigma)</span>
<span id="cb2-7"><a href="index.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, p, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb2-8"><a href="index.html#cb2-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Normalpdf(10.75, 0.8)&quot;</span>)</span>
<span id="cb2-9"><a href="index.html#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(y,<span class="dv">0</span>,y,p)</span>
<span id="cb2-10"><a href="index.html#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#cdf</span></span>
<span id="cb2-11"><a href="index.html#cb2-11" aria-hidden="true" tabindex="-1"></a>F <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(y, mu, sigma)</span>
<span id="cb2-12"><a href="index.html#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, F, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb2-13"><a href="index.html#cb2-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Normalcdf(10.75, 0.8)&quot;</span>)</span>
<span id="cb2-14"><a href="index.html#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(y,<span class="dv">0</span>,y,F)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-4-1.png" alt="Normal pdf and cdf" width="672" />
<p class="caption">
Figure 1.2: Normal pdf and cdf
</p>
</div>
<p>The “bell curve” describes the density. The cdf is an S-curve. The mean of this distribution is <span class="math inline">\(10.75\)</span>. The density is symmetric about that value, the mode of this distribution is <span class="math inline">\(\mu = 10.75\)</span> and the median of this distribution is <span class="math inline">\(\mu = 10.75\)</span>.</p>
<p>Speaking of means and modes and medians, let’s define these things!</p>
<p>The <strong>mean</strong> is the center of mass. It’s the weighted average of the possible values weighted by their probabilities.</p>
<p>The <strong>mode</strong> is the value with the highest probability (or highest probability density for a continuous rv).</p>
<p>The <strong>median</strong> is the <span class="math inline">\(.50\)</span> quantile. The point where half the probability is to the left and half the probability is to the right.</p>
<p>These are all measures of “location.”</p>
<p>If we want a measure of the “spread” of a distribution we can use the standard deviation. Def: The <strong>variance</strong> of a random variable is defined by <span class="math inline">\(\text{Var}(Y) = E{ ( Y -E(Y) )^2 } = E(Y^2) - E(Y)^2\)</span> expected squared distance from the mean value. Take the square root of this (to back to the original scale) and call that quantity the standard deviation.</p>
<p>We can also measure location and spread both using quantiles! For discrete distributions quantiles are weird. The <span class="math inline">\(\alpha\)</span>-quantile of a distribution is the value <span class="math inline">\(y_{\alpha}\)</span> such that <span class="math inline">\(F(y_{\alpha}) = \alpha\)</span>. Consider the normal cdf below. Where is the <span class="math inline">\(.75\)</span> quantile?</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="index.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, F, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb3-2"><a href="index.html#cb3-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Normalcdf(10.75, 0.8)&quot;</span>)</span>
<span id="cb3-3"><a href="index.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>( <span class="at">h =</span> <span class="fl">0.75</span>)</span>
<span id="cb3-4"><a href="index.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">qnorm</span>(<span class="fl">0.75</span>, mu, sigma))</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="bayesianS21_files/figure-html/unnamed-chunk-5-1.png" alt="Normal cdf with quantile" width="70%" height="70%" />
<p class="caption">
Figure 1.3: Normal cdf with quantile
</p>
</div>
<p>In R, I can find normal quantiles using the <code>qnorm</code> function. With discrete distributions the cdf is a step function it jumps, so there’s not a uniquely defined point where the cdf curve passes through <span class="math inline">\(.75\)</span> say.</p>
<p>A <span class="math inline">\(50\%\)</span> probability interval for the distribution of <span class="math inline">\(Y\)</span> is <span class="math inline">\((y_{.25},~y_{.75}).\)</span> A <span class="math inline">\(95\%\)</span> probability interval is <span class="math inline">\((y_{.025}, ~ y_{.975}),\)</span> where <span class="math inline">\(y_\alpha\)</span> represents <span class="math inline">\(\alpha\)</span> quantile of <span class="math inline">\(y\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="exchangeability.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesianS21.pdf", "bayesianS21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
